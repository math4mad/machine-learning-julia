[
  {
    "objectID": "workflow/tailwindcss.html",
    "href": "workflow/tailwindcss.html",
    "title": "embedding tailwind css",
    "section": "",
    "text": "test tailwind css\n\nExample heading New\n\n\nNotifications 4\n\n\n❶ ❷ ❸ ❹ ❺ ➅ ➆ ➇ ➈ ➉\n\n\n❶ ❷ ❸ ❹ ❺ ➅ ➆ ➇ ➈ ➉\n\n\n\n\nFirst Name\n\n\nLast Name\n\n\nPoints\n\n\n\n\nJill\n\n\nSmith\n\n\n50\n\n\n\n\nJill\n\n\nSmith\n\n\n50\n\n\n\n\nJill\n\n\nSmith\n\n\n50"
  },
  {
    "objectID": "workflow/composing-models.html",
    "href": "workflow/composing-models.html",
    "title": "Composing Models",
    "section": "",
    "text": "MLJ 中组合模型有三种形式\n\n\n\n\n\n    %%| label: fig-mlj-composing model\n    %%| fig-cap: \"MLJ  composing model\"\n    %%| fig-width: 6.5\n    %%| echo: true\n    graph TD\n        C(Composing Models)\n        C --&gt;|One| D([Pipeline Model ])\n        C --&gt;|Two| E([Ensemble Model])\n        C --&gt;|Three| F([Stack Model])",
    "crumbs": [
      "Workflow",
      "composing models"
    ]
  },
  {
    "objectID": "start/getting-start.html",
    "href": "start/getting-start.html",
    "title": "Build first MLJ machine learning model",
    "section": "",
    "text": "we followed workflow, step by step\n\n\nfor iris dataset ref: Iris Dataset EDA\n\nusing Pipe\nusing CSV,DataFrames\nusing MLJ\ndf=CSV.File(\"../data/iris.csv\")|&gt;DataFrame;\n\n\nschema(df)\n\n\n┌──────────────┬────────────┬──────────┐\n│ names        │ scitypes   │ types    │\n├──────────────┼────────────┼──────────┤\n│ sepal_length │ Continuous │ Float64  │\n│ sepal_width  │ Continuous │ Float64  │\n│ petal_length │ Continuous │ Float64  │\n│ petal_width  │ Continuous │ Float64  │\n│ species      │ Textual    │ String15 │\n└──────────────┴────────────┴──────────┘\n\n\n\n\nfor species need transform to Categorical type\n\ndf.species= coerce(df.species,Multiclass);\nschema(df)\n\n\n┌──────────────┬───────────────┬────────────────────────────────────┐\n│ names        │ scitypes      │ types                              │\n├──────────────┼───────────────┼────────────────────────────────────┤\n│ sepal_length │ Continuous    │ Float64                            │\n│ sepal_width  │ Continuous    │ Float64                            │\n│ petal_length │ Continuous    │ Float64                            │\n│ petal_width  │ Continuous    │ Float64                            │\n│ species      │ Multiclass{3} │ CategoricalValue{String15, UInt32} │\n└──────────────┴───────────────┴────────────────────────────────────┘\n\n\n\n\nnow we need to split data\n\ny, X = unpack(df, ==(:species); rng=123)\n\n\n(CategoricalArrays.CategoricalValue{String15, UInt32}[String15(\"virginica\"), String15(\"versicolor\"), String15(\"virginica\"), String15(\"setosa\"), String15(\"versicolor\"), String15(\"virginica\"), String15(\"setosa\"), String15(\"versicolor\"), String15(\"setosa\"), String15(\"virginica\")  …  String15(\"virginica\"), String15(\"virginica\"), String15(\"virginica\"), String15(\"versicolor\"), String15(\"virginica\"), String15(\"setosa\"), String15(\"virginica\"), String15(\"virginica\"), String15(\"setosa\"), String15(\"setosa\")], 150×4 DataFrame\n Row │ sepal_length  sepal_width  petal_length  petal_width \n     │ Float64       Float64      Float64       Float64     \n─────┼──────────────────────────────────────────────────────\n   1 │          6.7          3.3           5.7          2.1\n   2 │          5.7          2.8           4.1          1.3\n   3 │          7.2          3.0           5.8          1.6\n   4 │          4.4          2.9           1.4          0.2\n   5 │          5.6          2.5           3.9          1.1\n   6 │          6.5          3.0           5.2          2.0\n   7 │          4.4          3.0           1.3          0.2\n   8 │          6.1          2.9           4.7          1.4\n   9 │          5.4          3.9           1.7          0.4\n  10 │          4.9          2.5           4.5          1.7\n  11 │          6.3          2.5           4.9          1.5\n  ⋮  │      ⋮             ⋮            ⋮             ⋮\n 141 │          6.4          2.7           5.3          1.9\n 142 │          6.8          3.2           5.9          2.3\n 143 │          6.9          3.1           5.4          2.1\n 144 │          6.1          2.8           4.0          1.3\n 145 │          6.7          2.5           5.8          1.8\n 146 │          5.0          3.5           1.3          0.3\n 147 │          7.6          3.0           6.6          2.1\n 148 │          6.3          2.5           5.0          1.9\n 149 │          5.1          3.8           1.6          0.2\n 150 │          5.0          3.6           1.4          0.2\n                                            129 rows omitted)\n\n\n\n\n\n\n\nres=@pipe models(matching(X,y))|&gt; DataFrame|&gt;select(_,[:name,:package_name])\n\n53×2 DataFrame28 rows omitted\n\n\n\nRow\nname\npackage_name\n\n\n\nString\nString\n\n\n\n\n1\nAdaBoostClassifier\nMLJScikitLearnInterface\n\n\n2\nAdaBoostStumpClassifier\nDecisionTree\n\n\n3\nBaggingClassifier\nMLJScikitLearnInterface\n\n\n4\nBayesianLDA\nMLJScikitLearnInterface\n\n\n5\nBayesianLDA\nMultivariateStats\n\n\n6\nBayesianQDA\nMLJScikitLearnInterface\n\n\n7\nBayesianSubspaceLDA\nMultivariateStats\n\n\n8\nCatBoostClassifier\nCatBoost\n\n\n9\nConstantClassifier\nMLJModels\n\n\n10\nDecisionTreeClassifier\nBetaML\n\n\n11\nDecisionTreeClassifier\nDecisionTree\n\n\n12\nDeterministicConstantClassifier\nMLJModels\n\n\n13\nDummyClassifier\nMLJScikitLearnInterface\n\n\n⋮\n⋮\n⋮\n\n\n42\nRandomForestClassifier\nMLJScikitLearnInterface\n\n\n43\nRidgeCVClassifier\nMLJScikitLearnInterface\n\n\n44\nRidgeClassifier\nMLJScikitLearnInterface\n\n\n45\nSGDClassifier\nMLJScikitLearnInterface\n\n\n46\nSVC\nLIBSVM\n\n\n47\nSVMClassifier\nMLJScikitLearnInterface\n\n\n48\nSVMLinearClassifier\nMLJScikitLearnInterface\n\n\n49\nSVMNuClassifier\nMLJScikitLearnInterface\n\n\n50\nStableForestClassifier\nSIRUS\n\n\n51\nStableRulesClassifier\nSIRUS\n\n\n52\nSubspaceLDA\nMultivariateStats\n\n\n53\nXGBoostClassifier\nXGBoost\n\n\n\n\n\n\nsimple way to look into model details:\ndoc($name, pkg=$package_name)\n\ndoc(\"AdaBoostStumpClassifier\", pkg=\"DecisionTree\")\n\nAdaBoostStumpClassifier\nA model type for constructing a Ada-boosted stump classifier, based on DecisionTree.jl, and implementing the MLJ model interface.\nFrom MLJ, the type can be imported using\nAdaBoostStumpClassifier = @load AdaBoostStumpClassifier pkg=DecisionTree\nDo model = AdaBoostStumpClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in AdaBoostStumpClassifier(n_iter=...).\n\n\nIn MLJ or MLJBase, bind an instance model to data with\nmach = machine(model, X, y)\nwhere:\n\nX: any table of input features (eg, a DataFrame) whose columns each have one of the following element scitypes: Continuous, Count, or &lt;:OrderedFactor; check column scitypes with schema(X)\ny: the target, which can be any AbstractVector whose element scitype is &lt;:OrderedFactor or &lt;:Multiclass; check the scitype with scitype(y)\n\nTrain the machine with fit!(mach, rows=...).\n\n\n\n\nn_iter=10: number of iterations of AdaBoost\nfeature_importance: method to use for computing feature importances. One of (:impurity, :split)\nrng=Random.GLOBAL_RNG: random number generator or seed\n\n\n\n\n\npredict(mach, Xnew): return predictions of the target given features Xnew having the same scitype as X above. Predictions are probabilistic, but uncalibrated.\npredict_mode(mach, Xnew): instead return the mode of each prediction above.\n\n\n\n\nThe fields of fitted_params(mach) are:\n\nstumps: the Ensemble object returned by the core DecisionTree.jl algorithm.\ncoefficients: the stump coefficients (one per stump)\n\n\n\n\n\nfeatures: the names of the features encountered in training\n\n\n\n\n\nfeature_importances(mach) returns a vector of (feature::Symbol =&gt; importance) pairs; the type of importance is determined by the hyperparameter feature_importance (see above)\n\n\n\n\nusing MLJ\nBooster = @load AdaBoostStumpClassifier pkg=DecisionTree\nbooster = Booster(n_iter=15)\n\nX, y = @load_iris\nmach = machine(booster, X, y) |&gt; fit!\n\nXnew = (sepal_length = [6.4, 7.2, 7.4],\n        sepal_width = [2.8, 3.0, 2.8],\n        petal_length = [5.6, 5.8, 6.1],\n        petal_width = [2.1, 1.6, 1.9],)\nyhat = predict(mach, Xnew) # probabilistic predictions\npredict_mode(mach, Xnew)   # point predictions\npdf.(yhat, \"virginica\")    # probabilities for the \"verginica\" class\n\nfitted_params(mach).stumps # raw `Ensemble` object from DecisionTree.jl\nfitted_params(mach).coefs  # coefficient associated with each stump\nfeature_importances(mach)\nSee also DecisionTree.jl and the unwrapped model type MLJDecisionTreeInterface.DecisionTree.AdaBoostStumpClassifier.\n\n\n\n\n\n\nthere are too many models, so you just add code if repel notice missing package , you just install packages.\n\nTree = @load DecisionTreeClassifier pkg=DecisionTree\n\nimport MLJDecisionTreeInterface ✔\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n\n\nMLJDecisionTreeInterface.DecisionTreeClassifier\n\n\n\n\n\n\n tree=Tree()\n\nDecisionTreeClassifier(\n  max_depth = -1, \n  min_samples_leaf = 1, \n  min_samples_split = 2, \n  min_purity_increase = 0.0, \n  n_subfeatures = 0, \n  post_prune = false, \n  merge_purity_threshold = 1.0, \n  display_depth = 5, \n  feature_importance = :impurity, \n  rng = Random._GLOBAL_RNG())\n\n\n\n\n\n\nevaluate(tree, X, y,\n         resampling=CV(shuffle=true),\n         measures=[log_loss, accuracy],\n         verbosity=0)\n\n\nPerformanceEvaluation object with these fields:\n  model, measure, operation, measurement, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_rows, resampling, repeats\nExtract:\n┌──────────────────────┬──────────────┬─────────────┬─────────┬─────────────────\n│ measure              │ operation    │ measurement │ 1.96*SE │ per_fold       ⋯\n├──────────────────────┼──────────────┼─────────────┼─────────┼─────────────────\n│ LogLoss(             │ predict      │ 2.4         │ 1.31    │ [1.44, 2.88, 2 ⋯\n│   tol = 2.22045e-16) │              │             │         │                ⋯\n│ Accuracy()           │ predict_mode │ 0.933       │ 0.0362  │ [0.96, 0.92, 1 ⋯\n└──────────────────────┴──────────────┴─────────────┴─────────┴─────────────────\n                                                                1 column omitted\n\n\n\n\n\n\n\nfill model and data to machine\n\nmach=machine(tree, X,y)\n\nuntrained Machine; caches model-specific representations of data\n  model: DecisionTreeClassifier(max_depth = -1, …)\n  args: \n    1:  Source @757 ⏎ Table{AbstractVector{Continuous}}\n    2:  Source @003 ⏎ AbstractVector{Multiclass{3}}\n\n\n\n\n\n\ntrain, test = partition(eachindex(y), 0.7); \n\n\n\n\nuse train index to inform model to we must not use all data!!!\n\nfit!(mach, rows=train)\n\n[ Info: Training machine(DecisionTreeClassifier(max_depth = -1, …), …).\n\n\ntrained Machine; caches model-specific representations of data\n  model: DecisionTreeClassifier(max_depth = -1, …)\n  args: \n    1:  Source @757 ⏎ Table{AbstractVector{Continuous}}\n    2:  Source @003 ⏎ AbstractVector{Multiclass{3}}\n\n\n\n\n\nsame with other julia stats model , X must be a matrix ,even if it is one row and one column data\n\nyhat = predict_mode(mach, X[test,:])\n\n45-element CategoricalArrays.CategoricalArray{String15,1,UInt32}:\n String15(\"versicolor\")\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"virginica\")\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"versicolor\")\n String15(\"versicolor\")\n String15(\"virginica\")\n String15(\"versicolor\")\n String15(\"virginica\")\n String15(\"versicolor\")\n String15(\"virginica\")\n ⋮\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"virginica\")\n String15(\"virginica\")\n String15(\"virginica\")\n String15(\"versicolor\")\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"virginica\")\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"setosa\")\n\n\n\n\n\n\naccuracy(yhat,y[test])\n\n0.9555555555555556\n\n\n\nlevels(yhat)\n\n3-element Vector{String15}:\n \"setosa\"\n \"versicolor\"\n \"virginica\"",
    "crumbs": [
      "Start",
      "build first model"
    ]
  },
  {
    "objectID": "start/getting-start.html#getting-start",
    "href": "start/getting-start.html#getting-start",
    "title": "Build first MLJ machine learning model",
    "section": "",
    "text": "we followed workflow, step by step\n\n\nfor iris dataset ref: Iris Dataset EDA\n\nusing Pipe\nusing CSV,DataFrames\nusing MLJ\ndf=CSV.File(\"../data/iris.csv\")|&gt;DataFrame;\n\n\nschema(df)\n\n\n┌──────────────┬────────────┬──────────┐\n│ names        │ scitypes   │ types    │\n├──────────────┼────────────┼──────────┤\n│ sepal_length │ Continuous │ Float64  │\n│ sepal_width  │ Continuous │ Float64  │\n│ petal_length │ Continuous │ Float64  │\n│ petal_width  │ Continuous │ Float64  │\n│ species      │ Textual    │ String15 │\n└──────────────┴────────────┴──────────┘\n\n\n\n\nfor species need transform to Categorical type\n\ndf.species= coerce(df.species,Multiclass);\nschema(df)\n\n\n┌──────────────┬───────────────┬────────────────────────────────────┐\n│ names        │ scitypes      │ types                              │\n├──────────────┼───────────────┼────────────────────────────────────┤\n│ sepal_length │ Continuous    │ Float64                            │\n│ sepal_width  │ Continuous    │ Float64                            │\n│ petal_length │ Continuous    │ Float64                            │\n│ petal_width  │ Continuous    │ Float64                            │\n│ species      │ Multiclass{3} │ CategoricalValue{String15, UInt32} │\n└──────────────┴───────────────┴────────────────────────────────────┘\n\n\n\n\nnow we need to split data\n\ny, X = unpack(df, ==(:species); rng=123)\n\n\n(CategoricalArrays.CategoricalValue{String15, UInt32}[String15(\"virginica\"), String15(\"versicolor\"), String15(\"virginica\"), String15(\"setosa\"), String15(\"versicolor\"), String15(\"virginica\"), String15(\"setosa\"), String15(\"versicolor\"), String15(\"setosa\"), String15(\"virginica\")  …  String15(\"virginica\"), String15(\"virginica\"), String15(\"virginica\"), String15(\"versicolor\"), String15(\"virginica\"), String15(\"setosa\"), String15(\"virginica\"), String15(\"virginica\"), String15(\"setosa\"), String15(\"setosa\")], 150×4 DataFrame\n Row │ sepal_length  sepal_width  petal_length  petal_width \n     │ Float64       Float64      Float64       Float64     \n─────┼──────────────────────────────────────────────────────\n   1 │          6.7          3.3           5.7          2.1\n   2 │          5.7          2.8           4.1          1.3\n   3 │          7.2          3.0           5.8          1.6\n   4 │          4.4          2.9           1.4          0.2\n   5 │          5.6          2.5           3.9          1.1\n   6 │          6.5          3.0           5.2          2.0\n   7 │          4.4          3.0           1.3          0.2\n   8 │          6.1          2.9           4.7          1.4\n   9 │          5.4          3.9           1.7          0.4\n  10 │          4.9          2.5           4.5          1.7\n  11 │          6.3          2.5           4.9          1.5\n  ⋮  │      ⋮             ⋮            ⋮             ⋮\n 141 │          6.4          2.7           5.3          1.9\n 142 │          6.8          3.2           5.9          2.3\n 143 │          6.9          3.1           5.4          2.1\n 144 │          6.1          2.8           4.0          1.3\n 145 │          6.7          2.5           5.8          1.8\n 146 │          5.0          3.5           1.3          0.3\n 147 │          7.6          3.0           6.6          2.1\n 148 │          6.3          2.5           5.0          1.9\n 149 │          5.1          3.8           1.6          0.2\n 150 │          5.0          3.6           1.4          0.2\n                                            129 rows omitted)\n\n\n\n\n\n\n\nres=@pipe models(matching(X,y))|&gt; DataFrame|&gt;select(_,[:name,:package_name])\n\n53×2 DataFrame28 rows omitted\n\n\n\nRow\nname\npackage_name\n\n\n\nString\nString\n\n\n\n\n1\nAdaBoostClassifier\nMLJScikitLearnInterface\n\n\n2\nAdaBoostStumpClassifier\nDecisionTree\n\n\n3\nBaggingClassifier\nMLJScikitLearnInterface\n\n\n4\nBayesianLDA\nMLJScikitLearnInterface\n\n\n5\nBayesianLDA\nMultivariateStats\n\n\n6\nBayesianQDA\nMLJScikitLearnInterface\n\n\n7\nBayesianSubspaceLDA\nMultivariateStats\n\n\n8\nCatBoostClassifier\nCatBoost\n\n\n9\nConstantClassifier\nMLJModels\n\n\n10\nDecisionTreeClassifier\nBetaML\n\n\n11\nDecisionTreeClassifier\nDecisionTree\n\n\n12\nDeterministicConstantClassifier\nMLJModels\n\n\n13\nDummyClassifier\nMLJScikitLearnInterface\n\n\n⋮\n⋮\n⋮\n\n\n42\nRandomForestClassifier\nMLJScikitLearnInterface\n\n\n43\nRidgeCVClassifier\nMLJScikitLearnInterface\n\n\n44\nRidgeClassifier\nMLJScikitLearnInterface\n\n\n45\nSGDClassifier\nMLJScikitLearnInterface\n\n\n46\nSVC\nLIBSVM\n\n\n47\nSVMClassifier\nMLJScikitLearnInterface\n\n\n48\nSVMLinearClassifier\nMLJScikitLearnInterface\n\n\n49\nSVMNuClassifier\nMLJScikitLearnInterface\n\n\n50\nStableForestClassifier\nSIRUS\n\n\n51\nStableRulesClassifier\nSIRUS\n\n\n52\nSubspaceLDA\nMultivariateStats\n\n\n53\nXGBoostClassifier\nXGBoost\n\n\n\n\n\n\nsimple way to look into model details:\ndoc($name, pkg=$package_name)\n\ndoc(\"AdaBoostStumpClassifier\", pkg=\"DecisionTree\")\n\nAdaBoostStumpClassifier\nA model type for constructing a Ada-boosted stump classifier, based on DecisionTree.jl, and implementing the MLJ model interface.\nFrom MLJ, the type can be imported using\nAdaBoostStumpClassifier = @load AdaBoostStumpClassifier pkg=DecisionTree\nDo model = AdaBoostStumpClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in AdaBoostStumpClassifier(n_iter=...).\n\n\nIn MLJ or MLJBase, bind an instance model to data with\nmach = machine(model, X, y)\nwhere:\n\nX: any table of input features (eg, a DataFrame) whose columns each have one of the following element scitypes: Continuous, Count, or &lt;:OrderedFactor; check column scitypes with schema(X)\ny: the target, which can be any AbstractVector whose element scitype is &lt;:OrderedFactor or &lt;:Multiclass; check the scitype with scitype(y)\n\nTrain the machine with fit!(mach, rows=...).\n\n\n\n\nn_iter=10: number of iterations of AdaBoost\nfeature_importance: method to use for computing feature importances. One of (:impurity, :split)\nrng=Random.GLOBAL_RNG: random number generator or seed\n\n\n\n\n\npredict(mach, Xnew): return predictions of the target given features Xnew having the same scitype as X above. Predictions are probabilistic, but uncalibrated.\npredict_mode(mach, Xnew): instead return the mode of each prediction above.\n\n\n\n\nThe fields of fitted_params(mach) are:\n\nstumps: the Ensemble object returned by the core DecisionTree.jl algorithm.\ncoefficients: the stump coefficients (one per stump)\n\n\n\n\n\nfeatures: the names of the features encountered in training\n\n\n\n\n\nfeature_importances(mach) returns a vector of (feature::Symbol =&gt; importance) pairs; the type of importance is determined by the hyperparameter feature_importance (see above)\n\n\n\n\nusing MLJ\nBooster = @load AdaBoostStumpClassifier pkg=DecisionTree\nbooster = Booster(n_iter=15)\n\nX, y = @load_iris\nmach = machine(booster, X, y) |&gt; fit!\n\nXnew = (sepal_length = [6.4, 7.2, 7.4],\n        sepal_width = [2.8, 3.0, 2.8],\n        petal_length = [5.6, 5.8, 6.1],\n        petal_width = [2.1, 1.6, 1.9],)\nyhat = predict(mach, Xnew) # probabilistic predictions\npredict_mode(mach, Xnew)   # point predictions\npdf.(yhat, \"virginica\")    # probabilities for the \"verginica\" class\n\nfitted_params(mach).stumps # raw `Ensemble` object from DecisionTree.jl\nfitted_params(mach).coefs  # coefficient associated with each stump\nfeature_importances(mach)\nSee also DecisionTree.jl and the unwrapped model type MLJDecisionTreeInterface.DecisionTree.AdaBoostStumpClassifier.\n\n\n\n\n\n\nthere are too many models, so you just add code if repel notice missing package , you just install packages.\n\nTree = @load DecisionTreeClassifier pkg=DecisionTree\n\nimport MLJDecisionTreeInterface ✔\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n\n\nMLJDecisionTreeInterface.DecisionTreeClassifier\n\n\n\n\n\n\n tree=Tree()\n\nDecisionTreeClassifier(\n  max_depth = -1, \n  min_samples_leaf = 1, \n  min_samples_split = 2, \n  min_purity_increase = 0.0, \n  n_subfeatures = 0, \n  post_prune = false, \n  merge_purity_threshold = 1.0, \n  display_depth = 5, \n  feature_importance = :impurity, \n  rng = Random._GLOBAL_RNG())\n\n\n\n\n\n\nevaluate(tree, X, y,\n         resampling=CV(shuffle=true),\n         measures=[log_loss, accuracy],\n         verbosity=0)\n\n\nPerformanceEvaluation object with these fields:\n  model, measure, operation, measurement, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_rows, resampling, repeats\nExtract:\n┌──────────────────────┬──────────────┬─────────────┬─────────┬─────────────────\n│ measure              │ operation    │ measurement │ 1.96*SE │ per_fold       ⋯\n├──────────────────────┼──────────────┼─────────────┼─────────┼─────────────────\n│ LogLoss(             │ predict      │ 2.4         │ 1.31    │ [1.44, 2.88, 2 ⋯\n│   tol = 2.22045e-16) │              │             │         │                ⋯\n│ Accuracy()           │ predict_mode │ 0.933       │ 0.0362  │ [0.96, 0.92, 1 ⋯\n└──────────────────────┴──────────────┴─────────────┴─────────┴─────────────────\n                                                                1 column omitted\n\n\n\n\n\n\n\nfill model and data to machine\n\nmach=machine(tree, X,y)\n\nuntrained Machine; caches model-specific representations of data\n  model: DecisionTreeClassifier(max_depth = -1, …)\n  args: \n    1:  Source @757 ⏎ Table{AbstractVector{Continuous}}\n    2:  Source @003 ⏎ AbstractVector{Multiclass{3}}\n\n\n\n\n\n\ntrain, test = partition(eachindex(y), 0.7); \n\n\n\n\nuse train index to inform model to we must not use all data!!!\n\nfit!(mach, rows=train)\n\n[ Info: Training machine(DecisionTreeClassifier(max_depth = -1, …), …).\n\n\ntrained Machine; caches model-specific representations of data\n  model: DecisionTreeClassifier(max_depth = -1, …)\n  args: \n    1:  Source @757 ⏎ Table{AbstractVector{Continuous}}\n    2:  Source @003 ⏎ AbstractVector{Multiclass{3}}\n\n\n\n\n\nsame with other julia stats model , X must be a matrix ,even if it is one row and one column data\n\nyhat = predict_mode(mach, X[test,:])\n\n45-element CategoricalArrays.CategoricalArray{String15,1,UInt32}:\n String15(\"versicolor\")\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"virginica\")\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"versicolor\")\n String15(\"versicolor\")\n String15(\"virginica\")\n String15(\"versicolor\")\n String15(\"virginica\")\n String15(\"versicolor\")\n String15(\"virginica\")\n ⋮\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"virginica\")\n String15(\"virginica\")\n String15(\"virginica\")\n String15(\"versicolor\")\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"virginica\")\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"setosa\")\n\n\n\n\n\n\naccuracy(yhat,y[test])\n\n0.9555555555555556\n\n\n\nlevels(yhat)\n\n3-element Vector{String15}:\n \"setosa\"\n \"versicolor\"\n \"virginica\"",
    "crumbs": [
      "Start",
      "build first model"
    ]
  },
  {
    "objectID": "packages/statesbase.html",
    "href": "packages/statesbase.html",
    "title": "StatsBase.jl",
    "section": "",
    "text": "StatsBase.jl"
  },
  {
    "objectID": "packages/glm.html",
    "href": "packages/glm.html",
    "title": "GLM.jl",
    "section": "",
    "text": "GLM.jl",
    "crumbs": [
      "GLM"
    ]
  },
  {
    "objectID": "learn/example.html",
    "href": "learn/example.html",
    "title": "Code-insertion Example",
    "section": "",
    "text": "quoted block added before the post"
  },
  {
    "objectID": "learn/example.html#heading",
    "href": "learn/example.html#heading",
    "title": "Code-insertion Example",
    "section": "Heading",
    "text": "Heading\nThis filter adds code immediately before and after the post."
  },
  {
    "objectID": "learn/example.html#appended-section-after-the-post",
    "href": "learn/example.html#appended-section-after-the-post",
    "title": "Code-insertion Example",
    "section": "Appended Section After the Post",
    "text": "Appended Section After the Post"
  },
  {
    "objectID": "dataset/iris.html",
    "href": "dataset/iris.html",
    "title": "🌺🌸Iris dataset",
    "section": "",
    "text": "info\n\n\n\nIRIS is a an famous dataset for statistics and data science. t has long history. we will use it angain and agian.rery time wo use iris dataset we can get new knowledget is like exercies for running",
    "crumbs": [
      "Dataset",
      "1.iris"
    ]
  },
  {
    "objectID": "dataset/iris.html#load-csv",
    "href": "dataset/iris.html#load-csv",
    "title": "🌺🌸Iris dataset",
    "section": "1. load csv",
    "text": "1. load csv\n\n using CSV,DataFrames,Tidier\n using CairoMakie,AlgebraOfGraphics\n\n #df=download(\"http://localhost:5220/data/iris.csv\")|&gt;CSV.File|&gt;DataFrame;\n df=CSV.File(\"../data/iris.csv\")|&gt;DataFrame\n first(df,5)\n\n5×5 DataFrame\n\n\n\nRow\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\nFloat64\nFloat64\nFloat64\nFloat64\nString15\n\n\n\n\n1\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n2\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n3\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\nCSV,DataFrames,Tidier 用于数据框处理\nCairoMakie,AlgebraOfGraphics用于绘图",
    "crumbs": [
      "Dataset",
      "1.iris"
    ]
  },
  {
    "objectID": "dataset/iris.html#describe-dataframe",
    "href": "dataset/iris.html#describe-dataframe",
    "title": "🌺🌸Iris dataset",
    "section": "2. describe dataframe",
    "text": "2. describe dataframe\n\ndescribe(df)\n\n5×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nDataType\n\n\n\n\n1\nsepal_length\n5.84333\n4.3\n5.8\n7.9\n0\nFloat64\n\n\n2\nsepal_width\n3.054\n2.0\n3.0\n4.4\n0\nFloat64\n\n\n3\npetal_length\n3.75867\n1.0\n4.35\n6.9\n0\nFloat64\n\n\n4\npetal_width\n1.19867\n0.1\n1.3\n2.5\n0\nFloat64\n\n\n5\nspecies\n\nsetosa\n\nvirginica\n0\nString15",
    "crumbs": [
      "Dataset",
      "1.iris"
    ]
  },
  {
    "objectID": "dataset/iris.html#eda",
    "href": "dataset/iris.html#eda",
    "title": "🌺🌸Iris dataset",
    "section": "3. EDA",
    "text": "3. EDA\n\n3.1 density plot\n\npalette = (color=tuple.([\"#FC7808\", \"#8C00EC\", \"#107A78\"], 0.65),\n           marker=[:circle, :utriangle, :rect])\ncycle = Cycle([:color, :marker], covary=true)\n\n\ndatalayer=data(df)\nmappinglayer1=mapping(:sepal_length,:sepal_width,color=:species)\nmappinglayer2=mapping(:sepal_length,color=:species)\nmappinglayer3=mapping(:sepal_width,color=:species)\n\nvisuallayer1=visual(Scatter,strokewidth=1,storkecolor=:black)\nvisuallayer2=visual(Density)\nvisuallayer3=visual(Density,direction=:y)\nplc=datalayer*mappinglayer1*visuallayer1  # top pic\nplt=datalayer*mappinglayer2*visuallayer2  # main pic\nplr=datalayer*mappinglayer3*visuallayer3  # right pic\n\nwith_theme(theme_light(),resolution = (600,400), palette=palette, Scatter=(cycle=cycle,)) do\n        fig = Figure()\n        axs = [Axis(fig[2,1], xlabel = \"sepal_length\", ylabel = \"sepal_width\"),\n            Axis(fig[1,1]), Axis(fig[2,2])]\n        dots = draw!(axs[1], plc)  # bject for lengend extract species info\n        draw!(axs[2], plt)\n        draw!(axs[3], plr)\n        # getting the right layout aspect\n        colsize!(fig.layout, 1, Auto(4.0))\n        rowsize!(fig.layout, 1, Auto(1/3))\n        colgap!(fig.layout,3)\n        rowgap!(fig.layout, 3)\n        linkxaxes!(axs[1], axs[2])\n        linkyaxes!(axs[1], axs[3])\n        hidedecorations!.(axs[2:3], grid=false)\n        \n        legend!(fig[1,2], dots)\n        fig\n    end\n\n\n\n\n\n\n\n\n\n\n3.2 pair plot\n\ncats=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\nlen=length(cats)\ncats=Symbol.(cats)\n\n\ndata_layer=data(df)\n\nfunction make_layer(sy1::Symbol,sy2::Symbol)\n   mapping_layer=sy1==sy2 ? mapping(sy1,color=:species) : mapping(sy1,sy2,color=:species)\n   vis_layer=sy1==sy2 ? visual(Density,color=:species) : visual(Scatter,color=:species,strokewidth=1,storkecolor=:black)\n   return  data_layer*mapping_layer*vis_layer\nend\n\nwith_theme(theme_minimal(),resolution = (800,800), palette=palette, Scatter=(cycle=cycle,)) do\n        cl=0.1\n        fig = Figure()\n        for i in 1:len\n            for  j in 1:len\n                ax=Axis(fig[i,j])\n                j==1 ? ax.ylabel= String(cats[i]) : nothing\n                i==len ? ax.xlabel= String(cats[j]) : nothing\n                layer=make_layer(cats[i],cats[j])\n              [i,j]==[len,len] ? cl=draw!(ax,layer) : draw!(ax,layer)\n            end\n        end\n        legend!(fig[2:3,5], cl)\n        fig\nend\n\n\n\n\n\n\n\n\n\n\n3.3 box plot\ntransform four feature to attribute and value\n\nlong_data=@pivot_longer(df, 1:5, names_to = attribute, values_to = value,-species)\nfirst(long_data,5)\n\n5×3 DataFrame\n\n\n\nRow\nspecies\nattribute\nvalue\n\n\n\nString15\nString\nFloat64\n\n\n\n\n1\nsetosa\nsepal_length\n5.1\n\n\n2\nsetosa\nsepal_length\n4.9\n\n\n3\nsetosa\nsepal_length\n4.7\n\n\n4\nsetosa\nsepal_length\n4.6\n\n\n5\nsetosa\nsepal_length\n5.0\n\n\n\n\n\n\n\ndata(long_data) * visual(BoxPlot, show_notch=true) *\n    mapping(:attribute, :value, color=:species, dodge=:species) |&gt; draw\n\n\n\n\n\n\n\n\n\n\n3.4 correlation of numerical variables",
    "crumbs": [
      "Dataset",
      "1.iris"
    ]
  },
  {
    "objectID": "dataset/boston-house.html",
    "href": "dataset/boston-house.html",
    "title": "🏠🏡🏣 Boston Housing dataset",
    "section": "",
    "text": "info\n\n\n\nBoston Housing is another famous dataset",
    "crumbs": [
      "Dataset",
      "3.boston housing"
    ]
  },
  {
    "objectID": "dataset/boston-house.html#load-csv",
    "href": "dataset/boston-house.html#load-csv",
    "title": "🏠🏡🏣 Boston Housing dataset",
    "section": "1. load csv",
    "text": "1. load csv\n\n using CSV,DataFrames,Tidier\n using CairoMakie,AlgebraOfGraphics\n\ndf=CSV.File(\"../data/housing.csv\")|&gt;DataFrame\n first(df,5)\n\n5×14 DataFrame\n\n\n\nRow\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nb\nlstat\nmedv\n\n\n\nFloat64\nFloat64\nFloat64\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\nInt64\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n0.00632\n18.0\n2.31\n0\n0.538\n6.575\n65.2\n4.09\n1\n296\n15.3\n396.9\n4.98\n24.0\n\n\n2\n0.02731\n0.0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242\n17.8\n396.9\n9.14\n21.6\n\n\n3\n0.02729\n0.0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242\n17.8\n392.83\n4.03\n34.7\n\n\n4\n0.03237\n0.0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222\n18.7\n394.63\n2.94\n33.4\n\n\n5\n0.06905\n0.0\n2.18\n0\n0.458\n7.147\n54.2\n6.0622\n3\n222\n18.7\n396.9\n5.33\n36.2",
    "crumbs": [
      "Dataset",
      "3.boston housing"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "dataset/index.html",
    "href": "dataset/index.html",
    "title": "Make friends with data!",
    "section": "",
    "text": "first step to start data science is recognize your data only you get familiar with the data , then make flow work",
    "crumbs": [
      "Dataset",
      "intro"
    ]
  },
  {
    "objectID": "dataset/penguins.html",
    "href": "dataset/penguins.html",
    "title": "🐧🐧 Penguins dataset",
    "section": "",
    "text": "info\n\n\n\nPenguins data set is another famous dataset for data science, compare with Iris, Penguins has more categorical feature.\nplease reference :\n\nPalmer penguins website\n[Aog Tutorial 🐧]",
    "crumbs": [
      "Dataset",
      "2.penguins"
    ]
  },
  {
    "objectID": "dataset/penguins.html#load-csv",
    "href": "dataset/penguins.html#load-csv",
    "title": "🐧🐧 Penguins dataset",
    "section": "1. load csv",
    "text": "1. load csv\n\nusing CSV,DataFrames,Tidier\nusing CairoMakie,AlgebraOfGraphics,MakieThemes\nMakie.set_theme!(ggthemr(:dust))\n\ndf=CSV.File(\"../data/penguine_data.csv\")|&gt;DataFrame\nfirst(df,5)\n\n5×7 DataFrame\n\n\n\nRow\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\nString15\nString15\nString7\nString7\nString3\nString7\nString7\n\n\n\n\n1\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmale\n\n\n2\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nfemale\n\n\n3\nAdelie\nTorgersen\n40.3\n18\n195\n3250\nfemale\n\n\n4\nAdelie\nTorgersen\nNA\nNA\nNA\nNA\nNA\n\n\n5\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale",
    "crumbs": [
      "Dataset",
      "2.penguins"
    ]
  },
  {
    "objectID": "dataset/penguins.html#describe-dataframe",
    "href": "dataset/penguins.html#describe-dataframe",
    "title": "🐧🐧 Penguins dataset",
    "section": "2. describe dataframe",
    "text": "2. describe dataframe\n\ndescribe(df)\n\n7×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nNothing\nInlineSt…\nNothing\nInlineSt…\nInt64\nDataType\n\n\n\n\n1\nspecies\n\nAdelie\n\nGentoo\n0\nString15\n\n\n2\nisland\n\nBiscoe\n\nTorgersen\n0\nString15\n\n\n3\nbill_length_mm\n\n32.1\n\nNA\n0\nString7\n\n\n4\nbill_depth_mm\n\n13.1\n\nNA\n0\nString7\n\n\n5\nflipper_length_mm\n\n172\n\nNA\n0\nString3\n\n\n6\nbody_mass_g\n\n2700\n\nNA\n0\nString7\n\n\n7\nsex\n\nNA\n\nmale\n0\nString7",
    "crumbs": [
      "Dataset",
      "2.penguins"
    ]
  },
  {
    "objectID": "dataset/penguins.html#eda",
    "href": "dataset/penguins.html#eda",
    "title": "🐧🐧 Penguins dataset",
    "section": "3. EDA",
    "text": "3. EDA\n\n3.1 frequency\n\ndatalayer=data(df)\nmappinglayer=frequency()*mapping(:species,color = :island,dodge = :island)\naxis = (width = 225, height = 225)\ndraw(datalayer*mappinglayer,axis=axis)\n\n\n\n\n\n\n\nFigure 1: fig-penguins-fequency\n\n\n\n\n\n\n\n3.2 correlation of two variables\n\n#show(names(df))\ncats=[\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\nlen=length(cats)\ncats=Symbol.(cats)\n\n4-element Vector{Symbol}:\n :bill_length_mm\n :bill_depth_mm\n :flipper_length_mm\n :body_mass_g\n\n\n\npalette = (color=tuple.([\"#FC7808\", \"#8C00EC\", \"#107A78\"], 0.65),\n           marker=[:circle, :utriangle, :rect])\ncycle = Cycle([:color, :marker], covary=true)\n\nfunction make_layer(sy1::Symbol,sy2::Symbol)\n   mapping_layer=sy1==sy2 ? mapping(sy1,color=:species) : mapping(sy1,sy2,color=:species)\n   vis_layer=sy1==sy2 ? visual(Density,color=:species) : visual(Scatter,color=:species,strokewidth=1,storkecolor=:black)\n   return  datalayer*mapping_layer*vis_layer\nend\n\nwith_theme(ggthemr(:dust),resolution = (800,800), palette=palette, Scatter=(cycle=cycle,)) do\n        cl=0.1\n        fig = Figure()\n        for i in 1:len\n            for  j in 1:len\n                ax=Axis(fig[i,j])\n                j==1 ? ax.ylabel= String(cats[i]) : nothing\n                i==len ? ax.xlabel= String(cats[j]) : nothing\n                layer=make_layer(cats[i],cats[j])\n              [i,j]==[len,len] ? cl=draw!(ax,layer) : draw!(ax,layer)\n            end\n        end\n        legend!(fig[2:3,5], cl)\n        fig\nend\n\n\n\n\n\n\n\nFigure 2: fig-penguins-pairplot\n\n\n\n\n\n\n\n3.3 varying weights of the penguins\n\nax=(width = 225, height = 225)\nmappinglayer2=mapping(:species, :bill_depth_mm;color=:species)\nvislayer2=visual(BoxPlot,show_notch=true)\ndraw(datalayer*mappinglayer2*vislayer2,axis=ax)\n\n\n\n\n\n\n\nFigure 3: fig-penguins-bodymass",
    "crumbs": [
      "Dataset",
      "2.penguins"
    ]
  },
  {
    "objectID": "learn/Julia-MachineLearning.html",
    "href": "learn/Julia-MachineLearning.html",
    "title": "Julia-MachineLearning",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "learn/Julia-MachineLearning.html#quarto",
    "href": "learn/Julia-MachineLearning.html#quarto",
    "title": "Julia-MachineLearning",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "learn/index.html",
    "href": "learn/index.html",
    "title": "Leanring",
    "section": "",
    "text": "Learn",
    "crumbs": [
      "Learn"
    ]
  },
  {
    "objectID": "packages/index.html",
    "href": "packages/index.html",
    "title": "Packages",
    "section": "",
    "text": "List\n\nGLM.jl"
  },
  {
    "objectID": "start/index.html",
    "href": "start/index.html",
    "title": "Start",
    "section": "",
    "text": "import MLJ:evaluate,MLJInterface\nusing MLJ,DataFrames\niris=load_iris()|&gt;DataFrame\n\ny, X = unpack(iris, ==(:target); rng=123);\n\nfunction  build_model(X,y)\n    Tree = @load DecisionTreeClassifier pkg=DecisionTree\n    tree = Tree()\n    evaluate(tree, X, y, resampling=CV(shuffle=true),\n        measures=[log_loss, accuracy],\n        verbosity=0)\nend\n\nbuild_model(X,y)",
    "crumbs": [
      "Start",
      "GET STARTED"
    ]
  },
  {
    "objectID": "workflow/index.html",
    "href": "workflow/index.html",
    "title": "MLJ workflow",
    "section": "",
    "text": "graph TD\n    A[(Data)] --&gt;|Tidy Data| B(Ingestion)\n    B --&gt; C{Model Search}\n    C --&gt;|One| D[Regression]\n    C --&gt;|Two| E[Classification]\n    C --&gt;|Three| F[Dimension Reduction]\n    C --&gt;|Four| G[Ensemble Models]\n    C --&gt;|Five| H[Other Models]\n    D--&gt;J(Instantiating Model)\n    E--&gt;J\n    F--&gt;J\n    G--&gt;J\n    H--&gt;J\n    J--&gt;K(Evaluating Model)\n    K--&gt;L(Inspecting training results)\n    L--&gt;M(hyperparameter tuning)\n    M--&gt;N(Constructing linear pipelines)\n    N--&gt;O(ensemble of models) \n    O--&gt;P(Performance curves)\n\n\n\n\n\ngraph TD\n    A[(Data)] --&gt;|Tidy Data| B(Ingestion)\n    B --&gt; C{Model Search}\n    C --&gt;|One| D[Regression]\n    C --&gt;|Two| E[Classification]\n    C --&gt;|Three| F[Dimension Reduction]\n    C --&gt;|Four| G[Ensemble Models]\n    C --&gt;|Five| H[Other Models]\n    D--&gt;J(Instantiating Model)\n    E--&gt;J\n    F--&gt;J\n    G--&gt;J\n    H--&gt;J\n    J--&gt;K(Evaluating Model)\n    K--&gt;L(Inspecting training results)\n    L--&gt;M(hyperparameter tuning)\n    M--&gt;N(Constructing linear pipelines)\n    N--&gt;O(ensemble of models) \n    O--&gt;P(Performance curves)\n\n\n\n\nFigure 1: MLJ main work flow",
    "crumbs": [
      "Workflow",
      "flowchart"
    ]
  },
  {
    "objectID": "workflow/index.html#common-workflow",
    "href": "workflow/index.html#common-workflow",
    "title": "MLJ workflow",
    "section": "",
    "text": "graph TD\n    A[(Data)] --&gt;|Tidy Data| B(Ingestion)\n    B --&gt; C{Model Search}\n    C --&gt;|One| D[Regression]\n    C --&gt;|Two| E[Classification]\n    C --&gt;|Three| F[Dimension Reduction]\n    C --&gt;|Four| G[Ensemble Models]\n    C --&gt;|Five| H[Other Models]\n    D--&gt;J(Instantiating Model)\n    E--&gt;J\n    F--&gt;J\n    G--&gt;J\n    H--&gt;J\n    J--&gt;K(Evaluating Model)\n    K--&gt;L(Inspecting training results)\n    L--&gt;M(hyperparameter tuning)\n    M--&gt;N(Constructing linear pipelines)\n    N--&gt;O(ensemble of models) \n    O--&gt;P(Performance curves)\n\n\n\n\n\ngraph TD\n    A[(Data)] --&gt;|Tidy Data| B(Ingestion)\n    B --&gt; C{Model Search}\n    C --&gt;|One| D[Regression]\n    C --&gt;|Two| E[Classification]\n    C --&gt;|Three| F[Dimension Reduction]\n    C --&gt;|Four| G[Ensemble Models]\n    C --&gt;|Five| H[Other Models]\n    D--&gt;J(Instantiating Model)\n    E--&gt;J\n    F--&gt;J\n    G--&gt;J\n    H--&gt;J\n    J--&gt;K(Evaluating Model)\n    K--&gt;L(Inspecting training results)\n    L--&gt;M(hyperparameter tuning)\n    M--&gt;N(Constructing linear pipelines)\n    N--&gt;O(ensemble of models) \n    O--&gt;P(Performance curves)\n\n\n\n\nFigure 1: MLJ main work flow",
    "crumbs": [
      "Workflow",
      "flowchart"
    ]
  },
  {
    "objectID": "dataset/boston-house.html#describe-dataframe",
    "href": "dataset/boston-house.html#describe-dataframe",
    "title": "🏠🏡🏣 Boston Housing dataset",
    "section": "2. describe dataframe",
    "text": "2. describe dataframe\n\ndescribe(df)\n\n14×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nFloat64\nReal\nFloat64\nReal\nInt64\nDataType\n\n\n\n\n1\ncrim\n3.61352\n0.00632\n0.25651\n88.9762\n0\nFloat64\n\n\n2\nzn\n11.3636\n0.0\n0.0\n100.0\n0\nFloat64\n\n\n3\nindus\n11.1368\n0.46\n9.69\n27.74\n0\nFloat64\n\n\n4\nchas\n0.06917\n0\n0.0\n1\n0\nInt64\n\n\n5\nnox\n0.554695\n0.385\n0.538\n0.871\n0\nFloat64\n\n\n6\nrm\n6.28463\n3.561\n6.2085\n8.78\n0\nFloat64\n\n\n7\nage\n68.5749\n2.9\n77.5\n100.0\n0\nFloat64\n\n\n8\ndis\n3.79504\n1.1296\n3.20745\n12.1265\n0\nFloat64\n\n\n9\nrad\n9.54941\n1\n5.0\n24\n0\nInt64\n\n\n10\ntax\n408.237\n187\n330.0\n711\n0\nInt64\n\n\n11\nptratio\n18.4555\n12.6\n19.05\n22.0\n0\nFloat64\n\n\n12\nb\n356.674\n0.32\n391.44\n396.9\n0\nFloat64\n\n\n13\nlstat\n12.6531\n1.73\n11.36\n37.97\n0\nFloat64\n\n\n14\nmedv\n22.5328\n5.0\n21.2\n50.0\n0\nFloat64",
    "crumbs": [
      "Dataset",
      "3.boston housing"
    ]
  },
  {
    "objectID": "dataset/boston-house.html#eda",
    "href": "dataset/boston-house.html#eda",
    "title": "🏠🏡🏣 Boston Housing dataset",
    "section": "3 EDA",
    "text": "3 EDA\n\n3.1 CHA\ncharles river is a famous river around harvard univ and mit so , we want to know by ther river side affect house price\n\ndf1=@chain df begin\n   @group_by(chas)\n   @summarize(mean=mean(medv),median=median(medv))\n\nend\n\nresolution = (500, 250)\nfig = Figure(; resolution)\nax1 = Axis(fig[1, 1],xlabel=\"chas\",ylabel=\"mean\")\nax2 = Axis(fig[1, 2],xlabel=\"chas\",ylabel=\"median\")\n\ndatalayer1=data(df1)\nmappinglayer1=mapping(:chas,:mean, color=:chas) \nmappinglayer2=mapping(:chas,:median, color=:chas)\nvislayer=visual(BarPlot,bar_labels=:y,flip_labels_at=23)\n\nplt1 = datalayer1* mappinglayer1*vislayer\nplt2 = datalayer1* mappinglayer2*vislayer\ndraw!(ax1,plt1)\ngrid=draw!(ax2,plt2)\nlegend!(fig[1, 3], grid)\nfig\n\n\n\n\n\n\n\n\n\n\n3.2 Rad count\n\ndf2=@chain df begin\n    @group_by(rad)\n    @summarize(count=n())\n    @ungroup\nend\n\n9×2 DataFrame\n\n\n\nRow\nrad\ncount\n\n\n\nInt64\nInt64\n\n\n\n\n1\n1\n20\n\n\n2\n2\n24\n\n\n3\n3\n38\n\n\n4\n4\n110\n\n\n5\n5\n115\n\n\n6\n6\n26\n\n\n7\n7\n17\n\n\n8\n8\n24\n\n\n9\n24\n132\n\n\n\n\n\n\n\nlet \n    resolution = (400, 300)\n    fig = Figure(; resolution)\n    ax = Axis(fig[1, 1],xlabel=\"rad\",ylabel=\"count\")\n    \n    datalayer32=data(df2)\n    mappinglayer32=mapping(:rad,:count,color=:rad)\n    vislayer32=visual(BarPlot,bar_labels=:y,flip_labels_at=130)\n    plt=datalayer32*mappinglayer32*vislayer32\n    draw!(ax,plt)\n    fig\nend\n\n\n\n\n\n\n\n\n\n\n3.3 Rad price mean\n\ndf33=@chain df begin\n    @group_by(rad)\n    @summarize(mean=mean(medv))\n    @ungroup\nend\n\n9×2 DataFrame\n\n\n\nRow\nrad\nmean\n\n\n\nInt64\nFloat64\n\n\n\n\n1\n1\n24.365\n\n\n2\n2\n26.8333\n\n\n3\n3\n27.9289\n\n\n4\n4\n21.3873\n\n\n5\n5\n25.707\n\n\n6\n6\n20.9769\n\n\n7\n7\n27.1059\n\n\n8\n8\n30.3583\n\n\n9\n24\n16.4038\n\n\n\n\n\n\n\n\n3.4 medv density\n\ndatalayer34=data(df)\nmappinglayer34=mapping(:medv)\nvislayer341=visual(Density,color=(:lightgreen,0.6),strokewidth=1,strokecolor=:black)\nvislayer342=visual(Hist,strokewidth=1,strokecolor=:black,normalization = :pdf,color=(:red,0.5))\ndraw(datalayer34*mappinglayer34*(vislayer341+vislayer342))",
    "crumbs": [
      "Dataset",
      "3.boston housing"
    ]
  },
  {
    "objectID": "dataset/mtcar.html",
    "href": "dataset/mtcar.html",
    "title": "🚗🚕🚙 auto-mpg dataset",
    "section": "",
    "text": "using CSV,DataFrames,Tidier\nusing CairoMakie,AlgebraOfGraphics,MakieThemes\nusing StatsBase\nusing ScientificTypes\nMakie.set_theme!(ggthemr(:flat));\n\n\ndf=CSV.File(\"../data/auto-mpg.csv\")|&gt;DataFrame\ndf=@chain df begin\n   @clean_names\nend\nfirst(df,5)\n\n\n\n\n5×7 DataFrame\n\n\n\nRow\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nmodel_year\n\n\n\nFloat64\nInt64\nFloat64\nInt64?\nInt64\nFloat64\nInt64\n\n\n\n\n1\n18.0\n8\n307.0\n130\n3504\n12.0\n70\n\n\n2\n15.0\n8\n350.0\n165\n3693\n11.5\n70\n\n\n3\n18.0\n8\n318.0\n150\n3436\n11.0\n70\n\n\n4\n16.0\n8\n304.0\n150\n3433\n12.0\n70\n\n\n5\n17.0\n8\n302.0\n140\n3449\n10.5\n70\n\n\n\n\n\n\n\nTable 1: auto-mpg dataset",
    "crumbs": [
      "Dataset",
      "4.auto mpg"
    ]
  },
  {
    "objectID": "dataset/mtcar.html#load-csv",
    "href": "dataset/mtcar.html#load-csv",
    "title": "🚗🚕🚙 auto-mpg dataset",
    "section": "",
    "text": "using CSV,DataFrames,Tidier\nusing CairoMakie,AlgebraOfGraphics,MakieThemes\nusing StatsBase\nusing ScientificTypes\nMakie.set_theme!(ggthemr(:flat));\n\n\ndf=CSV.File(\"../data/auto-mpg.csv\")|&gt;DataFrame\ndf=@chain df begin\n   @clean_names\nend\nfirst(df,5)\n\n\n\n\n5×7 DataFrame\n\n\n\nRow\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nmodel_year\n\n\n\nFloat64\nInt64\nFloat64\nInt64?\nInt64\nFloat64\nInt64\n\n\n\n\n1\n18.0\n8\n307.0\n130\n3504\n12.0\n70\n\n\n2\n15.0\n8\n350.0\n165\n3693\n11.5\n70\n\n\n3\n18.0\n8\n318.0\n150\n3436\n11.0\n70\n\n\n4\n16.0\n8\n304.0\n150\n3433\n12.0\n70\n\n\n5\n17.0\n8\n302.0\n140\n3449\n10.5\n70\n\n\n\n\n\n\n\nTable 1: auto-mpg dataset",
    "crumbs": [
      "Dataset",
      "4.auto mpg"
    ]
  },
  {
    "objectID": "dataset/mtcar.html#describe-data",
    "href": "dataset/mtcar.html#describe-data",
    "title": "🚗🚕🚙 auto-mpg dataset",
    "section": "2. describe data",
    "text": "2. describe data\n\ndropmissing!(df)\n@show describe(df);\n\ndescribe(df) = 7×7 DataFrame\n Row │ variable      mean        min     median   max     nmissing  eltype\n     │ Symbol        Float64     Real    Float64  Real    Int64     DataType\n─────┼───────────────────────────────────────────────────────────────────────\n   1 │ mpg             23.5172      9.0     23.0    46.6         0  Float64\n   2 │ cylinders        5.45707     3        4.0     8           0  Int64\n   3 │ displacement   193.65       68.0    148.5   455.0         0  Float64\n   4 │ horsepower     104.189      46       92.0   230           0  Int64\n   5 │ weight        2973.0      1613     2803.5  5140           0  Int64\n   6 │ acceleration    15.5558      8.0     15.5    24.8         0  Float64\n   7 │ model_year      76.0278     70       76.0    82           0  Int64",
    "crumbs": [
      "Dataset",
      "4.auto mpg"
    ]
  },
  {
    "objectID": "dataset/mtcar.html#heatmap-of-variables",
    "href": "dataset/mtcar.html#heatmap-of-variables",
    "title": "🚗🚕🚙 auto-mpg dataset",
    "section": "3. heatmap of variables",
    "text": "3. heatmap of variables\n\ndf_cor = df|&gt;Matrix|&gt;cor.|&gt; d -&gt; round(d, digits=2)\nlabels=names(df)\nfunction plot_cov_cor()\n    fig = Figure(resolution=(800, 400)) \n    ax1 = Axis(fig[1, 1]; xticks=(1:7, labels), yticks=(1:7, labels), title=\"corr of mpg variables\",\n    xticklabelrotation = pi/8,\n    yreversed=true)\n    hm = heatmap!(ax1, df_cor)\n    Colorbar(fig[1, 2], hm)\n    [text!(ax1, x, y; text=string(df_cor[x, y]), color=:white, fontsize=18, align=(:center, :center)) for x in 1:7, y in 1:7]\n    fig\nend\nplot_cov_cor()\n\n\n\n\n\n\n\nFigure 1: fig-automag-coorlation",
    "crumbs": [
      "Dataset",
      "4.auto mpg"
    ]
  },
  {
    "objectID": "dataset/mtcar.html#eda",
    "href": "dataset/mtcar.html#eda",
    "title": "🚗🚕🚙 auto-mpg dataset",
    "section": "3. EDA",
    "text": "3. EDA\n\n3.1 heatmap of variables\n\ndf_cor = df|&gt;Matrix|&gt;cor.|&gt; d -&gt; round(d, digits=2)\nlabels=names(df)\nfunction plot_cov_cor()\n    fig = Figure(resolution=(800, 400)) \n    ax1 = Axis(fig[1, 1]; xticks=(1:7, labels), yticks=(1:7, labels), title=\"corr of mpg variables\",\n    xticklabelrotation = pi/8,\n    yreversed=true)\n    hm = heatmap!(ax1, df_cor)\n    Colorbar(fig[1, 2], hm)\n    [text!(ax1, x, y; text=string(df_cor[x, y]), color=:white, fontsize=18, align=(:center, :center)) for x in 1:7, y in 1:7]\n    fig\nend\nplot_cov_cor()\n\n\n\n\n\n\n\nFigure 1: fig-automag-coorlation\n\n\n\n\n\n\n\n3.2 Univariate Analysis\n\n3.2.1 cylinders data\nplot_univariate plot univariate count\n\n    function plot_univariate(df::AbstractDataFrame,feature1::Symbol,feature2::Symbol,cats::Symbol)\n    ax=(width = 225, height = 225)\n    data_layer=data(df)\n    mappinglayer=mapping(feature1,feature2,color=cats)\n    vislayer=visual(BarPlot,bar_labels=:y,flip_labels_at=130)\n    plt=data_layer*mappinglayer*vislayer\n        draw(plt,axis=ax)\n    end\n\nplot_univariate (generic function with 1 method)\n\n\n\ndf321=@chain df begin\n    @group_by(cylinders)\n    @summarize(count=n())\n    @ungroup\nend\nplot_univariate(df321,:cylinders,:count,:cylinders)\n\n\n\n\n\n\n\n\n\n\n3.2.2 model_year count\n\ndf322=@chain df begin\n    @group_by(model_year)\n    @summarize(count=n())\n    @ungroup\nend\n\nplot_univariate(df322,:model_year,:count,:model_year)\n\n\n\n\n\n\n\n\n\n\n3.2.3 density of horsepower\n\nax=(width = 400, height = 300)\ndatalayer323=data(df)\nmappinglayer323=mapping(:horsepower)\nvislayer3231=visual(AlgebraOfGraphics.Density,color=(:lightgreen,0.6),strokewidth=1,strokecolor=:black)\nvislayer3232=visual(AlgebraOfGraphics.Hist,strokewidth=1,strokecolor=:black,normalization = :pdf,color=(:red,0.5))\ndraw(datalayer323*mappinglayer323*(vislayer3231+vislayer3232),axis=ax)\n\n\n\n\n\n\n\nFigure 2: fig-automag-horsepower-density\n\n\n\n\n\n\n\n\n3.3 multivariate analysis\n\n3.3.1 mpg by cylinders\n\nlet\n    ax=(width =250, height = 250)\n    datalayer=data(df)\n    mappinglayer=mapping(:cylinders,:mpg,color=:cylinders)\n    vislayer=visual(BoxPlot)\n   data(df) * visual(BoxPlot) *\n    mapping(:cylinders, :mpg, color=:cylinders) |&gt;d-&gt;draw(d,axis=ax)\nend\n\n\n\n\n\n\n\n\n\n\n3.3.2 mpg by model_year\n\nlet\n    ax=(width =250, height = 250)\n    data(df) * visual(BoxPlot) *\n    mapping(:model_year, :mpg, color=:model_year) |&gt;d-&gt;draw(d,axis=ax)\nend\n\n\n\n\n\n\n\n\n\n\n3.3.3 horsepower-mpg relatiion\n\nax=(width =250, height = 250)\n plt1 = data(df)*mapping(:horsepower,:mpg) * linear()\n #plt2 = data(df)*mapping(:horsepower,:mpg) \n draw(plt1, axis=ax)",
    "crumbs": [
      "Dataset",
      "4.auto mpg"
    ]
  },
  {
    "objectID": "workflow/mlj-cheatsheet.html",
    "href": "workflow/mlj-cheatsheet.html",
    "title": "MLJ Cheatsheet",
    "section": "",
    "text": "Special title treatment\n\n\njulia&gt; using MLJ\njulia&gt; MLJ_VERSION # version of MLJ for this cheatsheet\nv\"0.20.2\"",
    "crumbs": [
      "Workflow",
      "mlj cheatsheet"
    ]
  },
  {
    "objectID": "workflow/mlj-cheatsheet.html#starting-an-interactive-mlj-session",
    "href": "workflow/mlj-cheatsheet.html#starting-an-interactive-mlj-session",
    "title": "MLJ Cheatsheet",
    "section": "",
    "text": "Special title treatment\n\n\njulia&gt; using MLJ\njulia&gt; MLJ_VERSION # version of MLJ for this cheatsheet\nv\"0.20.2\"",
    "crumbs": [
      "Workflow",
      "mlj cheatsheet"
    ]
  },
  {
    "objectID": "workflow/mlj-cheatsheet.html#model-search",
    "href": "workflow/mlj-cheatsheet.html#model-search",
    "title": "MLJ Cheatsheet",
    "section": "model search",
    "text": "model search\n\n\nmodel search\n\n\n\ninfo(\"RidgeRegressor\", pkg=\"MultivariateStats\")\n\n\ndoc(\"DecisionTreeClassifier\", pkg=\"DecisionTree\")\n\n\nmodels()\n\n\nmodels(Tree)\n\n\nmodels(matching(X))\n\n\nmodels(matching(X, y))",
    "crumbs": [
      "Workflow",
      "mlj cheatsheet"
    ]
  },
  {
    "objectID": "workflow/mlj-cheatsheet.html#scitypes-and-coercion",
    "href": "workflow/mlj-cheatsheet.html#scitypes-and-coercion",
    "title": "MLJ Cheatsheet",
    "section": "Scitypes and coercion",
    "text": "Scitypes and coercion\n\n\n\nScitypes and coercion\n\n\n\n\nschema(X)\n\n\ncoerce(y, Multiclass)\n\n\ncoerce(X, :x1 =&gt; Continuous, :x2 =&gt; OrderedFactor)\n\n\ncoerce(X, Count =&gt; Continuous)",
    "crumbs": [
      "Workflow",
      "mlj cheatsheet"
    ]
  },
  {
    "objectID": "workflow/mlj-cheatsheet.html#ingesting-data",
    "href": "workflow/mlj-cheatsheet.html#ingesting-data",
    "title": "MLJ Cheatsheet",
    "section": "Ingesting data",
    "text": "Ingesting data\n\n\n\nIngesting data\n\n\n\n\n\nusing RDatasets\nchanning = dataset(\"boot\", \"channing\")\ny, X =  unpack(channing, ==(:Exit); rng=123)\n\n\ntrain, valid, test = partition(eachindex(y), 0.7, 0.2, rng=1234) for 70:20:10 ratio\n\n\nXtrain, Xvalid, Xtest = partition(X, 0.5, 0.3, rng=123)\n\n\nX, y = make_blobs(100, 2) (also: make_moons, make_circles)\n\n\nX, y = make_regression(100, 2)",
    "crumbs": [
      "Workflow",
      "mlj cheatsheet"
    ]
  },
  {
    "objectID": "workflow/mlj-cheatsheet.html#blockquote",
    "href": "workflow/mlj-cheatsheet.html#blockquote",
    "title": "MLJ Cheatsheet",
    "section": "blockquote",
    "text": "blockquote\n\n\nQuote\n\n\n\n\nA well-known quote, contained in a blockquote element.\n\n\nSomeone famous in Source Title",
    "crumbs": [
      "Workflow",
      "mlj cheatsheet"
    ]
  },
  {
    "objectID": "workflow/mlj-cheatsheet.html#machine-co",
    "href": "workflow/mlj-cheatsheet.html#machine-co",
    "title": "MLJ Cheatsheet",
    "section": "Machine Co",
    "text": "Machine Co",
    "crumbs": [
      "Workflow",
      "mlj cheatsheet"
    ]
  },
  {
    "objectID": "workflow/mlj-cheatsheet.html#machine-construct",
    "href": "workflow/mlj-cheatsheet.html#machine-construct",
    "title": "MLJ Cheatsheet",
    "section": "Machine Construct",
    "text": "Machine Construct",
    "crumbs": [
      "Workflow",
      "mlj cheatsheet"
    ]
  },
  {
    "objectID": "workflow/mlj-cheatsheet.html#machine-construction",
    "href": "workflow/mlj-cheatsheet.html#machine-construction",
    "title": "MLJ Cheatsheet",
    "section": "Machine Construction",
    "text": "Machine Construction\n\n\n\nMachine Construction\n\n\n\n\nmodel = KNNRegressor(K=1) and mach = machine(model, X, y)\n\n\nmodel = OneHotEncoder() and mach = machine(model, X)",
    "crumbs": [
      "Workflow",
      "mlj cheatsheet"
    ]
  },
  {
    "objectID": "workflow/mlj-cheatsheet.html#fitting",
    "href": "workflow/mlj-cheatsheet.html#fitting",
    "title": "MLJ Cheatsheet",
    "section": "fitting",
    "text": "fitting\n\n\n\nfitting\n\n\n\n\n  fit!(mach, rows=1:100, verbosity=1, force=false) (defaults shown)",
    "crumbs": [
      "Workflow",
      "mlj cheatsheet"
    ]
  },
  {
    "objectID": "learn/ensemble-model.html",
    "href": "learn/ensemble-model.html",
    "title": "mlj Ensemble model",
    "section": "",
    "text": "using MLJ\nimport DataFrames: DataFrame\nusing PrettyPrinting\nusing StableRNGs\nusing CairoMakie,AlgebraOfGraphics,MakieThemes\nrng = StableRNG(512)\n\nStableRNGs.LehmerRNG(state=0x00000000000000000000000000000401)",
    "crumbs": [
      "Learn",
      "ensemble model"
    ]
  },
  {
    "objectID": "learn/ensemble-model.html#loading-package",
    "href": "learn/ensemble-model.html#loading-package",
    "title": "mlj Ensemble model",
    "section": "",
    "text": "using MLJ\nimport DataFrames: DataFrame\nusing PrettyPrinting\nusing StableRNGs\nusing CairoMakie,AlgebraOfGraphics,MakieThemes\nrng = StableRNG(512)\n\nStableRNGs.LehmerRNG(state=0x00000000000000000000000000000401)",
    "crumbs": [
      "Learn",
      "ensemble model"
    ]
  },
  {
    "objectID": "learn/ensemble-model.html#make-data",
    "href": "learn/ensemble-model.html#make-data",
    "title": "mlj Ensemble model",
    "section": "2. make data",
    "text": "2. make data\n\nXraw = rand(rng, 300, 3)\ny = exp.(Xraw[:,1] - Xraw[:,2] - 2Xraw[:,3] + 0.1*rand(rng, 300))\nX = DataFrame(Xraw, :auto)\ntrain, test = partition(eachindex(y), 0.7);",
    "crumbs": [
      "Learn",
      "ensemble model"
    ]
  },
  {
    "objectID": "learn/ensemble-model.html#load",
    "href": "learn/ensemble-model.html#load",
    "title": "mlj Ensemble model",
    "section": "3. load",
    "text": "3. load",
    "crumbs": [
      "Learn",
      "ensemble model"
    ]
  },
  {
    "objectID": "learn/ensemble-model.html#load-machine-model",
    "href": "learn/ensemble-model.html#load-machine-model",
    "title": "mlj Ensemble model",
    "section": "3. load machine model",
    "text": "3. load machine model\n\nKNNRegressor = @load KNNRegressor\nknn_model = KNNRegressor(K=10)\n\nimport NearestNeighborModels ✔\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n\n\nKNNRegressor(\n  K = 10, \n  algorithm = :kdtree, \n  metric = Distances.Euclidean(0.0), \n  leafsize = 10, \n  reorder = true, \n  weights = NearestNeighborModels.Uniform())",
    "crumbs": [
      "Learn",
      "ensemble model"
    ]
  },
  {
    "objectID": "learn/ensemble-model.html#section",
    "href": "learn/ensemble-model.html#section",
    "title": "mlj Ensemble model",
    "section": "4",
    "text": "4",
    "crumbs": [
      "Learn",
      "ensemble model"
    ]
  },
  {
    "objectID": "learn/ensemble-model.html#instantiate-model",
    "href": "learn/ensemble-model.html#instantiate-model",
    "title": "mlj Ensemble model",
    "section": "4. instantiate model",
    "text": "4. instantiate model\n\nknn=machine(knn_model,X,y)\n\nuntrained Machine; caches model-specific representations of data\n  model: KNNRegressor(K = 10, …)\n  args: \n    1:  Source @472 ⏎ Table{AbstractVector{Continuous}}\n    2:  Source @452 ⏎ AbstractVector{Continuous}",
    "crumbs": [
      "Learn",
      "ensemble model"
    ]
  },
  {
    "objectID": "learn/ensemble-model.html#fit-model",
    "href": "learn/ensemble-model.html#fit-model",
    "title": "mlj Ensemble model",
    "section": "5. fit model",
    "text": "5. fit model\n\nfit!(knn, rows=train)\n\n[ Info: Training machine(KNNRegressor(K = 10, …), …).\n\n\ntrained Machine; caches model-specific representations of data\n  model: KNNRegressor(K = 10, …)\n  args: \n    1:  Source @646 ⏎ Table{AbstractVector{Continuous}}\n    2:  Source @252 ⏎ AbstractVector{Continuous}\n\n\npreidict\n\nŷ = predict(knn, rows=test)\n\n90-element Vector{Float64}:\n 0.34467109322355544\n 0.47021416109338776\n 0.3605963245828601\n 0.7952383260523377\n 0.8175983630884535\n 0.2787449628967793\n 1.376995501664209\n 0.6335451619415926\n 0.24132363918064742\n 0.48242991564636817\n 1.006001124602383\n 0.4619208719838038\n 0.21005488086190915\n ⋮\n 0.17746741478027778\n 1.26371952798802\n 0.8821595000833542\n 1.3986725366131116\n 0.33686536328173955\n 0.28638455204396673\n 0.5602321105178957\n 0.8161827139978965\n 0.1998143885279288\n 0.8256665297053332\n 0.44366704202674506\n 0.7669517046982802\n\n\nevaluate quality\n\nrms(ŷ, y[test])\n\n0.06389980172436369",
    "crumbs": [
      "Learn",
      "ensemble model"
    ]
  },
  {
    "objectID": "learn/ensemble-model.html#single-model",
    "href": "learn/ensemble-model.html#single-model",
    "title": "mlj Ensemble model",
    "section": "3. single model",
    "text": "3. single model\n\n3.1 load machine model\n\nKNNRegressor = @load KNNRegressor\nknn_model = KNNRegressor(K=10)\n\nimport NearestNeighborModels ✔\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n\n\nKNNRegressor(\n  K = 10, \n  algorithm = :kdtree, \n  metric = Distances.Euclidean(0.0), \n  leafsize = 10, \n  reorder = true, \n  weights = NearestNeighborModels.Uniform())\n\n\n\n\n3.2 instantiate model\n\nknn=machine(knn_model,X,y)\n\nuntrained Machine; caches model-specific representations of data\n  model: KNNRegressor(K = 10, …)\n  args: \n    1:  Source @724 ⏎ Table{AbstractVector{Continuous}}\n    2:  Source @606 ⏎ AbstractVector{Continuous}\n\n\n\n\n3.3 fit model\n\nfit!(knn, rows=train)\n\n[ Info: Training machine(KNNRegressor(K = 10, …), …).\n\n\ntrained Machine; caches model-specific representations of data\n  model: KNNRegressor(K = 10, …)\n  args: \n    1:  Source @724 ⏎ Table{AbstractVector{Continuous}}\n    2:  Source @606 ⏎ AbstractVector{Continuous}\n\n\npreidict\n\nŷ = predict(knn, rows=test)\n\n90-element Vector{Float64}:\n 0.34467109322355544\n 0.47021416109338776\n 0.3605963245828601\n 0.7952383260523377\n 0.8175983630884535\n 0.2787449628967793\n 1.376995501664209\n 0.6335451619415926\n 0.24132363918064742\n 0.48242991564636817\n 1.006001124602383\n 0.4619208719838038\n 0.21005488086190915\n ⋮\n 0.17746741478027778\n 1.26371952798802\n 0.8821595000833542\n 1.3986725366131116\n 0.33686536328173955\n 0.28638455204396673\n 0.5602321105178957\n 0.8161827139978965\n 0.1998143885279288\n 0.8256665297053332\n 0.44366704202674506\n 0.7669517046982802\n\n\nevaluate quality\n\nrms(ŷ, y[test])\n\n0.06389980172436369",
    "crumbs": [
      "Learn",
      "ensemble model"
    ]
  },
  {
    "objectID": "learn/ensemble-model.html#homogengous-models",
    "href": "learn/ensemble-model.html#homogengous-models",
    "title": "mlj Ensemble model",
    "section": "4 homogengous models",
    "text": "4 homogengous models",
    "crumbs": [
      "Learn",
      "ensemble model"
    ]
  },
  {
    "objectID": "learn/ensemble-model.html#homogengous-ens",
    "href": "learn/ensemble-model.html#homogengous-ens",
    "title": "mlj Ensemble model",
    "section": "4 homogengous ens",
    "text": "4 homogengous ens",
    "crumbs": [
      "Learn",
      "ensemble model"
    ]
  },
  {
    "objectID": "learn/ensemble-model.html#homogengous-ensembles",
    "href": "learn/ensemble-model.html#homogengous-ensembles",
    "title": "mlj Ensemble model",
    "section": "4 homogengous ensembles",
    "text": "4 homogengous ensembles\n\nensemble_model = EnsembleModel(model=knn_model, n=20);\n\n\nensemble = machine(ensemble_model, X, y)\n\nuntrained Machine; caches model-specific representations of data\n  model: DeterministicEnsembleModel(model = KNNRegressor(K = 10, …), …)\n  args: \n    1:  Source @720 ⏎ Table{AbstractVector{Continuous}}\n    2:  Source @516 ⏎ AbstractVector{Continuous}\n\n\nevaluate ensemble models\n\nestimates = evaluate!(ensemble, resampling=CV())\n\n\nPerformanceEvaluation object with these fields:\n  model, measure, operation, measurement, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_rows, resampling, repeats\nExtract:\n┌──────────┬───────────┬─────────────┬─────────┬────────────────────────────────\n│ measure  │ operation │ measurement │ 1.96*SE │ per_fold                      ⋯\n├──────────┼───────────┼─────────────┼─────────┼────────────────────────────────\n│ LPLoss(  │ predict   │ 0.00725     │ 0.00277 │ [0.00786, 0.0127, 0.0054, 0.0 ⋯\n│   p = 2) │           │             │         │                               ⋯\n└──────────┴───────────┴─────────────┴─────────┴────────────────────────────────\n                                                                1 column omitted\n\n\n\n\n\n4.1 Systme tuning Params\nfirst show params:\n\nparams(ensemble_model) |&gt; pprint\n\n(model = (K = 10,\n          algorithm = :kdtree,\n          metric = Distances.Euclidean(0.0),\n          leafsize = 10,\n          reorder = true,\n          weights = NearestNeighborModels.Uniform()),\n atomic_weights = [],\n bagging_fraction = 0.8,\n rng = Random._GLOBAL_RNG(),\n n = 20,\n acceleration = CPU1{Nothing}(nothing),\n out_of_bag_measure = [])\n\n\nconstruct range of params\n\nB_range = range(ensemble_model, :bagging_fraction,\n                lower=0.5, upper=1.0)\nK_range = range(ensemble_model, :(model.K),\n                lower=1, upper=20)\n\nNumericRange(1 ≤ model.K ≤ 20; origin=10.5, unit=9.5)\n\n\nconstruct tune models\n\ntm = TunedModel(model=ensemble_model,\n                tuning=Grid(resolution=10), # 10x10 grid\n                resampling=Holdout(fraction_train=0.8, rng=StableRNG(42)),\n                ranges=[B_range, K_range])\n\n[ Info: No measure specified. Setting measure=LPLoss(p = 2). \n\n\nDeterministicTunedModel(\n  model = DeterministicEnsembleModel(\n        model = KNNRegressor(K = 10, …), \n        atomic_weights = Float64[], \n        bagging_fraction = 0.8, \n        rng = Random._GLOBAL_RNG(), \n        n = 20, \n        acceleration = CPU1{Nothing}(nothing), \n        out_of_bag_measure = Any[]), \n  tuning = Grid(\n        goal = nothing, \n        resolution = 10, \n        shuffle = true, \n        rng = Random._GLOBAL_RNG()), \n  resampling = Holdout(\n        fraction_train = 0.8, \n        shuffle = true, \n        rng = StableRNGs.LehmerRNG(state=0x00000000000000000000000000000055)), \n  measure = LPLoss(p = 2), \n  weights = nothing, \n  class_weights = nothing, \n  operation = nothing, \n  range = MLJBase.NumericRange{T, MLJBase.Bounded, Symbol} where T[NumericRange(0.5 ≤ bagging_fraction ≤ 1.0; origin=0.75, unit=0.25), NumericRange(1 ≤ model.K ≤ 20; origin=10.5, unit=9.5)], \n  selection_heuristic = MLJTuning.NaiveSelection(nothing), \n  train_best = true, \n  repeats = 1, \n  n = nothing, \n  acceleration = CPU1{Nothing}(nothing), \n  acceleration_resampling = CPU1{Nothing}(nothing), \n  check_measure = true, \n  cache = true)\n\n\nfit model\n\ntuned_ensemble = machine(tm, X, y)\nfit!(tuned_ensemble, rows=train);\n\n[ Info: Training machine(DeterministicTunedModel(model = DeterministicEnsembleModel(model = KNNRegressor(K = 10, …), …), …), …).\n[ Info: Attempting to evaluate 100 models.\nEvaluating over 100 metamodels:   0%[&gt;                        ]  ETA: N/AEvaluating over 100 metamodels:   1%[&gt;                        ]  ETA: 0:00:00Evaluating over 100 metamodels:   2%[&gt;                        ]  ETA: 0:00:00Evaluating over 100 metamodels:   3%[&gt;                        ]  ETA: 0:00:00Evaluating over 100 metamodels:   4%[=&gt;                       ]  ETA: 0:00:00Evaluating over 100 metamodels:   5%[=&gt;                       ]  ETA: 0:00:00Evaluating over 100 metamodels:   6%[=&gt;                       ]  ETA: 0:00:00Evaluating over 100 metamodels:   7%[=&gt;                       ]  ETA: 0:00:00Evaluating over 100 metamodels:   8%[==&gt;                      ]  ETA: 0:00:00Evaluating over 100 metamodels:   9%[==&gt;                      ]  ETA: 0:00:00Evaluating over 100 metamodels:  10%[==&gt;                      ]  ETA: 0:00:00Evaluating over 100 metamodels:  11%[==&gt;                      ]  ETA: 0:00:00Evaluating over 100 metamodels:  12%[===&gt;                     ]  ETA: 0:00:00Evaluating over 100 metamodels:  13%[===&gt;                     ]  ETA: 0:00:00Evaluating over 100 metamodels:  14%[===&gt;                     ]  ETA: 0:00:00Evaluating over 100 metamodels:  15%[===&gt;                     ]  ETA: 0:00:00Evaluating over 100 metamodels:  16%[====&gt;                    ]  ETA: 0:00:00Evaluating over 100 metamodels:  17%[====&gt;                    ]  ETA: 0:00:00Evaluating over 100 metamodels:  18%[====&gt;                    ]  ETA: 0:00:00Evaluating over 100 metamodels:  19%[====&gt;                    ]  ETA: 0:00:00Evaluating over 100 metamodels:  20%[=====&gt;                   ]  ETA: 0:00:00Evaluating over 100 metamodels:  21%[=====&gt;                   ]  ETA: 0:00:00Evaluating over 100 metamodels:  22%[=====&gt;                   ]  ETA: 0:00:00Evaluating over 100 metamodels:  23%[=====&gt;                   ]  ETA: 0:00:00Evaluating over 100 metamodels:  24%[======&gt;                  ]  ETA: 0:00:00Evaluating over 100 metamodels:  25%[======&gt;                  ]  ETA: 0:00:00Evaluating over 100 metamodels:  26%[======&gt;                  ]  ETA: 0:00:00Evaluating over 100 metamodels:  27%[======&gt;                  ]  ETA: 0:00:00Evaluating over 100 metamodels:  28%[=======&gt;                 ]  ETA: 0:00:00Evaluating over 100 metamodels:  29%[=======&gt;                 ]  ETA: 0:00:00Evaluating over 100 metamodels:  30%[=======&gt;                 ]  ETA: 0:00:00Evaluating over 100 metamodels:  31%[=======&gt;                 ]  ETA: 0:00:00Evaluating over 100 metamodels:  32%[========&gt;                ]  ETA: 0:00:00Evaluating over 100 metamodels:  33%[========&gt;                ]  ETA: 0:00:00Evaluating over 100 metamodels:  34%[========&gt;                ]  ETA: 0:00:00Evaluating over 100 metamodels:  35%[========&gt;                ]  ETA: 0:00:00Evaluating over 100 metamodels:  36%[=========&gt;               ]  ETA: 0:00:00Evaluating over 100 metamodels:  37%[=========&gt;               ]  ETA: 0:00:00Evaluating over 100 metamodels:  38%[=========&gt;               ]  ETA: 0:00:00Evaluating over 100 metamodels:  39%[=========&gt;               ]  ETA: 0:00:00Evaluating over 100 metamodels:  40%[==========&gt;              ]  ETA: 0:00:00Evaluating over 100 metamodels:  41%[==========&gt;              ]  ETA: 0:00:00Evaluating over 100 metamodels:  42%[==========&gt;              ]  ETA: 0:00:00Evaluating over 100 metamodels:  43%[==========&gt;              ]  ETA: 0:00:00Evaluating over 100 metamodels:  44%[===========&gt;             ]  ETA: 0:00:00Evaluating over 100 metamodels:  45%[===========&gt;             ]  ETA: 0:00:00Evaluating over 100 metamodels:  46%[===========&gt;             ]  ETA: 0:00:00Evaluating over 100 metamodels:  47%[===========&gt;             ]  ETA: 0:00:00Evaluating over 100 metamodels:  48%[============&gt;            ]  ETA: 0:00:00Evaluating over 100 metamodels:  49%[============&gt;            ]  ETA: 0:00:00Evaluating over 100 metamodels:  50%[============&gt;            ]  ETA: 0:00:00Evaluating over 100 metamodels:  51%[============&gt;            ]  ETA: 0:00:00Evaluating over 100 metamodels:  52%[=============&gt;           ]  ETA: 0:00:00Evaluating over 100 metamodels:  53%[=============&gt;           ]  ETA: 0:00:00Evaluating over 100 metamodels:  54%[=============&gt;           ]  ETA: 0:00:00Evaluating over 100 metamodels:  55%[=============&gt;           ]  ETA: 0:00:00Evaluating over 100 metamodels:  56%[==============&gt;          ]  ETA: 0:00:00Evaluating over 100 metamodels:  57%[==============&gt;          ]  ETA: 0:00:00Evaluating over 100 metamodels:  58%[==============&gt;          ]  ETA: 0:00:00Evaluating over 100 metamodels:  59%[==============&gt;          ]  ETA: 0:00:00Evaluating over 100 metamodels:  60%[===============&gt;         ]  ETA: 0:00:00Evaluating over 100 metamodels:  61%[===============&gt;         ]  ETA: 0:00:00Evaluating over 100 metamodels:  62%[===============&gt;         ]  ETA: 0:00:00Evaluating over 100 metamodels:  63%[===============&gt;         ]  ETA: 0:00:00Evaluating over 100 metamodels:  64%[================&gt;        ]  ETA: 0:00:00Evaluating over 100 metamodels:  65%[================&gt;        ]  ETA: 0:00:00Evaluating over 100 metamodels:  66%[================&gt;        ]  ETA: 0:00:00Evaluating over 100 metamodels:  67%[================&gt;        ]  ETA: 0:00:00Evaluating over 100 metamodels:  68%[=================&gt;       ]  ETA: 0:00:00Evaluating over 100 metamodels:  69%[=================&gt;       ]  ETA: 0:00:00Evaluating over 100 metamodels:  70%[=================&gt;       ]  ETA: 0:00:00Evaluating over 100 metamodels:  71%[=================&gt;       ]  ETA: 0:00:00Evaluating over 100 metamodels:  72%[==================&gt;      ]  ETA: 0:00:00Evaluating over 100 metamodels:  73%[==================&gt;      ]  ETA: 0:00:00Evaluating over 100 metamodels:  74%[==================&gt;      ]  ETA: 0:00:00Evaluating over 100 metamodels:  75%[==================&gt;      ]  ETA: 0:00:00Evaluating over 100 metamodels:  76%[===================&gt;     ]  ETA: 0:00:00Evaluating over 100 metamodels:  77%[===================&gt;     ]  ETA: 0:00:00Evaluating over 100 metamodels:  78%[===================&gt;     ]  ETA: 0:00:00Evaluating over 100 metamodels:  79%[===================&gt;     ]  ETA: 0:00:00Evaluating over 100 metamodels:  80%[====================&gt;    ]  ETA: 0:00:00Evaluating over 100 metamodels:  81%[====================&gt;    ]  ETA: 0:00:00Evaluating over 100 metamodels:  82%[====================&gt;    ]  ETA: 0:00:00Evaluating over 100 metamodels:  83%[====================&gt;    ]  ETA: 0:00:00Evaluating over 100 metamodels:  84%[=====================&gt;   ]  ETA: 0:00:00Evaluating over 100 metamodels:  85%[=====================&gt;   ]  ETA: 0:00:00Evaluating over 100 metamodels:  86%[=====================&gt;   ]  ETA: 0:00:00Evaluating over 100 metamodels:  87%[=====================&gt;   ]  ETA: 0:00:00Evaluating over 100 metamodels:  88%[======================&gt;  ]  ETA: 0:00:00Evaluating over 100 metamodels:  89%[======================&gt;  ]  ETA: 0:00:00Evaluating over 100 metamodels:  90%[======================&gt;  ]  ETA: 0:00:00Evaluating over 100 metamodels:  91%[======================&gt;  ]  ETA: 0:00:00Evaluating over 100 metamodels:  92%[=======================&gt; ]  ETA: 0:00:00Evaluating over 100 metamodels:  93%[=======================&gt; ]  ETA: 0:00:00Evaluating over 100 metamodels:  94%[=======================&gt; ]  ETA: 0:00:00Evaluating over 100 metamodels:  95%[=======================&gt; ]  ETA: 0:00:00Evaluating over 100 metamodels:  96%[========================&gt;]  ETA: 0:00:00Evaluating over 100 metamodels:  97%[========================&gt;]  ETA: 0:00:00Evaluating over 100 metamodels:  98%[========================&gt;]  ETA: 0:00:00Evaluating over 100 metamodels:  99%[========================&gt;]  ETA: 0:00:00Evaluating over 100 metamodels: 100%[=========================] Time: 0:00:00\n\n\n\n\n4.2 report tunning results\n\nbest_ensemble = fitted_params(tuned_ensemble).best_model\n@show best_ensemble.model.K\n@show best_ensemble.bagging_fraction\n\nbest_ensemble.model.K = 3\nbest_ensemble.bagging_fraction = 0.6111111111111112\n\n\n0.6111111111111112\n\n\nget detail report\n\nr = report(tuned_ensemble)\n\n(best_model = DeterministicEnsembleModel(model = KNNRegressor(K = 3, …), …),\n best_history_entry = (model = DeterministicEnsembleModel(model = KNNRegressor(K = 3, …), …),\n                       measure = StatisticalMeasuresBase.RobustMeasure{StatisticalMeasuresBase.FussyMeasure{StatisticalMeasuresBase.RobustMeasure{StatisticalMeasuresBase.Multimeasure{StatisticalMeasuresBase.SupportsMissingsMeasure{StatisticalMeasures.LPLossOnScalars{Int64}}, Nothing, StatisticalMeasuresBase.Mean, typeof(identity)}}, Nothing}}[LPLoss(p = 2)],\n                       measurement = [0.0014622515676628268],\n                       per_fold = [[0.0014622515676628268]],),\n history = NamedTuple{(:model, :measure, :measurement, :per_fold), Tuple{MLJEnsembles.DeterministicEnsembleModel{NearestNeighborModels.KNNRegressor}, Vector{StatisticalMeasuresBase.RobustMeasure{StatisticalMeasuresBase.FussyMeasure{StatisticalMeasuresBase.RobustMeasure{StatisticalMeasuresBase.Multimeasure{StatisticalMeasuresBase.SupportsMissingsMeasure{StatisticalMeasures.LPLossOnScalars{Int64}}, Nothing, StatisticalMeasuresBase.Mean, typeof(identity)}}, Nothing}}}, Vector{Float64}, Vector{Vector{Float64}}}}[(model = DeterministicEnsembleModel(model = KNNRegressor(K = 18, …), …), measure = [LPLoss(p = 2)], measurement = [0.006563470407815136], per_fold = [[0.006563470407815136]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 16, …), …), measure = [LPLoss(p = 2)], measurement = [0.0122246059374603], per_fold = [[0.0122246059374603]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 1, …), …), measure = [LPLoss(p = 2)], measurement = [0.0036628015206465734], per_fold = [[0.0036628015206465734]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 14, …), …), measure = [LPLoss(p = 2)], measurement = [0.008020213502274467], per_fold = [[0.008020213502274467]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 5, …), …), measure = [LPLoss(p = 2)], measurement = [0.001787953402180091], per_fold = [[0.001787953402180091]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 7, …), …), measure = [LPLoss(p = 2)], measurement = [0.0025945479977448303], per_fold = [[0.0025945479977448303]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 18, …), …), measure = [LPLoss(p = 2)], measurement = [0.020351582333467805], per_fold = [[0.020351582333467805]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 12, …), …), measure = [LPLoss(p = 2)], measurement = [0.009536648688229333], per_fold = [[0.009536648688229333]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 12, …), …), measure = [LPLoss(p = 2)], measurement = [0.009536648688229333], per_fold = [[0.009536648688229333]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 12, …), …), measure = [LPLoss(p = 2)], measurement = [0.009536648688229333], per_fold = [[0.009536648688229333]])  …  (model = DeterministicEnsembleModel(model = KNNRegressor(K = 20, …), …), measure = [LPLoss(p = 2)], measurement = [0.01646454995922476], per_fold = [[0.01646454995922476]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 14, …), …), measure = [LPLoss(p = 2)], measurement = [0.006456471195446319], per_fold = [[0.006456471195446319]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 1, …), …), measure = [LPLoss(p = 2)], measurement = [0.003151889979016522], per_fold = [[0.003151889979016522]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 3, …), …), measure = [LPLoss(p = 2)], measurement = [0.0030046273651692227], per_fold = [[0.0030046273651692227]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 14, …), …), measure = [LPLoss(p = 2)], measurement = [0.004745533719078165], per_fold = [[0.004745533719078165]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 7, …), …), measure = [LPLoss(p = 2)], measurement = [0.004590011170145359], per_fold = [[0.004590011170145359]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 7, …), …), measure = [LPLoss(p = 2)], measurement = [0.004590011170145359], per_fold = [[0.004590011170145359]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 3, …), …), measure = [LPLoss(p = 2)], measurement = [0.0036225306262139788], per_fold = [[0.0036225306262139788]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 1, …), …), measure = [LPLoss(p = 2)], measurement = [0.0031016792824338077], per_fold = [[0.0031016792824338077]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 12, …), …), measure = [LPLoss(p = 2)], measurement = [0.007526080413070994], per_fold = [[0.007526080413070994]])],\n best_report = (measures = Any[],\n                oob_measurements = missing,),\n plotting = (parameter_names = [\"bagging_fraction\", \"model.K\"],\n             parameter_scales = [:linear, :linear],\n             parameter_values = Any[0.8888888888888888 18; 0.6111111111111112 16; … ; 0.5555555555555556 1; 0.6111111111111112 12],\n             measurements = [0.006563470407815136, 0.0122246059374603, 0.0036628015206465734, 0.008020213502274467, 0.001787953402180091, 0.0025945479977448303, 0.020351582333467805, 0.009536648688229333, 0.009536648688229333, 0.009536648688229333  …  0.01646454995922476, 0.006456471195446319, 0.003151889979016522, 0.0030046273651692227, 0.004745533719078165, 0.004590011170145359, 0.004590011170145359, 0.0036225306262139788, 0.0031016792824338077, 0.007526080413070994],),)\n\n\n\n\n4.3 plot tunning results\n\nres = r.plotting\ntable=(vals_b = res.parameter_values[:, 1],\n            vals_k = res.parameter_values[:, 2],\n            measurement=res.measurements\n)\n\ndatalayer=data(table)\nmappinglayer=mapping(:vals_b,:vals_k,:measurement,color=:measurement)\nvislayler=visual(Tricontourf,colormap = :batlow)\nax=(width = 400, height = 400)\nplt=datalayer*mappinglayer*vislayler\ndraw(plt,axis=ax)\n\n\n\n\n\n\n\nFigure 1: fig-ensemble-params-tunning\n\n\n\n\n\n\n\n4.4 predict with ensemble model\n\nŷ = predict(tuned_ensemble, rows=test)\n@show rms(ŷ, y[test])\n\nrms(ŷ, y[test]) = 0.05120320246985217\n\n\n0.05120320246985217",
    "crumbs": [
      "Learn",
      "ensemble model"
    ]
  },
  {
    "objectID": "learn/ensemblemodel-with-bostonhouse.html",
    "href": "learn/ensemblemodel-with-bostonhouse.html",
    "title": "ensemblemodel-with-bostonhouse",
    "section": "",
    "text": "info\n\n\n\nRandom Forest :ref What is random forest?",
    "crumbs": [
      "Learn",
      "ensemble model with boston housing dataset"
    ]
  },
  {
    "objectID": "learn/ensemblemodel-with-bostonhouse.html#loading-package",
    "href": "learn/ensemblemodel-with-bostonhouse.html#loading-package",
    "title": "ensemblemodel-with-bostonhouse",
    "section": "1. loading package",
    "text": "1. loading package\n\nusing MLJ\nimport DataFrames: DataFrame\nusing PrettyPrinting\nusing StableRNGs\nusing CairoMakie,AlgebraOfGraphics,MakieThemes\nrng = StableRNG(512)\n\nStableRNGs.LehmerRNG(state=0x00000000000000000000000000000401)",
    "crumbs": [
      "Learn",
      "ensemble model with boston housing dataset"
    ]
  },
  {
    "objectID": "learn/ensemblemodel-with-bostonhouse.html#load-data",
    "href": "learn/ensemblemodel-with-bostonhouse.html#load-data",
    "title": "ensemblemodel-with-bostonhouse",
    "section": "2. load data",
    "text": "2. load data\n\nX, y = @load_boston\n\n((Crim = [0.00632, 0.02731, 0.02729, 0.03237, 0.06905, 0.02985, 0.08829, 0.14455, 0.21124, 0.17004  …  0.2896, 0.26838, 0.23912, 0.17783, 0.22438, 0.06263, 0.04527, 0.06076, 0.10959, 0.04741], Zn = [18.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.5, 12.5, 12.5, 12.5  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Indus = [2.31, 7.07, 7.07, 2.18, 2.18, 2.18, 7.87, 7.87, 7.87, 7.87  …  9.69, 9.69, 9.69, 9.69, 9.69, 11.93, 11.93, 11.93, 11.93, 11.93], NOx = [0.538, 0.469, 0.469, 0.458, 0.458, 0.458, 0.524, 0.524, 0.524, 0.524  …  0.585, 0.585, 0.585, 0.585, 0.585, 0.573, 0.573, 0.573, 0.573, 0.573], Rm = [6.575, 6.421, 7.185, 6.998, 7.147, 6.43, 6.012, 6.172, 5.631, 6.004  …  5.39, 5.794, 6.019, 5.569, 6.027, 6.593, 6.12, 6.976, 6.794, 6.03], Age = [65.2, 78.9, 61.1, 45.8, 54.2, 58.7, 66.6, 96.1, 100.0, 85.9  …  72.9, 70.6, 65.3, 73.5, 79.7, 69.1, 76.7, 91.0, 89.3, 80.8], Dis = [4.09, 4.9671, 4.9671, 6.0622, 6.0622, 6.0622, 5.5605, 5.9505, 6.0821, 6.5921  …  2.7986, 2.8927, 2.4091, 2.3999, 2.4982, 2.4786, 2.2875, 2.1675, 2.3889, 2.505], Rad = [1.0, 2.0, 2.0, 3.0, 3.0, 3.0, 5.0, 5.0, 5.0, 5.0  …  6.0, 6.0, 6.0, 6.0, 6.0, 1.0, 1.0, 1.0, 1.0, 1.0], Tax = [296.0, 242.0, 242.0, 222.0, 222.0, 222.0, 311.0, 311.0, 311.0, 311.0  …  391.0, 391.0, 391.0, 391.0, 391.0, 273.0, 273.0, 273.0, 273.0, 273.0], PTRatio = [15.3, 17.8, 17.8, 18.7, 18.7, 18.7, 15.2, 15.2, 15.2, 15.2  …  19.2, 19.2, 19.2, 19.2, 19.2, 21.0, 21.0, 21.0, 21.0, 21.0], Black = [396.9, 396.9, 392.83, 394.63, 396.9, 394.12, 395.6, 396.9, 386.63, 386.71  …  396.9, 396.9, 396.9, 395.77, 396.9, 391.99, 396.9, 396.9, 393.45, 396.9], LStat = [4.98, 9.14, 4.03, 2.94, 5.33, 5.21, 12.43, 19.15, 29.93, 17.1  …  21.14, 14.1, 12.92, 15.1, 14.33, 9.67, 9.08, 5.64, 6.48, 7.88]), [24.0, 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9  …  19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22.0, 11.9])",
    "crumbs": [
      "Learn",
      "ensemble model with boston housing dataset"
    ]
  },
  {
    "objectID": "learn/ensemblemodel-with-bostonhouse.html#section",
    "href": "learn/ensemblemodel-with-bostonhouse.html#section",
    "title": "ensemblemodel-with-bostonhouse",
    "section": "4.",
    "text": "4.",
    "crumbs": [
      "Learn",
      "ensemble model with boston housing dataset"
    ]
  },
  {
    "objectID": "learn/ensemblemodel-with-bostonhouse.html#single-model",
    "href": "learn/ensemblemodel-with-bostonhouse.html#single-model",
    "title": "ensemblemodel-with-bostonhouse",
    "section": "3. single model",
    "text": "3. single model\n\n3.1 load model\n\nDecisionTreeRegressor = @load DecisionTreeRegressor pkg=DecisionTree\n\nimport MLJDecisionTreeInterface ✔\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n\n\nMLJDecisionTreeInterface.DecisionTreeRegressor\n\n\n\n\n3.2 instantiate and evaulate model\n\ntree = machine(DecisionTreeRegressor(), X, y)\ne = evaluate!(tree, resampling=Holdout(fraction_train=0.8),\n              measure=[rms, rmslp1])\n\n\nPerformanceEvaluation object with these fields:\n  model, measure, operation, measurement, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_rows, resampling, repeats\nExtract:\n┌──────────────────────────────────────┬───────────┬─────────────┬──────────┐\n│ measure                              │ operation │ measurement │ per_fold │\n├──────────────────────────────────────┼───────────┼─────────────┼──────────┤\n│ RootMeanSquaredError()               │ predict   │ 7.06        │ [7.06]   │\n│ RootMeanSquaredLogProportionalError( │ predict   │ 0.328       │ [0.328]  │\n│   offset = 1)                        │           │             │          │\n└──────────────────────────────────────┴───────────┴─────────────┴──────────┘",
    "crumbs": [
      "Learn",
      "ensemble model with boston housing dataset"
    ]
  },
  {
    "objectID": "learn/ensemblemodel-with-bostonhouse.html#ensemble-mode-random-forest",
    "href": "learn/ensemblemodel-with-bostonhouse.html#ensemble-mode-random-forest",
    "title": "ensemblemodel-with-bostonhouse",
    "section": "4. Ensemble mode : Random Forest",
    "text": "4. Ensemble mode : Random Forest\n\n4.1 instantiate random forest\n\nforest = EnsembleModel(model=DecisionTreeRegressor())\nforest.model.n_subfeatures = 3\n\n3\n\n\n\n\n4.2 by learning curve to get best trees\n\nrng = StableRNG(5123) # for reproducibility\nm = machine(forest, X, y)\nr = range(forest, :n, lower=10, upper=1000)\ncurves = MLJ.learning_curve(m, resampling=Holdout(fraction_train=0.8, rng=rng),\n                         range=r, measure=rms);\n\n[ Info: Training machine(DeterministicTunedModel(model = DeterministicEnsembleModel(model = DecisionTreeRegressor(max_depth = -1, …), …), …), …).\n[ Info: Attempting to evaluate 30 models.\nEvaluating over 30 metamodels:   0%[&gt;                        ]  ETA: N/AEvaluating over 30 metamodels:   3%[&gt;                        ]  ETA: 0:00:00Evaluating over 30 metamodels:   7%[=&gt;                       ]  ETA: 0:00:00Evaluating over 30 metamodels:  10%[==&gt;                      ]  ETA: 0:00:00Evaluating over 30 metamodels:  13%[===&gt;                     ]  ETA: 0:00:00Evaluating over 30 metamodels:  17%[====&gt;                    ]  ETA: 0:00:00Evaluating over 30 metamodels:  20%[=====&gt;                   ]  ETA: 0:00:00Evaluating over 30 metamodels:  23%[=====&gt;                   ]  ETA: 0:00:00Evaluating over 30 metamodels:  27%[======&gt;                  ]  ETA: 0:00:00Evaluating over 30 metamodels:  30%[=======&gt;                 ]  ETA: 0:00:00Evaluating over 30 metamodels:  33%[========&gt;                ]  ETA: 0:00:00Evaluating over 30 metamodels:  37%[=========&gt;               ]  ETA: 0:00:00Evaluating over 30 metamodels:  40%[==========&gt;              ]  ETA: 0:00:00Evaluating over 30 metamodels:  43%[==========&gt;              ]  ETA: 0:00:00Evaluating over 30 metamodels:  47%[===========&gt;             ]  ETA: 0:00:00Evaluating over 30 metamodels:  50%[============&gt;            ]  ETA: 0:00:00Evaluating over 30 metamodels:  53%[=============&gt;           ]  ETA: 0:00:00Evaluating over 30 metamodels:  57%[==============&gt;          ]  ETA: 0:00:00Evaluating over 30 metamodels:  60%[===============&gt;         ]  ETA: 0:00:00Evaluating over 30 metamodels:  63%[===============&gt;         ]  ETA: 0:00:00Evaluating over 30 metamodels:  67%[================&gt;        ]  ETA: 0:00:00Evaluating over 30 metamodels:  70%[=================&gt;       ]  ETA: 0:00:00Evaluating over 30 metamodels:  73%[==================&gt;      ]  ETA: 0:00:00Evaluating over 30 metamodels:  77%[===================&gt;     ]  ETA: 0:00:00Evaluating over 30 metamodels:  80%[====================&gt;    ]  ETA: 0:00:00Evaluating over 30 metamodels:  83%[====================&gt;    ]  ETA: 0:00:00Evaluating over 30 metamodels:  87%[=====================&gt;   ]  ETA: 0:00:00Evaluating over 30 metamodels:  90%[======================&gt;  ]  ETA: 0:00:00Evaluating over 30 metamodels:  93%[=======================&gt; ]  ETA: 0:00:00Evaluating over 30 metamodels:  97%[========================&gt;]  ETA: 0:00:00Evaluating over 30 metamodels: 100%[=========================] Time: 0:00:00\n\n\n\nlet\n  ax=(width = 400, height = 400,xlabel=\"rmsr\",ylabel=\"numbers of trees\")\n  datalayer=data((parameter_values=curves.parameter_values,measurements=curves.measurements))\n  mappinglayer=mapping(:parameter_values,:measurements)\n  vislayer=visual(Lines,color=:blue)\n  plt=datalayer*mappinglayer*vislayer\n  draw(plt, axis=ax)\nend\n\n\n\n\n\n\n\n\n\n\n4.3 setting 150 trees\n\nforest.n = 150;\n\n\n\n4.4 setting tunning parameters range\n\nr_sf = range(forest, :(model.n_subfeatures), lower=1, upper=12)\nr_bf = range(forest, :bagging_fraction, lower=0.4, upper=1.0);\n\n\n\n4.4 tuning random forest\n\ntuned_forest = TunedModel(model=forest,\n                          tuning=Grid(resolution=3),\n                          resampling=CV(nfolds=6, rng=StableRNG(32)),\n                          ranges=[r_sf, r_bf],\n                          measure=rms)\nm = machine(tuned_forest, X, y)\ne = evaluate!(m, resampling=Holdout(fraction_train=0.8),\n              measure=[rms, rmslp1])\ne\n\n\nPerformanceEvaluation object with these fields:\n  model, measure, operation, measurement, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_rows, resampling, repeats\nExtract:\n┌──────────────────────────────────────┬───────────┬─────────────┬──────────┐\n│ measure                              │ operation │ measurement │ per_fold │\n├──────────────────────────────────────┼───────────┼─────────────┼──────────┤\n│ RootMeanSquaredError()               │ predict   │ 4.04        │ [4.04]   │\n│ RootMeanSquaredLogProportionalError( │ predict   │ 0.255       │ [0.255]  │\n│   offset = 1)                        │           │             │          │\n└──────────────────────────────────────┴───────────┴─────────────┴──────────┘\n\n\n\n\n\nr = report(m)\n\n(best_model = DeterministicEnsembleModel(model = DecisionTreeRegressor(max_depth = -1, …), …),\n best_history_entry = (model = DeterministicEnsembleModel(model = DecisionTreeRegressor(max_depth = -1, …), …),\n                       measure = StatisticalMeasuresBase.RobustMeasure{StatisticalMeasuresBase.FussyMeasure{StatisticalMeasuresBase.RobustMeasure{StatisticalMeasuresBase.Multimeasure{StatisticalMeasuresBase.SupportsMissingsMeasure{StatisticalMeasures.RootMeanSquaredErrorOnScalars}, Nothing, StatisticalMeasuresBase.RootMean{Int64}, typeof(identity)}}, Nothing}}[RootMeanSquaredError()],\n                       measurement = [3.5722631166673007],\n                       per_fold = [[2.4889985261233605, 2.385715818338134, 4.825513471374633, 3.9974877555114485, 3.4217104683246666, 3.708432091912111]],),\n history = NamedTuple{(:model, :measure, :measurement, :per_fold), Tuple{MLJEnsembles.DeterministicEnsembleModel{MLJDecisionTreeInterface.DecisionTreeRegressor}, Vector{StatisticalMeasuresBase.RobustMeasure{StatisticalMeasuresBase.FussyMeasure{StatisticalMeasuresBase.RobustMeasure{StatisticalMeasuresBase.Multimeasure{StatisticalMeasuresBase.SupportsMissingsMeasure{StatisticalMeasures.RootMeanSquaredErrorOnScalars}, Nothing, StatisticalMeasuresBase.RootMean{Int64}, typeof(identity)}}, Nothing}}}, Vector{Float64}, Vector{Vector{Float64}}}}[(model = DeterministicEnsembleModel(model = DecisionTreeRegressor(max_depth = -1, …), …), measure = [RootMeanSquaredError()], measurement = [3.8094502225320523], per_fold = [[2.6442935772490963, 2.6347662294070195, 5.180223943255861, 4.121633628755032, 3.402975674534002, 4.216118068170253]]), (model = DeterministicEnsembleModel(model = DecisionTreeRegressor(max_depth = -1, …), …), measure = [RootMeanSquaredError()], measurement = [5.416797814622986], per_fold = [[4.480610589273179, 4.094948763264285, 7.132537822015005, 5.384819020672911, 4.707260488286648, 6.097756283082266]]), (model = DeterministicEnsembleModel(model = DecisionTreeRegressor(max_depth = -1, …), …), measure = [RootMeanSquaredError()], measurement = [4.91695833115296], per_fold = [[3.6596309799981337, 3.741953403720741, 6.717443336628948, 4.937702185406267, 4.2470371282610575, 5.488318721228702]]), (model = DeterministicEnsembleModel(model = DecisionTreeRegressor(max_depth = -1, …), …), measure = [RootMeanSquaredError()], measurement = [3.9437875486722973], per_fold = [[3.0747877290149153, 3.073547224160072, 5.1088743894788875, 4.49232090548857, 3.717456825964539, 3.787278328193902]]), (model = DeterministicEnsembleModel(model = DecisionTreeRegressor(max_depth = -1, …), …), measure = [RootMeanSquaredError()], measurement = [4.5872235020609455], per_fold = [[3.512487539994043, 3.3544633706394067, 6.06724474892402, 4.767415489319376, 4.030311481897942, 5.188766348831169]]), (model = DeterministicEnsembleModel(model = DecisionTreeRegressor(max_depth = -1, …), …), measure = [RootMeanSquaredError()], measurement = [3.5722631166673007], per_fold = [[2.4889985261233605, 2.385715818338134, 4.825513471374633, 3.9974877555114485, 3.4217104683246666, 3.708432091912111]]), (model = DeterministicEnsembleModel(model = DecisionTreeRegressor(max_depth = -1, …), …), measure = [RootMeanSquaredError()], measurement = [4.088355178253196], per_fold = [[2.9594662972525345, 2.7408403025426593, 5.532571280122637, 4.37336784261903, 3.5178510319195486, 4.685615288115846]]), (model = DeterministicEnsembleModel(model = DecisionTreeRegressor(max_depth = -1, …), …), measure = [RootMeanSquaredError()], measurement = [4.237002137785962], per_fold = [[4.332862308738402, 3.5041798259437305, 5.466874767426336, 4.7402739799589515, 3.6186264610860777, 3.332125102319342]]), (model = DeterministicEnsembleModel(model = DecisionTreeRegressor(max_depth = -1, …), …), measure = [RootMeanSquaredError()], measurement = [4.102928466791321], per_fold = [[2.9936565278974525, 2.8277922166553706, 5.367651887312894, 4.8295750685245284, 3.545286939742617, 4.4057722267933634]])],\n best_report = (measures = Any[],\n                oob_measurements = missing,),\n plotting = (parameter_names = [\"model.n_subfeatures\", \"bagging_fraction\"],\n             parameter_scales = [:linear, :linear],\n             parameter_values = Any[6 0.7; 1 0.4; … ; 12 1.0; 12 0.4],\n             measurements = [3.8094502225320523, 5.416797814622986, 4.91695833115296, 3.9437875486722973, 4.5872235020609455, 3.5722631166673007, 4.088355178253196, 4.237002137785962, 4.102928466791321],),)\n\n\n\n\n4.5 plot tunning results\n\nres = r.plotting\ntable=(vals_b = res.parameter_values[:, 1],\n            vals_k = res.parameter_values[:, 2],\n            measurement=res.measurements\n)\n\ndatalayer=data(table)\nmappinglayer=mapping(:vals_b,:vals_k,:measurement,color=:measurement)\nvislayler=visual(Tricontourf,colormap = :batlow)\nax=(width = 400, height = 400)\nplt=datalayer*mappinglayer*vislayler\ndraw(plt,axis=ax)\n\n\n\n\n\n\n\nFigure 1: fig-ensemble-params-tunning\n\n\n\n\n\n\n\n4.6 predict with ensemble model\n\nŷ = predict(m,X)\n@show rms(ŷ, y)\n\nrms(ŷ, y) = 2.4308074793520404\n\n\n2.4308074793520404",
    "crumbs": [
      "Learn",
      "ensemble model with boston housing dataset"
    ]
  },
  {
    "objectID": "start/index.html#first",
    "href": "start/index.html#first",
    "title": "Start",
    "section": "",
    "text": "import MLJ:evaluate,MLJInterface\nusing MLJ,DataFrames\niris=load_iris()|&gt;DataFrame\n\ny, X = unpack(iris, ==(:target); rng=123);\n\nfunction  build_model(X,y)\n    Tree = @load DecisionTreeClassifier pkg=DecisionTree\n    tree = Tree()\n    evaluate(tree, X, y, resampling=CV(shuffle=true),\n        measures=[log_loss, accuracy],\n        verbosity=0)\nend\n\nbuild_model(X,y)",
    "crumbs": [
      "Start",
      "GET STARTED"
    ]
  },
  {
    "objectID": "start/index.html#first-code-snip",
    "href": "start/index.html#first-code-snip",
    "title": "Start",
    "section": "",
    "text": "import MLJ:evaluate,MLJInterface\nusing MLJ,DataFrames\niris=load_iris()|&gt;DataFrame\n\ny, X = unpack(iris, ==(:target); rng=123);\n\nfunction  build_model(X,y)\n    Tree = @load DecisionTreeClassifier pkg=DecisionTree\n    tree = Tree()\n    evaluate(tree, X, y, resampling=CV(shuffle=true),\n        measures=[log_loss, accuracy],\n        verbosity=0)\nend\n\nbuild_model(X,y)",
    "crumbs": [
      "Start",
      "GET STARTED"
    ]
  },
  {
    "objectID": "start/index.html#first-code-snippets",
    "href": "start/index.html#first-code-snippets",
    "title": "Start",
    "section": "",
    "text": "import MLJ:evaluate,MLJInterface\nusing MLJ,DataFrames\niris=load_iris()|&gt;DataFrame\n\ny, X = unpack(iris, ==(:target); rng=123);\n\nfunction  build_model(X,y)\n    Tree = @load DecisionTreeClassifier pkg=DecisionTree\n    tree = Tree()\n    evaluate(tree, X, y, resampling=CV(shuffle=true),\n        measures=[log_loss, accuracy],\n        verbosity=0)\nend\n\nbuild_model(X,y)",
    "crumbs": [
      "Start",
      "GET STARTED"
    ]
  },
  {
    "objectID": "learn/learning-network-1.html",
    "href": "learn/learning-network-1.html",
    "title": "Learning Network 1",
    "section": "",
    "text": "using MLJ\n    using MLJ\n    X, y = make_blobs(cluster_std=10.0, rng=123) \n    Xnew, _ = make_blobs(3) \n\n(Tables.MatrixTable{Matrix{Float64}} with 3 rows, 2 columns, and schema:\n :x1  Float64\n :x2  Float64, CategoricalArrays.CategoricalValue{Int64, UInt32}[3, 2, 1])",
    "crumbs": [
      "Learn",
      "learnig network 1"
    ]
  },
  {
    "objectID": "learn/learning-network-1.html#load-package",
    "href": "learn/learning-network-1.html#load-package",
    "title": "Learning Network 1",
    "section": "",
    "text": "using MLJ\n    using MLJ\n    X, y = make_blobs(cluster_std=10.0, rng=123) \n    Xnew, _ = make_blobs(3) \n\n(Tables.MatrixTable{Matrix{Float64}} with 3 rows, 2 columns, and schema:\n :x1  Float64\n :x2  Float64, CategoricalArrays.CategoricalValue{Int64, UInt32}[3, 2, 1])",
    "crumbs": [
      "Learn",
      "learnig network 1"
    ]
  },
  {
    "objectID": "learn/learning-network-1.html#load-models",
    "href": "learn/learning-network-1.html#load-models",
    "title": "Learning Network 1",
    "section": "2. load models",
    "text": "2. load models\n\n    pca = (@load PCA pkg=MultivariateStats verbosity=0)()\n    tree = (@load DecisionTreeClassifier pkg=DecisionTree verbosity=0)()\n\nDecisionTreeClassifier(\n  max_depth = -1, \n  min_samples_leaf = 1, \n  min_samples_split = 2, \n  min_purity_increase = 0.0, \n  n_subfeatures = 0, \n  post_prune = false, \n  merge_purity_threshold = 1.0, \n  display_depth = 5, \n  feature_importance = :impurity, \n  rng = Random._GLOBAL_RNG())",
    "crumbs": [
      "Learn",
      "learnig network 1"
    ]
  },
  {
    "objectID": "learn/learning-network-1.html#train-data",
    "href": "learn/learning-network-1.html#train-data",
    "title": "Learning Network 1",
    "section": "3. train data",
    "text": "3. train data",
    "crumbs": [
      "Learn",
      "learnig network 1"
    ]
  },
  {
    "objectID": "learn/learning-network-1.html#training-data-as",
    "href": "learn/learning-network-1.html#training-data-as",
    "title": "Learning Network 1",
    "section": "3. training data as",
    "text": "3. training data as",
    "crumbs": [
      "Learn",
      "learnig network 1"
    ]
  },
  {
    "objectID": "learn/learning-network-1.html#training-data-as-source",
    "href": "learn/learning-network-1.html#training-data-as-source",
    "title": "Learning Network 1",
    "section": "3. training data as source",
    "text": "3. training data as source\n\n    Xs = source(X)\n    ys = source(y)\n\nSource @773 ⏎ `AbstractVector{Multiclass{3}}`\n\n\n\nmach1 = machine(pca, Xs)\nx = transform(mach1, Xs)\n\n\nNode @412 → PCA(…)\n  args:\n    1:  Source @532\n  formula:\n    transform(\n      machine(PCA(maxoutdim = 0, …), …), \n      Source @532)\n\n\n\n\nmach2 = machine(tree, x, ys)\nyhat = predict(mach2, x)\n\n\nNode @878 → DecisionTreeClassifier(…)\n  args:\n    1:  Node @412 → PCA(…)\n  formula:\n    predict(\n      machine(DecisionTreeClassifier(max_depth = -1, …), …), \n      transform(\n        machine(PCA(maxoutdim = 0, …), …), \n        Source @532))\n\n\n\n\nfit!(yhat)\n\n[ Info: Training machine(PCA(maxoutdim = 0, …), …).\n[ Info: Training machine(DecisionTreeClassifier(max_depth = -1, …), …).\n\n\n\nNode @878 → DecisionTreeClassifier(…)\n  args:\n    1:  Node @412 → PCA(…)\n  formula:\n    predict(\n      machine(DecisionTreeClassifier(max_depth = -1, …), …), \n      transform(\n        machine(PCA(maxoutdim = 0, …), …), \n        Source @532))",
    "crumbs": [
      "Learn",
      "learnig network 1"
    ]
  },
  {
    "objectID": "learn/tailwind-test.html",
    "href": "learn/tailwind-test.html",
    "title": "tailwind css test",
    "section": "",
    "text": "Leslie Alexander\n\n\nleslie.alexander@example.com\n\n\n\n\n\nCo-Founder / CEO\n\n\nLast seen 3h ago\n\n\n\n\n\nHello world!\n\n\n\n\n\n\n\nChitChat\n\n\nYou have a new message!\n\n\n\n\n\n\n\n\nErin Lindford\n\n\nProduct Engineer\n\n\n\nMessage\n\n\n\n\n\n\n\n\n\nLeslie Alexander\n\n\nleslie.alexander@example.com\n\n\n\n\n\nCo-Founder / CEO\n\n\nLast seen 3h ago\n\n\n\n\n\n\n\n\nMichael Foster\n\n\nmichael.foster@example.com\n\n\n\n\n\nCo-Founder / CTO\n\n\nLast seen 3h ago\n\n\n\n\n\n\n\n\nDries Vincent\n\n\ndries.vincent@example.com\n\n\n\n\n\nBusiness Relations\n\n\n\n\n\n\n\n\nOnline\n\n\n\n\n\n\n\n\n\nLindsay Walton\n\n\nlindsay.walton@example.com\n\n\n\n\n\nFront-end Developer\n\n\nLast seen 3h ago\n\n\n\n\n\n\nJumbotron\n\n\n\n\nJumbotron",
    "crumbs": [
      "Learn",
      "tailwind css test"
    ]
  },
  {
    "objectID": "learn/pca-explained.html",
    "href": "learn/pca-explained.html",
    "title": "PCA explained",
    "section": "",
    "text": "PCA explained\n\n\n\n我们通过一个简单实例来直观介绍一下主成分分析(PCA).主成分分析对高维数据分析非常有用, 不失一般性, 在二维数据上仍然可以工作. 在数学中,最基本的方法是度量, 例如度量人的身高. 度量身高时我们需要一个工具, 直尺,卷尺,甚至是一根树枝都可以作为度量工具. 如果要度量两个属性, 例如同时度量身高和体重, 我们还需要一个称量工具. 对于二维数据,度量不同属性的工具本不能交叉使用. 在大多数情况下,属性都不可或缺,但是在有些特殊情况下, 有些属性就显得不那么重要. 例如有一条公路, 度量公路的属性有长度和宽度. 但是当我们在地图上观察公路的时候, 发现道路的宽度经常没有,出现的只是长度曲线. 所以在地图应用中, 道路的宽度信息不那么重要,在地图中, 很容易观察到长度信息比宽度信息重要. 从数值上看, 长度的跨度比宽度跨度大. 因此我们可以舍去宽度信息. 主成分分析实际就是上面这段直观解释的算法实现.\n\n\nLet’s take a brief example to visualize Principal Component Analysis (PCA), which is very useful for high-dimensional data analysis, without losing generality, and can still work on two-dimensional data. In mathematics, the most basic method is to measure, for example to measure a person’s height. We need a tool when measuring height, a ruler, a tape measure and even a branch can be used as a measurement tool. If we want to measure two attributes, such as measuring height and weight at the same time, we also need a weighing tool. For 2D data, tools to measure different properties cannot be used interchangeably. In most cases, attributes are indispensable, but in some special cases, some attributes are less important. For example, if there is a highway, the properties of the highway are measured by length and width. But when we look at the road on the map, we often don’t have the width of the road, but only the length curve. So in the map application, the width information of the road is not so important, and in the map, it is easy to observe that the length information is more important than the width information. Numerically, the span of the length is larger than the span of the width. So we can discard the width information. Principal component analysis is actually the algorithm implementation of the above intuitive explanation.\n\n\n using CSV,DataFrames,Tidier,Pipe\n using CairoMakie,AlgebraOfGraphics\n using Random,Distributions\n dist=Normal()\n Random.seed!(34343)\n\nTaskLocalRNG()\n\n\n\n  xs=range(0,4pi, 300)\n  fy(x)=5*sin(x)+3*x+2*rand(dist)\n  ys=fy.(xs)\n  df=DataFrame(xs=xs,ys=ys)\n\n300×2 DataFrame275 rows omitted\n\n\n\nRow\nxs\nys\n\n\n\nFloat64\nFloat64\n\n\n\n\n1\n0.0\n-0.272431\n\n\n2\n0.042028\n-1.63477\n\n\n3\n0.084056\n0.367143\n\n\n4\n0.126084\n1.39357\n\n\n5\n0.168112\n-4.05051\n\n\n6\n0.21014\n0.794208\n\n\n7\n0.252168\n0.94975\n\n\n8\n0.294196\n3.93994\n\n\n9\n0.336224\n3.71181\n\n\n10\n0.378252\n5.31367\n\n\n11\n0.42028\n1.89908\n\n\n12\n0.462308\n5.11653\n\n\n13\n0.504336\n7.07824\n\n\n⋮\n⋮\n⋮\n\n\n289\n12.1041\n33.2352\n\n\n290\n12.1461\n33.5851\n\n\n291\n12.1881\n33.8266\n\n\n292\n12.2301\n36.0369\n\n\n293\n12.2722\n36.9264\n\n\n294\n12.3142\n35.2095\n\n\n295\n12.3562\n36.0006\n\n\n296\n12.3983\n35.3022\n\n\n297\n12.4403\n33.5829\n\n\n298\n12.4823\n38.3843\n\n\n299\n12.5243\n38.147\n\n\n300\n12.5664\n36.3962\n\n\n\n\n\n\n\nax=(width=500,height=250)\ndlayer=data(df)\nmlayer=mapping(:xs,:ys)\nvlayer=visual(Scatter,markersize=6, color=(:blue,0.5),strokewidth=1,strokecolor=:black)\nplt1=dlayer*mlayer*vlayer\ndraw(plt1,axis=ax)\n\n\n\n\n\n\n\n\n\n对于二维数据, 理论上可以占满坐标系中所有的位置,但是实际的数据可能只出现在有限的位置, 我们可以说真实的数据是嵌入在二维空间中的\n\n\nFor two-dimensional data, theoretically all positions in the coordinate system can be filled, but the actual data may only appear in a limited number of positions, and we can say that the real data is embedded in the two-dimensional space\n\n\nmatrix=Matrix(df)|&gt;transpose\n\n2×300 transpose(::Matrix{Float64}) with eltype Float64:\n  0.0        0.042028  0.084056  …  12.4403  12.4823  12.5243  12.5664\n -0.272431  -1.63477   0.367143     33.5829  38.3843  38.147   36.3962\n\n\n\nSVD decomposition\n\nusing  LinearAlgebra\nfunction svd_block(A::Matrix)\n        U,Σ,V=svd(A)\n        return (i)-&gt;U[:,i]*Σ[i]*V[:,i]'\nend\n\nd=@pipe df|&gt;Matrix|&gt;svd_block\ndf2=@pipe d(1)|&gt;DataFrame(_,:auto)\n\ndlayer2=data(df2)\nmlayer2=mapping(:x1,:x2)\nplt2=dlayer2*mlayer2*visual(Scatter,markersize=6, color=(:yellow,0.5),strokewidth=1,strokecolor=:black)\ndraw(plt2)\n\n\n\n\n\n\n\n\n\ndraw(plt1+plt2)\n\n\n\n\n\n\n\n\n\ndf3=@pipe d(2)|&gt;DataFrame(_,:auto)\n\ndlayer3=data(df3)\nmlayer3=mapping(:x1,:x2)\nplt3=dlayer3*mlayer3*visual(Scatter,markersize=6, color=(:yellow,0.5),strokewidth=1,strokecolor=:black)\ndraw(plt3)\n\n\n\n\n\n\n\n\n\nax=(width=500,height=500)\ndraw(plt2+plt3,axis=ax)",
    "crumbs": [
      "Learn",
      "PCA  explained"
    ]
  },
  {
    "objectID": "learn/menu-test.html",
    "href": "learn/menu-test.html",
    "title": "Menu Test",
    "section": "",
    "text": "using WGLMakie\nfig = Figure()\n\nmenu = Menu(fig, options = [\"viridis\", \"heat\", \"blues\"], default = \"blues\")\n\nfuncs = [sqrt, x-&gt;x^2, sin, cos]\n\nmenu2 = Menu(fig,\n    options = zip([\"Square Root\", \"Square\", \"Sine\", \"Cosine\"], funcs),\n    default = \"Square\")\n\nfig[1, 1] = vgrid!(\n    Label(fig, \"Colormap\", width = nothing),\n    menu,\n    Label(fig, \"Function\", width = nothing),\n    menu2;\n    tellheight = false, width = 200)\n\nax = Axis(fig[1, 2])\n\nfunc = Observable{Any}(funcs[1])\n\nys = lift(func) do f\n    f.(0:0.3:10)\nend\nscat = scatter!(ax, ys, markersize = 10px, color = ys)\n\ncb = Colorbar(fig[1, 3], scat)\n\non(menu.selection) do s\n    scat.colormap = s\nend\nnotify(menu.selection)\n\non(menu2.selection) do s\n    func[] = s\n    autolimits!(ax)\nend\nnotify(menu2.selection)\n\nmenu2.is_open = true\n\nfig",
    "crumbs": [
      "Learn",
      "Menu Test"
    ]
  }
]
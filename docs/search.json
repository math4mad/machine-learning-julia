[
  {
    "objectID": "workflow/tailwindcss.html",
    "href": "workflow/tailwindcss.html",
    "title": "embedding tailwind css",
    "section": "",
    "text": "test tailwind css\n\nExample heading New\n\n\nNotifications 4\n\n\n❶ ❷ ❸ ❹ ❺ ➅ ➆ ➇ ➈ ➉\n\n\n❶ ❷ ❸ ❹ ❺ ➅ ➆ ➇ ➈ ➉\n\n\n\n\nFirst Name\n\n\nLast Name\n\n\nPoints\n\n\n\n\nJill\n\n\nSmith\n\n\n50\n\n\n\n\nJill\n\n\nSmith\n\n\n50\n\n\n\n\nJill\n\n\nSmith\n\n\n50"
  },
  {
    "objectID": "workflow/composing-models.html",
    "href": "workflow/composing-models.html",
    "title": "Composing Models",
    "section": "",
    "text": "MLJ 中组合模型有三种形式\n\n\n\n\n\n    %%| label: fig-mlj-composing model\n    %%| fig-cap: \"MLJ  composing model\"\n    %%| fig-width: 6.5\n    %%| echo: true\n    graph TD\n        C(Composing Models)\n        C --&gt;|One| D([Pipeline Model ])\n        C --&gt;|Two| E([Ensemble Model])\n        C --&gt;|Three| F([Stack Model])",
    "crumbs": [
      "Workflow",
      "composing models"
    ]
  },
  {
    "objectID": "start/getting-start.html",
    "href": "start/getting-start.html",
    "title": "Build first MLJ machine learning model",
    "section": "",
    "text": "we followed workflow, step by step\n\n\nfor iris dataset ref: Iris Dataset EDA\n\nusing Pipe\nusing CSV,DataFrames\nusing MLJ\ndf=CSV.File(\"../data/iris.csv\")|&gt;DataFrame;\n\n\nschema(df)\n\n\n┌──────────────┬────────────┬──────────┐\n│ names        │ scitypes   │ types    │\n├──────────────┼────────────┼──────────┤\n│ sepal_length │ Continuous │ Float64  │\n│ sepal_width  │ Continuous │ Float64  │\n│ petal_length │ Continuous │ Float64  │\n│ petal_width  │ Continuous │ Float64  │\n│ species      │ Textual    │ String15 │\n└──────────────┴────────────┴──────────┘\n\n\n\n\nfor species need transform to Categorical type\n\ndf.species= coerce(df.species,Multiclass);\nschema(df)\n\n\n┌──────────────┬───────────────┬────────────────────────────────────┐\n│ names        │ scitypes      │ types                              │\n├──────────────┼───────────────┼────────────────────────────────────┤\n│ sepal_length │ Continuous    │ Float64                            │\n│ sepal_width  │ Continuous    │ Float64                            │\n│ petal_length │ Continuous    │ Float64                            │\n│ petal_width  │ Continuous    │ Float64                            │\n│ species      │ Multiclass{3} │ CategoricalValue{String15, UInt32} │\n└──────────────┴───────────────┴────────────────────────────────────┘\n\n\n\n\nnow we need to split data\n\ny, X = unpack(df, ==(:species); rng=123)\n\n\n(CategoricalArrays.CategoricalValue{String15, UInt32}[String15(\"virginica\"), String15(\"versicolor\"), String15(\"virginica\"), String15(\"setosa\"), String15(\"versicolor\"), String15(\"virginica\"), String15(\"setosa\"), String15(\"versicolor\"), String15(\"setosa\"), String15(\"virginica\")  …  String15(\"virginica\"), String15(\"virginica\"), String15(\"virginica\"), String15(\"versicolor\"), String15(\"virginica\"), String15(\"setosa\"), String15(\"virginica\"), String15(\"virginica\"), String15(\"setosa\"), String15(\"setosa\")], 150×4 DataFrame\n Row │ sepal_length  sepal_width  petal_length  petal_width \n     │ Float64       Float64      Float64       Float64     \n─────┼──────────────────────────────────────────────────────\n   1 │          6.7          3.3           5.7          2.1\n   2 │          5.7          2.8           4.1          1.3\n   3 │          7.2          3.0           5.8          1.6\n   4 │          4.4          2.9           1.4          0.2\n   5 │          5.6          2.5           3.9          1.1\n   6 │          6.5          3.0           5.2          2.0\n   7 │          4.4          3.0           1.3          0.2\n   8 │          6.1          2.9           4.7          1.4\n   9 │          5.4          3.9           1.7          0.4\n  10 │          4.9          2.5           4.5          1.7\n  11 │          6.3          2.5           4.9          1.5\n  ⋮  │      ⋮             ⋮            ⋮             ⋮\n 141 │          6.4          2.7           5.3          1.9\n 142 │          6.8          3.2           5.9          2.3\n 143 │          6.9          3.1           5.4          2.1\n 144 │          6.1          2.8           4.0          1.3\n 145 │          6.7          2.5           5.8          1.8\n 146 │          5.0          3.5           1.3          0.3\n 147 │          7.6          3.0           6.6          2.1\n 148 │          6.3          2.5           5.0          1.9\n 149 │          5.1          3.8           1.6          0.2\n 150 │          5.0          3.6           1.4          0.2\n                                            129 rows omitted)\n\n\n\n\n\n\n\nres=@pipe models(matching(X,y))|&gt; DataFrame|&gt;select(_,[:name,:package_name])\n\n53×2 DataFrame28 rows omitted\n\n\n\nRow\nname\npackage_name\n\n\n\nString\nString\n\n\n\n\n1\nAdaBoostClassifier\nMLJScikitLearnInterface\n\n\n2\nAdaBoostStumpClassifier\nDecisionTree\n\n\n3\nBaggingClassifier\nMLJScikitLearnInterface\n\n\n4\nBayesianLDA\nMLJScikitLearnInterface\n\n\n5\nBayesianLDA\nMultivariateStats\n\n\n6\nBayesianQDA\nMLJScikitLearnInterface\n\n\n7\nBayesianSubspaceLDA\nMultivariateStats\n\n\n8\nCatBoostClassifier\nCatBoost\n\n\n9\nConstantClassifier\nMLJModels\n\n\n10\nDecisionTreeClassifier\nBetaML\n\n\n11\nDecisionTreeClassifier\nDecisionTree\n\n\n12\nDeterministicConstantClassifier\nMLJModels\n\n\n13\nDummyClassifier\nMLJScikitLearnInterface\n\n\n⋮\n⋮\n⋮\n\n\n42\nRandomForestClassifier\nMLJScikitLearnInterface\n\n\n43\nRidgeCVClassifier\nMLJScikitLearnInterface\n\n\n44\nRidgeClassifier\nMLJScikitLearnInterface\n\n\n45\nSGDClassifier\nMLJScikitLearnInterface\n\n\n46\nSVC\nLIBSVM\n\n\n47\nSVMClassifier\nMLJScikitLearnInterface\n\n\n48\nSVMLinearClassifier\nMLJScikitLearnInterface\n\n\n49\nSVMNuClassifier\nMLJScikitLearnInterface\n\n\n50\nStableForestClassifier\nSIRUS\n\n\n51\nStableRulesClassifier\nSIRUS\n\n\n52\nSubspaceLDA\nMultivariateStats\n\n\n53\nXGBoostClassifier\nXGBoost\n\n\n\n\n\n\nsimple way to look into model details:\ndoc($name, pkg=$package_name)\n\ndoc(\"AdaBoostStumpClassifier\", pkg=\"DecisionTree\")\n\nAdaBoostStumpClassifier\nA model type for constructing a Ada-boosted stump classifier, based on DecisionTree.jl, and implementing the MLJ model interface.\nFrom MLJ, the type can be imported using\nAdaBoostStumpClassifier = @load AdaBoostStumpClassifier pkg=DecisionTree\nDo model = AdaBoostStumpClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in AdaBoostStumpClassifier(n_iter=...).\n\n\nIn MLJ or MLJBase, bind an instance model to data with\nmach = machine(model, X, y)\nwhere:\n\nX: any table of input features (eg, a DataFrame) whose columns each have one of the following element scitypes: Continuous, Count, or &lt;:OrderedFactor; check column scitypes with schema(X)\ny: the target, which can be any AbstractVector whose element scitype is &lt;:OrderedFactor or &lt;:Multiclass; check the scitype with scitype(y)\n\nTrain the machine with fit!(mach, rows=...).\n\n\n\n\nn_iter=10: number of iterations of AdaBoost\nfeature_importance: method to use for computing feature importances. One of (:impurity, :split)\nrng=Random.GLOBAL_RNG: random number generator or seed\n\n\n\n\n\npredict(mach, Xnew): return predictions of the target given features Xnew having the same scitype as X above. Predictions are probabilistic, but uncalibrated.\npredict_mode(mach, Xnew): instead return the mode of each prediction above.\n\n\n\n\nThe fields of fitted_params(mach) are:\n\nstumps: the Ensemble object returned by the core DecisionTree.jl algorithm.\ncoefficients: the stump coefficients (one per stump)\n\n\n\n\n\nfeatures: the names of the features encountered in training\n\n\n\n\n\nfeature_importances(mach) returns a vector of (feature::Symbol =&gt; importance) pairs; the type of importance is determined by the hyperparameter feature_importance (see above)\n\n\n\n\nusing MLJ\nBooster = @load AdaBoostStumpClassifier pkg=DecisionTree\nbooster = Booster(n_iter=15)\n\nX, y = @load_iris\nmach = machine(booster, X, y) |&gt; fit!\n\nXnew = (sepal_length = [6.4, 7.2, 7.4],\n        sepal_width = [2.8, 3.0, 2.8],\n        petal_length = [5.6, 5.8, 6.1],\n        petal_width = [2.1, 1.6, 1.9],)\nyhat = predict(mach, Xnew) # probabilistic predictions\npredict_mode(mach, Xnew)   # point predictions\npdf.(yhat, \"virginica\")    # probabilities for the \"verginica\" class\n\nfitted_params(mach).stumps # raw `Ensemble` object from DecisionTree.jl\nfitted_params(mach).coefs  # coefficient associated with each stump\nfeature_importances(mach)\nSee also DecisionTree.jl and the unwrapped model type MLJDecisionTreeInterface.DecisionTree.AdaBoostStumpClassifier.\n\n\n\n\n\n\nthere are too many models, so you just add code if repel notice missing package , you just install packages.\n\nTree = @load DecisionTreeClassifier pkg=DecisionTree\n\nimport MLJDecisionTreeInterface ✔\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n\n\nMLJDecisionTreeInterface.DecisionTreeClassifier\n\n\n\n\n\n\n tree=Tree()\n\nDecisionTreeClassifier(\n  max_depth = -1, \n  min_samples_leaf = 1, \n  min_samples_split = 2, \n  min_purity_increase = 0.0, \n  n_subfeatures = 0, \n  post_prune = false, \n  merge_purity_threshold = 1.0, \n  display_depth = 5, \n  feature_importance = :impurity, \n  rng = Random._GLOBAL_RNG())\n\n\n\n\n\n\nevaluate(tree, X, y,\n         resampling=CV(shuffle=true),\n         measures=[log_loss, accuracy],\n         verbosity=0)\n\n\nPerformanceEvaluation object with these fields:\n  model, measure, operation, measurement, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_rows, resampling, repeats\nExtract:\n┌──────────────────────┬──────────────┬─────────────┬─────────┬─────────────────\n│ measure              │ operation    │ measurement │ 1.96*SE │ per_fold       ⋯\n├──────────────────────┼──────────────┼─────────────┼─────────┼─────────────────\n│ LogLoss(             │ predict      │ 2.4         │ 1.53    │ [2.88, 2.22e-1 ⋯\n│   tol = 2.22045e-16) │              │             │         │                ⋯\n│ Accuracy()           │ predict_mode │ 0.933       │ 0.0425  │ [0.92, 1.0, 0. ⋯\n└──────────────────────┴──────────────┴─────────────┴─────────┴─────────────────\n                                                                1 column omitted\n\n\n\n\n\n\n\nfill model and data to machine\n\nmach=machine(tree, X,y)\n\nuntrained Machine; caches model-specific representations of data\n  model: DecisionTreeClassifier(max_depth = -1, …)\n  args: \n    1:  Source @124 ⏎ Table{AbstractVector{Continuous}}\n    2:  Source @591 ⏎ AbstractVector{Multiclass{3}}\n\n\n\n\n\n\ntrain, test = partition(eachindex(y), 0.7); \n\n\n\n\nuse train index to inform model to we must not use all data!!!\n\nfit!(mach, rows=train)\n\n[ Info: Training machine(DecisionTreeClassifier(max_depth = -1, …), …).\n\n\ntrained Machine; caches model-specific representations of data\n  model: DecisionTreeClassifier(max_depth = -1, …)\n  args: \n    1:  Source @124 ⏎ Table{AbstractVector{Continuous}}\n    2:  Source @591 ⏎ AbstractVector{Multiclass{3}}\n\n\n\n\n\nsame with other julia stats model , X must be a matrix ,even if it is one row and one column data\n\nyhat = predict_mode(mach, X[test,:])\n\n45-element CategoricalArrays.CategoricalArray{String15,1,UInt32}:\n String15(\"versicolor\")\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"virginica\")\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"versicolor\")\n String15(\"versicolor\")\n String15(\"virginica\")\n String15(\"versicolor\")\n String15(\"virginica\")\n String15(\"versicolor\")\n String15(\"virginica\")\n ⋮\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"virginica\")\n String15(\"virginica\")\n String15(\"virginica\")\n String15(\"versicolor\")\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"virginica\")\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"setosa\")\n\n\n\n\n\n\naccuracy(yhat,y[test])\n\n0.9555555555555556\n\n\n\nlevels(yhat)\n\n3-element Vector{String15}:\n \"setosa\"\n \"versicolor\"\n \"virginica\"",
    "crumbs": [
      "Start",
      "build first model"
    ]
  },
  {
    "objectID": "start/getting-start.html#getting-start",
    "href": "start/getting-start.html#getting-start",
    "title": "Build first MLJ machine learning model",
    "section": "",
    "text": "we followed workflow, step by step\n\n\nfor iris dataset ref: Iris Dataset EDA\n\nusing Pipe\nusing CSV,DataFrames\nusing MLJ\ndf=CSV.File(\"../data/iris.csv\")|&gt;DataFrame;\n\n\nschema(df)\n\n\n┌──────────────┬────────────┬──────────┐\n│ names        │ scitypes   │ types    │\n├──────────────┼────────────┼──────────┤\n│ sepal_length │ Continuous │ Float64  │\n│ sepal_width  │ Continuous │ Float64  │\n│ petal_length │ Continuous │ Float64  │\n│ petal_width  │ Continuous │ Float64  │\n│ species      │ Textual    │ String15 │\n└──────────────┴────────────┴──────────┘\n\n\n\n\nfor species need transform to Categorical type\n\ndf.species= coerce(df.species,Multiclass);\nschema(df)\n\n\n┌──────────────┬───────────────┬────────────────────────────────────┐\n│ names        │ scitypes      │ types                              │\n├──────────────┼───────────────┼────────────────────────────────────┤\n│ sepal_length │ Continuous    │ Float64                            │\n│ sepal_width  │ Continuous    │ Float64                            │\n│ petal_length │ Continuous    │ Float64                            │\n│ petal_width  │ Continuous    │ Float64                            │\n│ species      │ Multiclass{3} │ CategoricalValue{String15, UInt32} │\n└──────────────┴───────────────┴────────────────────────────────────┘\n\n\n\n\nnow we need to split data\n\ny, X = unpack(df, ==(:species); rng=123)\n\n\n(CategoricalArrays.CategoricalValue{String15, UInt32}[String15(\"virginica\"), String15(\"versicolor\"), String15(\"virginica\"), String15(\"setosa\"), String15(\"versicolor\"), String15(\"virginica\"), String15(\"setosa\"), String15(\"versicolor\"), String15(\"setosa\"), String15(\"virginica\")  …  String15(\"virginica\"), String15(\"virginica\"), String15(\"virginica\"), String15(\"versicolor\"), String15(\"virginica\"), String15(\"setosa\"), String15(\"virginica\"), String15(\"virginica\"), String15(\"setosa\"), String15(\"setosa\")], 150×4 DataFrame\n Row │ sepal_length  sepal_width  petal_length  petal_width \n     │ Float64       Float64      Float64       Float64     \n─────┼──────────────────────────────────────────────────────\n   1 │          6.7          3.3           5.7          2.1\n   2 │          5.7          2.8           4.1          1.3\n   3 │          7.2          3.0           5.8          1.6\n   4 │          4.4          2.9           1.4          0.2\n   5 │          5.6          2.5           3.9          1.1\n   6 │          6.5          3.0           5.2          2.0\n   7 │          4.4          3.0           1.3          0.2\n   8 │          6.1          2.9           4.7          1.4\n   9 │          5.4          3.9           1.7          0.4\n  10 │          4.9          2.5           4.5          1.7\n  11 │          6.3          2.5           4.9          1.5\n  ⋮  │      ⋮             ⋮            ⋮             ⋮\n 141 │          6.4          2.7           5.3          1.9\n 142 │          6.8          3.2           5.9          2.3\n 143 │          6.9          3.1           5.4          2.1\n 144 │          6.1          2.8           4.0          1.3\n 145 │          6.7          2.5           5.8          1.8\n 146 │          5.0          3.5           1.3          0.3\n 147 │          7.6          3.0           6.6          2.1\n 148 │          6.3          2.5           5.0          1.9\n 149 │          5.1          3.8           1.6          0.2\n 150 │          5.0          3.6           1.4          0.2\n                                            129 rows omitted)\n\n\n\n\n\n\n\nres=@pipe models(matching(X,y))|&gt; DataFrame|&gt;select(_,[:name,:package_name])\n\n53×2 DataFrame28 rows omitted\n\n\n\nRow\nname\npackage_name\n\n\n\nString\nString\n\n\n\n\n1\nAdaBoostClassifier\nMLJScikitLearnInterface\n\n\n2\nAdaBoostStumpClassifier\nDecisionTree\n\n\n3\nBaggingClassifier\nMLJScikitLearnInterface\n\n\n4\nBayesianLDA\nMLJScikitLearnInterface\n\n\n5\nBayesianLDA\nMultivariateStats\n\n\n6\nBayesianQDA\nMLJScikitLearnInterface\n\n\n7\nBayesianSubspaceLDA\nMultivariateStats\n\n\n8\nCatBoostClassifier\nCatBoost\n\n\n9\nConstantClassifier\nMLJModels\n\n\n10\nDecisionTreeClassifier\nBetaML\n\n\n11\nDecisionTreeClassifier\nDecisionTree\n\n\n12\nDeterministicConstantClassifier\nMLJModels\n\n\n13\nDummyClassifier\nMLJScikitLearnInterface\n\n\n⋮\n⋮\n⋮\n\n\n42\nRandomForestClassifier\nMLJScikitLearnInterface\n\n\n43\nRidgeCVClassifier\nMLJScikitLearnInterface\n\n\n44\nRidgeClassifier\nMLJScikitLearnInterface\n\n\n45\nSGDClassifier\nMLJScikitLearnInterface\n\n\n46\nSVC\nLIBSVM\n\n\n47\nSVMClassifier\nMLJScikitLearnInterface\n\n\n48\nSVMLinearClassifier\nMLJScikitLearnInterface\n\n\n49\nSVMNuClassifier\nMLJScikitLearnInterface\n\n\n50\nStableForestClassifier\nSIRUS\n\n\n51\nStableRulesClassifier\nSIRUS\n\n\n52\nSubspaceLDA\nMultivariateStats\n\n\n53\nXGBoostClassifier\nXGBoost\n\n\n\n\n\n\nsimple way to look into model details:\ndoc($name, pkg=$package_name)\n\ndoc(\"AdaBoostStumpClassifier\", pkg=\"DecisionTree\")\n\nAdaBoostStumpClassifier\nA model type for constructing a Ada-boosted stump classifier, based on DecisionTree.jl, and implementing the MLJ model interface.\nFrom MLJ, the type can be imported using\nAdaBoostStumpClassifier = @load AdaBoostStumpClassifier pkg=DecisionTree\nDo model = AdaBoostStumpClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in AdaBoostStumpClassifier(n_iter=...).\n\n\nIn MLJ or MLJBase, bind an instance model to data with\nmach = machine(model, X, y)\nwhere:\n\nX: any table of input features (eg, a DataFrame) whose columns each have one of the following element scitypes: Continuous, Count, or &lt;:OrderedFactor; check column scitypes with schema(X)\ny: the target, which can be any AbstractVector whose element scitype is &lt;:OrderedFactor or &lt;:Multiclass; check the scitype with scitype(y)\n\nTrain the machine with fit!(mach, rows=...).\n\n\n\n\nn_iter=10: number of iterations of AdaBoost\nfeature_importance: method to use for computing feature importances. One of (:impurity, :split)\nrng=Random.GLOBAL_RNG: random number generator or seed\n\n\n\n\n\npredict(mach, Xnew): return predictions of the target given features Xnew having the same scitype as X above. Predictions are probabilistic, but uncalibrated.\npredict_mode(mach, Xnew): instead return the mode of each prediction above.\n\n\n\n\nThe fields of fitted_params(mach) are:\n\nstumps: the Ensemble object returned by the core DecisionTree.jl algorithm.\ncoefficients: the stump coefficients (one per stump)\n\n\n\n\n\nfeatures: the names of the features encountered in training\n\n\n\n\n\nfeature_importances(mach) returns a vector of (feature::Symbol =&gt; importance) pairs; the type of importance is determined by the hyperparameter feature_importance (see above)\n\n\n\n\nusing MLJ\nBooster = @load AdaBoostStumpClassifier pkg=DecisionTree\nbooster = Booster(n_iter=15)\n\nX, y = @load_iris\nmach = machine(booster, X, y) |&gt; fit!\n\nXnew = (sepal_length = [6.4, 7.2, 7.4],\n        sepal_width = [2.8, 3.0, 2.8],\n        petal_length = [5.6, 5.8, 6.1],\n        petal_width = [2.1, 1.6, 1.9],)\nyhat = predict(mach, Xnew) # probabilistic predictions\npredict_mode(mach, Xnew)   # point predictions\npdf.(yhat, \"virginica\")    # probabilities for the \"verginica\" class\n\nfitted_params(mach).stumps # raw `Ensemble` object from DecisionTree.jl\nfitted_params(mach).coefs  # coefficient associated with each stump\nfeature_importances(mach)\nSee also DecisionTree.jl and the unwrapped model type MLJDecisionTreeInterface.DecisionTree.AdaBoostStumpClassifier.\n\n\n\n\n\n\nthere are too many models, so you just add code if repel notice missing package , you just install packages.\n\nTree = @load DecisionTreeClassifier pkg=DecisionTree\n\nimport MLJDecisionTreeInterface ✔\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n\n\nMLJDecisionTreeInterface.DecisionTreeClassifier\n\n\n\n\n\n\n tree=Tree()\n\nDecisionTreeClassifier(\n  max_depth = -1, \n  min_samples_leaf = 1, \n  min_samples_split = 2, \n  min_purity_increase = 0.0, \n  n_subfeatures = 0, \n  post_prune = false, \n  merge_purity_threshold = 1.0, \n  display_depth = 5, \n  feature_importance = :impurity, \n  rng = Random._GLOBAL_RNG())\n\n\n\n\n\n\nevaluate(tree, X, y,\n         resampling=CV(shuffle=true),\n         measures=[log_loss, accuracy],\n         verbosity=0)\n\n\nPerformanceEvaluation object with these fields:\n  model, measure, operation, measurement, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_rows, resampling, repeats\nExtract:\n┌──────────────────────┬──────────────┬─────────────┬─────────┬─────────────────\n│ measure              │ operation    │ measurement │ 1.96*SE │ per_fold       ⋯\n├──────────────────────┼──────────────┼─────────────┼─────────┼─────────────────\n│ LogLoss(             │ predict      │ 2.4         │ 1.53    │ [2.88, 2.22e-1 ⋯\n│   tol = 2.22045e-16) │              │             │         │                ⋯\n│ Accuracy()           │ predict_mode │ 0.933       │ 0.0425  │ [0.92, 1.0, 0. ⋯\n└──────────────────────┴──────────────┴─────────────┴─────────┴─────────────────\n                                                                1 column omitted\n\n\n\n\n\n\n\nfill model and data to machine\n\nmach=machine(tree, X,y)\n\nuntrained Machine; caches model-specific representations of data\n  model: DecisionTreeClassifier(max_depth = -1, …)\n  args: \n    1:  Source @124 ⏎ Table{AbstractVector{Continuous}}\n    2:  Source @591 ⏎ AbstractVector{Multiclass{3}}\n\n\n\n\n\n\ntrain, test = partition(eachindex(y), 0.7); \n\n\n\n\nuse train index to inform model to we must not use all data!!!\n\nfit!(mach, rows=train)\n\n[ Info: Training machine(DecisionTreeClassifier(max_depth = -1, …), …).\n\n\ntrained Machine; caches model-specific representations of data\n  model: DecisionTreeClassifier(max_depth = -1, …)\n  args: \n    1:  Source @124 ⏎ Table{AbstractVector{Continuous}}\n    2:  Source @591 ⏎ AbstractVector{Multiclass{3}}\n\n\n\n\n\nsame with other julia stats model , X must be a matrix ,even if it is one row and one column data\n\nyhat = predict_mode(mach, X[test,:])\n\n45-element CategoricalArrays.CategoricalArray{String15,1,UInt32}:\n String15(\"versicolor\")\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"virginica\")\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"versicolor\")\n String15(\"versicolor\")\n String15(\"virginica\")\n String15(\"versicolor\")\n String15(\"virginica\")\n String15(\"versicolor\")\n String15(\"virginica\")\n ⋮\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"virginica\")\n String15(\"virginica\")\n String15(\"virginica\")\n String15(\"versicolor\")\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"virginica\")\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"setosa\")\n\n\n\n\n\n\naccuracy(yhat,y[test])\n\n0.9555555555555556\n\n\n\nlevels(yhat)\n\n3-element Vector{String15}:\n \"setosa\"\n \"versicolor\"\n \"virginica\"",
    "crumbs": [
      "Start",
      "build first model"
    ]
  },
  {
    "objectID": "packages/statesbase.html",
    "href": "packages/statesbase.html",
    "title": "StatsBase.jl",
    "section": "",
    "text": "StatsBase.jl"
  },
  {
    "objectID": "packages/glm.html",
    "href": "packages/glm.html",
    "title": "GLM.jl",
    "section": "",
    "text": "GLM.jl",
    "crumbs": [
      "GLM"
    ]
  },
  {
    "objectID": "learn/example.html",
    "href": "learn/example.html",
    "title": "Code-insertion Example",
    "section": "",
    "text": "quoted block added before the post"
  },
  {
    "objectID": "learn/example.html#heading",
    "href": "learn/example.html#heading",
    "title": "Code-insertion Example",
    "section": "Heading",
    "text": "Heading\nThis filter adds code immediately before and after the post."
  },
  {
    "objectID": "learn/example.html#appended-section-after-the-post",
    "href": "learn/example.html#appended-section-after-the-post",
    "title": "Code-insertion Example",
    "section": "Appended Section After the Post",
    "text": "Appended Section After the Post"
  },
  {
    "objectID": "dataset/iris.html",
    "href": "dataset/iris.html",
    "title": "🌺🌸Iris dataset",
    "section": "",
    "text": "info\n\n\n\nIRIS is a an famous dataset for statistics and data science. t has long history. we will use it angain and agian.rery time wo use iris dataset we can get new knowledget is like exercies for running",
    "crumbs": [
      "Dataset",
      "1.iris"
    ]
  },
  {
    "objectID": "dataset/iris.html#load-csv",
    "href": "dataset/iris.html#load-csv",
    "title": "🌺🌸Iris dataset",
    "section": "1. load csv",
    "text": "1. load csv\n\n using CSV,DataFrames,Tidier\n using CairoMakie,AlgebraOfGraphics\n\n #df=download(\"http://localhost:5220/data/iris.csv\")|&gt;CSV.File|&gt;DataFrame;\n df=CSV.File(\"../data/iris.csv\")|&gt;DataFrame\n first(df,5)\n\n5×5 DataFrame\n\n\n\nRow\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\nFloat64\nFloat64\nFloat64\nFloat64\nString15\n\n\n\n\n1\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n2\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n3\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\nCSV,DataFrames,Tidier 用于数据框处理\nCairoMakie,AlgebraOfGraphics用于绘图",
    "crumbs": [
      "Dataset",
      "1.iris"
    ]
  },
  {
    "objectID": "dataset/iris.html#describe-dataframe",
    "href": "dataset/iris.html#describe-dataframe",
    "title": "🌺🌸Iris dataset",
    "section": "2. describe dataframe",
    "text": "2. describe dataframe\n\ndescribe(df)\n\n5×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nDataType\n\n\n\n\n1\nsepal_length\n5.84333\n4.3\n5.8\n7.9\n0\nFloat64\n\n\n2\nsepal_width\n3.054\n2.0\n3.0\n4.4\n0\nFloat64\n\n\n3\npetal_length\n3.75867\n1.0\n4.35\n6.9\n0\nFloat64\n\n\n4\npetal_width\n1.19867\n0.1\n1.3\n2.5\n0\nFloat64\n\n\n5\nspecies\n\nsetosa\n\nvirginica\n0\nString15",
    "crumbs": [
      "Dataset",
      "1.iris"
    ]
  },
  {
    "objectID": "dataset/iris.html#eda",
    "href": "dataset/iris.html#eda",
    "title": "🌺🌸Iris dataset",
    "section": "3. EDA",
    "text": "3. EDA\n\n3.1 density plot\n\npalette = (color=tuple.([\"#FC7808\", \"#8C00EC\", \"#107A78\"], 0.65),\n           marker=[:circle, :utriangle, :rect])\ncycle = Cycle([:color, :marker], covary=true)\n\n\ndatalayer=data(df)\nmappinglayer1=mapping(:sepal_length,:sepal_width,color=:species)\nmappinglayer2=mapping(:sepal_length,color=:species)\nmappinglayer3=mapping(:sepal_width,color=:species)\n\nvisuallayer1=visual(Scatter,strokewidth=1,storkecolor=:black)\nvisuallayer2=visual(Density)\nvisuallayer3=visual(Density,direction=:y)\nplc=datalayer*mappinglayer1*visuallayer1  # top pic\nplt=datalayer*mappinglayer2*visuallayer2  # main pic\nplr=datalayer*mappinglayer3*visuallayer3  # right pic\n\nwith_theme(theme_light(),resolution = (600,400), palette=palette, Scatter=(cycle=cycle,)) do\n        fig = Figure()\n        axs = [Axis(fig[2,1], xlabel = \"sepal_length\", ylabel = \"sepal_width\"),\n            Axis(fig[1,1]), Axis(fig[2,2])]\n        dots = draw!(axs[1], plc)  # bject for lengend extract species info\n        draw!(axs[2], plt)\n        draw!(axs[3], plr)\n        # getting the right layout aspect\n        colsize!(fig.layout, 1, Auto(4.0))\n        rowsize!(fig.layout, 1, Auto(1/3))\n        colgap!(fig.layout,3)\n        rowgap!(fig.layout, 3)\n        linkxaxes!(axs[1], axs[2])\n        linkyaxes!(axs[1], axs[3])\n        hidedecorations!.(axs[2:3], grid=false)\n        \n        legend!(fig[1,2], dots)\n        fig\n    end\n\n\n\n\n\n\n\n\n\n\n3.2 pair plot\n\ncats=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\nlen=length(cats)\ncats=Symbol.(cats)\n\n\ndata_layer=data(df)\n\nfunction make_layer(sy1::Symbol,sy2::Symbol)\n   mapping_layer=sy1==sy2 ? mapping(sy1,color=:species) : mapping(sy1,sy2,color=:species)\n   vis_layer=sy1==sy2 ? visual(Density,color=:species) : visual(Scatter,color=:species,strokewidth=1,storkecolor=:black)\n   return  data_layer*mapping_layer*vis_layer\nend\n\nwith_theme(theme_minimal(),resolution = (800,800), palette=palette, Scatter=(cycle=cycle,)) do\n        cl=0.1\n        fig = Figure()\n        for i in 1:len\n            for  j in 1:len\n                ax=Axis(fig[i,j])\n                j==1 ? ax.ylabel= String(cats[i]) : nothing\n                i==len ? ax.xlabel= String(cats[j]) : nothing\n                layer=make_layer(cats[i],cats[j])\n              [i,j]==[len,len] ? cl=draw!(ax,layer) : draw!(ax,layer)\n            end\n        end\n        legend!(fig[2:3,5], cl)\n        fig\nend\n\n\n\n\n\n\n\n\n\n\n3.3 box plot\ntransform four feature to attribute and value\n\nlong_data=@pivot_longer(df, 1:5, names_to = attribute, values_to = value,-species)\nfirst(long_data,5)\n\n5×3 DataFrame\n\n\n\nRow\nspecies\nattribute\nvalue\n\n\n\nString15\nString\nFloat64\n\n\n\n\n1\nsetosa\nsepal_length\n5.1\n\n\n2\nsetosa\nsepal_length\n4.9\n\n\n3\nsetosa\nsepal_length\n4.7\n\n\n4\nsetosa\nsepal_length\n4.6\n\n\n5\nsetosa\nsepal_length\n5.0\n\n\n\n\n\n\n\ndata(long_data) * visual(BoxPlot, show_notch=true) *\n    mapping(:attribute, :value, color=:species, dodge=:species) |&gt; draw\n\n\n\n\n\n\n\n\n\n\n3.4 correlation of numerical variables",
    "crumbs": [
      "Dataset",
      "1.iris"
    ]
  },
  {
    "objectID": "dataset/boston-house.html",
    "href": "dataset/boston-house.html",
    "title": "🏠🏡🏣 Boston Housing dataset",
    "section": "",
    "text": "info\n\n\n\nBoston Housing is another famous dataset",
    "crumbs": [
      "Dataset",
      "3.boston housing"
    ]
  },
  {
    "objectID": "dataset/boston-house.html#load-csv",
    "href": "dataset/boston-house.html#load-csv",
    "title": "🏠🏡🏣 Boston Housing dataset",
    "section": "1. load csv",
    "text": "1. load csv\n\n using CSV,DataFrames,Tidier\n using CairoMakie,AlgebraOfGraphics\n\ndf=CSV.File(\"../data/housing.csv\")|&gt;DataFrame\n first(df,5)\n\n5×14 DataFrame\n\n\n\nRow\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nb\nlstat\nmedv\n\n\n\nFloat64\nFloat64\nFloat64\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\nInt64\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n0.00632\n18.0\n2.31\n0\n0.538\n6.575\n65.2\n4.09\n1\n296\n15.3\n396.9\n4.98\n24.0\n\n\n2\n0.02731\n0.0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242\n17.8\n396.9\n9.14\n21.6\n\n\n3\n0.02729\n0.0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242\n17.8\n392.83\n4.03\n34.7\n\n\n4\n0.03237\n0.0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222\n18.7\n394.63\n2.94\n33.4\n\n\n5\n0.06905\n0.0\n2.18\n0\n0.458\n7.147\n54.2\n6.0622\n3\n222\n18.7\n396.9\n5.33\n36.2",
    "crumbs": [
      "Dataset",
      "3.boston housing"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "dataset/index.html",
    "href": "dataset/index.html",
    "title": "Make friends with data!",
    "section": "",
    "text": "first step to start data science is recognize your data only you get familiar with the data , then make flow work",
    "crumbs": [
      "Dataset",
      "intro"
    ]
  },
  {
    "objectID": "dataset/penguins.html",
    "href": "dataset/penguins.html",
    "title": "🐧🐧 Penguins dataset",
    "section": "",
    "text": "info\n\n\n\nPenguins data set is another famous dataset for data science, compare with Iris, Penguins has more categorical feature.\nplease reference :\n\nPalmer penguins website\n[Aog Tutorial 🐧]",
    "crumbs": [
      "Dataset",
      "2.penguins"
    ]
  },
  {
    "objectID": "dataset/penguins.html#load-csv",
    "href": "dataset/penguins.html#load-csv",
    "title": "🐧🐧 Penguins dataset",
    "section": "1. load csv",
    "text": "1. load csv\n\nusing CSV,DataFrames,Tidier\nusing CairoMakie,AlgebraOfGraphics,MakieThemes\nMakie.set_theme!(ggthemr(:dust))\n\ndf=CSV.File(\"../data/penguine_data.csv\")|&gt;DataFrame\nfirst(df,5)\n\n5×7 DataFrame\n\n\n\nRow\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\nString15\nString15\nString7\nString7\nString3\nString7\nString7\n\n\n\n\n1\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmale\n\n\n2\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nfemale\n\n\n3\nAdelie\nTorgersen\n40.3\n18\n195\n3250\nfemale\n\n\n4\nAdelie\nTorgersen\nNA\nNA\nNA\nNA\nNA\n\n\n5\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale",
    "crumbs": [
      "Dataset",
      "2.penguins"
    ]
  },
  {
    "objectID": "dataset/penguins.html#describe-dataframe",
    "href": "dataset/penguins.html#describe-dataframe",
    "title": "🐧🐧 Penguins dataset",
    "section": "2. describe dataframe",
    "text": "2. describe dataframe\n\ndescribe(df)\n\n7×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nNothing\nInlineSt…\nNothing\nInlineSt…\nInt64\nDataType\n\n\n\n\n1\nspecies\n\nAdelie\n\nGentoo\n0\nString15\n\n\n2\nisland\n\nBiscoe\n\nTorgersen\n0\nString15\n\n\n3\nbill_length_mm\n\n32.1\n\nNA\n0\nString7\n\n\n4\nbill_depth_mm\n\n13.1\n\nNA\n0\nString7\n\n\n5\nflipper_length_mm\n\n172\n\nNA\n0\nString3\n\n\n6\nbody_mass_g\n\n2700\n\nNA\n0\nString7\n\n\n7\nsex\n\nNA\n\nmale\n0\nString7",
    "crumbs": [
      "Dataset",
      "2.penguins"
    ]
  },
  {
    "objectID": "dataset/penguins.html#eda",
    "href": "dataset/penguins.html#eda",
    "title": "🐧🐧 Penguins dataset",
    "section": "3. EDA",
    "text": "3. EDA\n\n3.1 frequency\n\ndatalayer=data(df)\nmappinglayer=frequency()*mapping(:species,color = :island,dodge = :island)\naxis = (width = 225, height = 225)\ndraw(datalayer*mappinglayer,axis=axis)\n\n\n\n\n\n\n\nFigure 1: fig-penguins-fequency\n\n\n\n\n\n\n\n3.2 correlation of two variables\n\n#show(names(df))\ncats=[\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\nlen=length(cats)\ncats=Symbol.(cats)\n\n4-element Vector{Symbol}:\n :bill_length_mm\n :bill_depth_mm\n :flipper_length_mm\n :body_mass_g\n\n\n\npalette = (color=tuple.([\"#FC7808\", \"#8C00EC\", \"#107A78\"], 0.65),\n           marker=[:circle, :utriangle, :rect])\ncycle = Cycle([:color, :marker], covary=true)\n\nfunction make_layer(sy1::Symbol,sy2::Symbol)\n   mapping_layer=sy1==sy2 ? mapping(sy1,color=:species) : mapping(sy1,sy2,color=:species)\n   vis_layer=sy1==sy2 ? visual(Density,color=:species) : visual(Scatter,color=:species,strokewidth=1,storkecolor=:black)\n   return  datalayer*mapping_layer*vis_layer\nend\n\nwith_theme(ggthemr(:dust),resolution = (800,800), palette=palette, Scatter=(cycle=cycle,)) do\n        cl=0.1\n        fig = Figure()\n        for i in 1:len\n            for  j in 1:len\n                ax=Axis(fig[i,j])\n                j==1 ? ax.ylabel= String(cats[i]) : nothing\n                i==len ? ax.xlabel= String(cats[j]) : nothing\n                layer=make_layer(cats[i],cats[j])\n              [i,j]==[len,len] ? cl=draw!(ax,layer) : draw!(ax,layer)\n            end\n        end\n        legend!(fig[2:3,5], cl)\n        fig\nend\n\n\n\n\n\n\n\nFigure 2: fig-penguins-pairplot\n\n\n\n\n\n\n\n3.3 varying weights of the penguins\n\nax=(width = 225, height = 225)\nmappinglayer2=mapping(:species, :bill_depth_mm;color=:species)\nvislayer2=visual(BoxPlot,show_notch=true)\ndraw(datalayer*mappinglayer2*vislayer2,axis=ax)\n\n\n\n\n\n\n\nFigure 3: fig-penguins-bodymass",
    "crumbs": [
      "Dataset",
      "2.penguins"
    ]
  },
  {
    "objectID": "learn/Julia-MachineLearning.html",
    "href": "learn/Julia-MachineLearning.html",
    "title": "Julia-MachineLearning",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "learn/Julia-MachineLearning.html#quarto",
    "href": "learn/Julia-MachineLearning.html#quarto",
    "title": "Julia-MachineLearning",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "learn/index.html",
    "href": "learn/index.html",
    "title": "Leanring",
    "section": "",
    "text": "Learn",
    "crumbs": [
      "Learn"
    ]
  },
  {
    "objectID": "packages/index.html",
    "href": "packages/index.html",
    "title": "Packages",
    "section": "",
    "text": "List\n\nGLM.jl"
  },
  {
    "objectID": "start/index.html",
    "href": "start/index.html",
    "title": "Start",
    "section": "",
    "text": "Start",
    "crumbs": [
      "Start",
      "GET STARTED"
    ]
  },
  {
    "objectID": "workflow/index.html",
    "href": "workflow/index.html",
    "title": "MLJ workflow",
    "section": "",
    "text": "graph TD\n    A[(Data)] --&gt;|Tidy Data| B(Ingestion)\n    B --&gt; C{Model Search}\n    C --&gt;|One| D[Regression]\n    C --&gt;|Two| E[Classification]\n    C --&gt;|Three| F[Dimension Reduction]\n    C --&gt;|Four| G[Ensemble Models]\n    C --&gt;|Five| H[Other Models]\n    D--&gt;J(Instantiating Model)\n    E--&gt;J\n    F--&gt;J\n    G--&gt;J\n    H--&gt;J\n    J--&gt;K(Evaluating Model)\n    K--&gt;L(Inspecting training results)\n    L--&gt;M(hyperparameter tuning)\n    M--&gt;N(Constructing linear pipelines)\n    N--&gt;O(ensemble of models) \n    O--&gt;P(Performance curves)\n\n\n\n\n\ngraph TD\n    A[(Data)] --&gt;|Tidy Data| B(Ingestion)\n    B --&gt; C{Model Search}\n    C --&gt;|One| D[Regression]\n    C --&gt;|Two| E[Classification]\n    C --&gt;|Three| F[Dimension Reduction]\n    C --&gt;|Four| G[Ensemble Models]\n    C --&gt;|Five| H[Other Models]\n    D--&gt;J(Instantiating Model)\n    E--&gt;J\n    F--&gt;J\n    G--&gt;J\n    H--&gt;J\n    J--&gt;K(Evaluating Model)\n    K--&gt;L(Inspecting training results)\n    L--&gt;M(hyperparameter tuning)\n    M--&gt;N(Constructing linear pipelines)\n    N--&gt;O(ensemble of models) \n    O--&gt;P(Performance curves)\n\n\n\n\nFigure 1: MLJ main work flow",
    "crumbs": [
      "Workflow",
      "flowchart"
    ]
  },
  {
    "objectID": "workflow/index.html#common-workflow",
    "href": "workflow/index.html#common-workflow",
    "title": "MLJ workflow",
    "section": "",
    "text": "graph TD\n    A[(Data)] --&gt;|Tidy Data| B(Ingestion)\n    B --&gt; C{Model Search}\n    C --&gt;|One| D[Regression]\n    C --&gt;|Two| E[Classification]\n    C --&gt;|Three| F[Dimension Reduction]\n    C --&gt;|Four| G[Ensemble Models]\n    C --&gt;|Five| H[Other Models]\n    D--&gt;J(Instantiating Model)\n    E--&gt;J\n    F--&gt;J\n    G--&gt;J\n    H--&gt;J\n    J--&gt;K(Evaluating Model)\n    K--&gt;L(Inspecting training results)\n    L--&gt;M(hyperparameter tuning)\n    M--&gt;N(Constructing linear pipelines)\n    N--&gt;O(ensemble of models) \n    O--&gt;P(Performance curves)\n\n\n\n\n\ngraph TD\n    A[(Data)] --&gt;|Tidy Data| B(Ingestion)\n    B --&gt; C{Model Search}\n    C --&gt;|One| D[Regression]\n    C --&gt;|Two| E[Classification]\n    C --&gt;|Three| F[Dimension Reduction]\n    C --&gt;|Four| G[Ensemble Models]\n    C --&gt;|Five| H[Other Models]\n    D--&gt;J(Instantiating Model)\n    E--&gt;J\n    F--&gt;J\n    G--&gt;J\n    H--&gt;J\n    J--&gt;K(Evaluating Model)\n    K--&gt;L(Inspecting training results)\n    L--&gt;M(hyperparameter tuning)\n    M--&gt;N(Constructing linear pipelines)\n    N--&gt;O(ensemble of models) \n    O--&gt;P(Performance curves)\n\n\n\n\nFigure 1: MLJ main work flow",
    "crumbs": [
      "Workflow",
      "flowchart"
    ]
  }
]
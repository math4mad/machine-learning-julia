[
  {
    "objectID": "workflow/tailwindcss.html",
    "href": "workflow/tailwindcss.html",
    "title": "embedding tailwind css",
    "section": "",
    "text": "test tailwind css\n\nExample heading New\n\n\nNotifications 4\n\n\nâ¶ â· â¸ â¹ âº â… â† â‡ âˆ â‰\n\n\nâ¶ â· â¸ â¹ âº â… â† â‡ âˆ â‰\n\n\n\n\nFirst Name\n\n\nLast Name\n\n\nPoints\n\n\n\n\nJill\n\n\nSmith\n\n\n50\n\n\n\n\nJill\n\n\nSmith\n\n\n50\n\n\n\n\nJill\n\n\nSmith\n\n\n50"
  },
  {
    "objectID": "workflow/composing-models.html",
    "href": "workflow/composing-models.html",
    "title": "Composing Models",
    "section": "",
    "text": "MLJ ä¸­ç»„åˆæ¨¡å‹æœ‰ä¸‰ç§å½¢å¼\n\n\n\n\n\n    %%| label: fig-mlj-composing model\n    %%| fig-cap: \"MLJ  composing model\"\n    %%| fig-width: 6.5\n    %%| echo: true\n    graph TD\n        C(Composing Models)\n        C --&gt;|One| D([Pipeline Model ])\n        C --&gt;|Two| E([Ensemble Model])\n        C --&gt;|Three| F([Stack Model])",
    "crumbs": [
      "Workflow",
      "composing models"
    ]
  },
  {
    "objectID": "start/getting-start.html",
    "href": "start/getting-start.html",
    "title": "Build first MLJ machine learning model",
    "section": "",
    "text": "we followed workflow, step by step\n\n\nfor iris dataset ref: Iris Dataset EDA\n\nusing Pipe\nusing CSV,DataFrames\nusing MLJ\ndf=CSV.File(\"../data/iris.csv\")|&gt;DataFrame;\n\n\nschema(df)\n\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ names        â”‚ scitypes   â”‚ types    â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ sepal_length â”‚ Continuous â”‚ Float64  â”‚\nâ”‚ sepal_width  â”‚ Continuous â”‚ Float64  â”‚\nâ”‚ petal_length â”‚ Continuous â”‚ Float64  â”‚\nâ”‚ petal_width  â”‚ Continuous â”‚ Float64  â”‚\nâ”‚ species      â”‚ Textual    â”‚ String15 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n\nfor species need transform to Categorical type\n\ndf.species= coerce(df.species,Multiclass);\nschema(df)\n\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ names        â”‚ scitypes      â”‚ types                              â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ sepal_length â”‚ Continuous    â”‚ Float64                            â”‚\nâ”‚ sepal_width  â”‚ Continuous    â”‚ Float64                            â”‚\nâ”‚ petal_length â”‚ Continuous    â”‚ Float64                            â”‚\nâ”‚ petal_width  â”‚ Continuous    â”‚ Float64                            â”‚\nâ”‚ species      â”‚ Multiclass{3} â”‚ CategoricalValue{String15, UInt32} â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n\nnow we need to split data\n\ny, X = unpack(df, ==(:species); rng=123)\n\n\n(CategoricalArrays.CategoricalValue{String15, UInt32}[String15(\"virginica\"), String15(\"versicolor\"), String15(\"virginica\"), String15(\"setosa\"), String15(\"versicolor\"), String15(\"virginica\"), String15(\"setosa\"), String15(\"versicolor\"), String15(\"setosa\"), String15(\"virginica\")  â€¦  String15(\"virginica\"), String15(\"virginica\"), String15(\"virginica\"), String15(\"versicolor\"), String15(\"virginica\"), String15(\"setosa\"), String15(\"virginica\"), String15(\"virginica\"), String15(\"setosa\"), String15(\"setosa\")], 150Ã—4 DataFrame\n Row â”‚ sepal_length  sepal_width  petal_length  petal_width \n     â”‚ Float64       Float64      Float64       Float64     \nâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n   1 â”‚          6.7          3.3           5.7          2.1\n   2 â”‚          5.7          2.8           4.1          1.3\n   3 â”‚          7.2          3.0           5.8          1.6\n   4 â”‚          4.4          2.9           1.4          0.2\n   5 â”‚          5.6          2.5           3.9          1.1\n   6 â”‚          6.5          3.0           5.2          2.0\n   7 â”‚          4.4          3.0           1.3          0.2\n   8 â”‚          6.1          2.9           4.7          1.4\n   9 â”‚          5.4          3.9           1.7          0.4\n  10 â”‚          4.9          2.5           4.5          1.7\n  11 â”‚          6.3          2.5           4.9          1.5\n  â‹®  â”‚      â‹®             â‹®            â‹®             â‹®\n 141 â”‚          6.4          2.7           5.3          1.9\n 142 â”‚          6.8          3.2           5.9          2.3\n 143 â”‚          6.9          3.1           5.4          2.1\n 144 â”‚          6.1          2.8           4.0          1.3\n 145 â”‚          6.7          2.5           5.8          1.8\n 146 â”‚          5.0          3.5           1.3          0.3\n 147 â”‚          7.6          3.0           6.6          2.1\n 148 â”‚          6.3          2.5           5.0          1.9\n 149 â”‚          5.1          3.8           1.6          0.2\n 150 â”‚          5.0          3.6           1.4          0.2\n                                            129 rows omitted)\n\n\n\n\n\n\n\nres=@pipe models(matching(X,y))|&gt; DataFrame|&gt;select(_,[:name,:package_name])\n\n53Ã—2 DataFrame28 rows omitted\n\n\n\nRow\nname\npackage_name\n\n\n\nString\nString\n\n\n\n\n1\nAdaBoostClassifier\nMLJScikitLearnInterface\n\n\n2\nAdaBoostStumpClassifier\nDecisionTree\n\n\n3\nBaggingClassifier\nMLJScikitLearnInterface\n\n\n4\nBayesianLDA\nMLJScikitLearnInterface\n\n\n5\nBayesianLDA\nMultivariateStats\n\n\n6\nBayesianQDA\nMLJScikitLearnInterface\n\n\n7\nBayesianSubspaceLDA\nMultivariateStats\n\n\n8\nCatBoostClassifier\nCatBoost\n\n\n9\nConstantClassifier\nMLJModels\n\n\n10\nDecisionTreeClassifier\nBetaML\n\n\n11\nDecisionTreeClassifier\nDecisionTree\n\n\n12\nDeterministicConstantClassifier\nMLJModels\n\n\n13\nDummyClassifier\nMLJScikitLearnInterface\n\n\nâ‹®\nâ‹®\nâ‹®\n\n\n42\nRandomForestClassifier\nMLJScikitLearnInterface\n\n\n43\nRidgeCVClassifier\nMLJScikitLearnInterface\n\n\n44\nRidgeClassifier\nMLJScikitLearnInterface\n\n\n45\nSGDClassifier\nMLJScikitLearnInterface\n\n\n46\nSVC\nLIBSVM\n\n\n47\nSVMClassifier\nMLJScikitLearnInterface\n\n\n48\nSVMLinearClassifier\nMLJScikitLearnInterface\n\n\n49\nSVMNuClassifier\nMLJScikitLearnInterface\n\n\n50\nStableForestClassifier\nSIRUS\n\n\n51\nStableRulesClassifier\nSIRUS\n\n\n52\nSubspaceLDA\nMultivariateStats\n\n\n53\nXGBoostClassifier\nXGBoost\n\n\n\n\n\n\nsimple way to look into model details:\ndoc($name, pkg=$package_name)\n\ndoc(\"AdaBoostStumpClassifier\", pkg=\"DecisionTree\")\n\nAdaBoostStumpClassifier\nA model type for constructing a Ada-boosted stump classifier, based on DecisionTree.jl, and implementing the MLJ model interface.\nFrom MLJ, the type can be imported using\nAdaBoostStumpClassifier = @load AdaBoostStumpClassifier pkg=DecisionTree\nDo model = AdaBoostStumpClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in AdaBoostStumpClassifier(n_iter=...).\n\n\nIn MLJ or MLJBase, bind an instance model to data with\nmach = machine(model, X, y)\nwhere:\n\nX: any table of input features (eg, a DataFrame) whose columns each have one of the following element scitypes: Continuous, Count, or &lt;:OrderedFactor; check column scitypes with schema(X)\ny: the target, which can be any AbstractVector whose element scitype is &lt;:OrderedFactor or &lt;:Multiclass; check the scitype with scitype(y)\n\nTrain the machine with fit!(mach, rows=...).\n\n\n\n\nn_iter=10: number of iterations of AdaBoost\nfeature_importance: method to use for computing feature importances. One of (:impurity, :split)\nrng=Random.GLOBAL_RNG: random number generator or seed\n\n\n\n\n\npredict(mach, Xnew): return predictions of the target given features Xnew having the same scitype as X above. Predictions are probabilistic, but uncalibrated.\npredict_mode(mach, Xnew): instead return the mode of each prediction above.\n\n\n\n\nThe fields of fitted_params(mach) are:\n\nstumps: the Ensemble object returned by the core DecisionTree.jl algorithm.\ncoefficients: the stump coefficients (one per stump)\n\n\n\n\n\nfeatures: the names of the features encountered in training\n\n\n\n\n\nfeature_importances(mach) returns a vector of (feature::Symbol =&gt; importance) pairs; the type of importance is determined by the hyperparameter feature_importance (see above)\n\n\n\n\nusing MLJ\nBooster = @load AdaBoostStumpClassifier pkg=DecisionTree\nbooster = Booster(n_iter=15)\n\nX, y = @load_iris\nmach = machine(booster, X, y) |&gt; fit!\n\nXnew = (sepal_length = [6.4, 7.2, 7.4],\n        sepal_width = [2.8, 3.0, 2.8],\n        petal_length = [5.6, 5.8, 6.1],\n        petal_width = [2.1, 1.6, 1.9],)\nyhat = predict(mach, Xnew) # probabilistic predictions\npredict_mode(mach, Xnew)   # point predictions\npdf.(yhat, \"virginica\")    # probabilities for the \"verginica\" class\n\nfitted_params(mach).stumps # raw `Ensemble` object from DecisionTree.jl\nfitted_params(mach).coefs  # coefficient associated with each stump\nfeature_importances(mach)\nSee also DecisionTree.jl and the unwrapped model type MLJDecisionTreeInterface.DecisionTree.AdaBoostStumpClassifier.\n\n\n\n\n\n\nthere are too many models, so you just add code if repel notice missing package , you just install packages.\n\nTree = @load DecisionTreeClassifier pkg=DecisionTree\n\nimport MLJDecisionTreeInterface âœ”\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n\n\nMLJDecisionTreeInterface.DecisionTreeClassifier\n\n\n\n\n\n\n tree=Tree()\n\nDecisionTreeClassifier(\n  max_depth = -1, \n  min_samples_leaf = 1, \n  min_samples_split = 2, \n  min_purity_increase = 0.0, \n  n_subfeatures = 0, \n  post_prune = false, \n  merge_purity_threshold = 1.0, \n  display_depth = 5, \n  feature_importance = :impurity, \n  rng = Random._GLOBAL_RNG())\n\n\n\n\n\n\nevaluate(tree, X, y,\n         resampling=CV(shuffle=true),\n         measures=[log_loss, accuracy],\n         verbosity=0)\n\n\nPerformanceEvaluation object with these fields:\n  model, measure, operation, measurement, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_rows, resampling, repeats\nExtract:\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”‚ measure              â”‚ operation    â”‚ measurement â”‚ 1.96*SE â”‚ per_fold       â‹¯\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”‚ LogLoss(             â”‚ predict      â”‚ 2.4         â”‚ 1.53    â”‚ [2.88, 2.22e-1 â‹¯\nâ”‚   tol = 2.22045e-16) â”‚              â”‚             â”‚         â”‚                â‹¯\nâ”‚ Accuracy()           â”‚ predict_mode â”‚ 0.933       â”‚ 0.0425  â”‚ [0.92, 1.0, 0. â‹¯\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n                                                                1 column omitted\n\n\n\n\n\n\n\nfill model and data to machine\n\nmach=machine(tree, X,y)\n\nuntrained Machine; caches model-specific representations of data\n  model: DecisionTreeClassifier(max_depth = -1, â€¦)\n  args: \n    1:  Source @124 â Table{AbstractVector{Continuous}}\n    2:  Source @591 â AbstractVector{Multiclass{3}}\n\n\n\n\n\n\ntrain, test = partition(eachindex(y), 0.7); \n\n\n\n\nuse train index to inform model to we must not use all data!!!\n\nfit!(mach, rows=train)\n\n[ Info: Training machine(DecisionTreeClassifier(max_depth = -1, â€¦), â€¦).\n\n\ntrained Machine; caches model-specific representations of data\n  model: DecisionTreeClassifier(max_depth = -1, â€¦)\n  args: \n    1:  Source @124 â Table{AbstractVector{Continuous}}\n    2:  Source @591 â AbstractVector{Multiclass{3}}\n\n\n\n\n\nsame with other julia stats model , X must be a matrix ,even if it is one row and one column data\n\nyhat = predict_mode(mach, X[test,:])\n\n45-element CategoricalArrays.CategoricalArray{String15,1,UInt32}:\n String15(\"versicolor\")\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"virginica\")\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"versicolor\")\n String15(\"versicolor\")\n String15(\"virginica\")\n String15(\"versicolor\")\n String15(\"virginica\")\n String15(\"versicolor\")\n String15(\"virginica\")\n â‹®\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"virginica\")\n String15(\"virginica\")\n String15(\"virginica\")\n String15(\"versicolor\")\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"virginica\")\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"setosa\")\n\n\n\n\n\n\naccuracy(yhat,y[test])\n\n0.9555555555555556\n\n\n\nlevels(yhat)\n\n3-element Vector{String15}:\n \"setosa\"\n \"versicolor\"\n \"virginica\"",
    "crumbs": [
      "Start",
      "build first model"
    ]
  },
  {
    "objectID": "start/getting-start.html#getting-start",
    "href": "start/getting-start.html#getting-start",
    "title": "Build first MLJ machine learning model",
    "section": "",
    "text": "we followed workflow, step by step\n\n\nfor iris dataset ref: Iris Dataset EDA\n\nusing Pipe\nusing CSV,DataFrames\nusing MLJ\ndf=CSV.File(\"../data/iris.csv\")|&gt;DataFrame;\n\n\nschema(df)\n\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ names        â”‚ scitypes   â”‚ types    â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ sepal_length â”‚ Continuous â”‚ Float64  â”‚\nâ”‚ sepal_width  â”‚ Continuous â”‚ Float64  â”‚\nâ”‚ petal_length â”‚ Continuous â”‚ Float64  â”‚\nâ”‚ petal_width  â”‚ Continuous â”‚ Float64  â”‚\nâ”‚ species      â”‚ Textual    â”‚ String15 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n\nfor species need transform to Categorical type\n\ndf.species= coerce(df.species,Multiclass);\nschema(df)\n\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ names        â”‚ scitypes      â”‚ types                              â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ sepal_length â”‚ Continuous    â”‚ Float64                            â”‚\nâ”‚ sepal_width  â”‚ Continuous    â”‚ Float64                            â”‚\nâ”‚ petal_length â”‚ Continuous    â”‚ Float64                            â”‚\nâ”‚ petal_width  â”‚ Continuous    â”‚ Float64                            â”‚\nâ”‚ species      â”‚ Multiclass{3} â”‚ CategoricalValue{String15, UInt32} â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n\nnow we need to split data\n\ny, X = unpack(df, ==(:species); rng=123)\n\n\n(CategoricalArrays.CategoricalValue{String15, UInt32}[String15(\"virginica\"), String15(\"versicolor\"), String15(\"virginica\"), String15(\"setosa\"), String15(\"versicolor\"), String15(\"virginica\"), String15(\"setosa\"), String15(\"versicolor\"), String15(\"setosa\"), String15(\"virginica\")  â€¦  String15(\"virginica\"), String15(\"virginica\"), String15(\"virginica\"), String15(\"versicolor\"), String15(\"virginica\"), String15(\"setosa\"), String15(\"virginica\"), String15(\"virginica\"), String15(\"setosa\"), String15(\"setosa\")], 150Ã—4 DataFrame\n Row â”‚ sepal_length  sepal_width  petal_length  petal_width \n     â”‚ Float64       Float64      Float64       Float64     \nâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n   1 â”‚          6.7          3.3           5.7          2.1\n   2 â”‚          5.7          2.8           4.1          1.3\n   3 â”‚          7.2          3.0           5.8          1.6\n   4 â”‚          4.4          2.9           1.4          0.2\n   5 â”‚          5.6          2.5           3.9          1.1\n   6 â”‚          6.5          3.0           5.2          2.0\n   7 â”‚          4.4          3.0           1.3          0.2\n   8 â”‚          6.1          2.9           4.7          1.4\n   9 â”‚          5.4          3.9           1.7          0.4\n  10 â”‚          4.9          2.5           4.5          1.7\n  11 â”‚          6.3          2.5           4.9          1.5\n  â‹®  â”‚      â‹®             â‹®            â‹®             â‹®\n 141 â”‚          6.4          2.7           5.3          1.9\n 142 â”‚          6.8          3.2           5.9          2.3\n 143 â”‚          6.9          3.1           5.4          2.1\n 144 â”‚          6.1          2.8           4.0          1.3\n 145 â”‚          6.7          2.5           5.8          1.8\n 146 â”‚          5.0          3.5           1.3          0.3\n 147 â”‚          7.6          3.0           6.6          2.1\n 148 â”‚          6.3          2.5           5.0          1.9\n 149 â”‚          5.1          3.8           1.6          0.2\n 150 â”‚          5.0          3.6           1.4          0.2\n                                            129 rows omitted)\n\n\n\n\n\n\n\nres=@pipe models(matching(X,y))|&gt; DataFrame|&gt;select(_,[:name,:package_name])\n\n53Ã—2 DataFrame28 rows omitted\n\n\n\nRow\nname\npackage_name\n\n\n\nString\nString\n\n\n\n\n1\nAdaBoostClassifier\nMLJScikitLearnInterface\n\n\n2\nAdaBoostStumpClassifier\nDecisionTree\n\n\n3\nBaggingClassifier\nMLJScikitLearnInterface\n\n\n4\nBayesianLDA\nMLJScikitLearnInterface\n\n\n5\nBayesianLDA\nMultivariateStats\n\n\n6\nBayesianQDA\nMLJScikitLearnInterface\n\n\n7\nBayesianSubspaceLDA\nMultivariateStats\n\n\n8\nCatBoostClassifier\nCatBoost\n\n\n9\nConstantClassifier\nMLJModels\n\n\n10\nDecisionTreeClassifier\nBetaML\n\n\n11\nDecisionTreeClassifier\nDecisionTree\n\n\n12\nDeterministicConstantClassifier\nMLJModels\n\n\n13\nDummyClassifier\nMLJScikitLearnInterface\n\n\nâ‹®\nâ‹®\nâ‹®\n\n\n42\nRandomForestClassifier\nMLJScikitLearnInterface\n\n\n43\nRidgeCVClassifier\nMLJScikitLearnInterface\n\n\n44\nRidgeClassifier\nMLJScikitLearnInterface\n\n\n45\nSGDClassifier\nMLJScikitLearnInterface\n\n\n46\nSVC\nLIBSVM\n\n\n47\nSVMClassifier\nMLJScikitLearnInterface\n\n\n48\nSVMLinearClassifier\nMLJScikitLearnInterface\n\n\n49\nSVMNuClassifier\nMLJScikitLearnInterface\n\n\n50\nStableForestClassifier\nSIRUS\n\n\n51\nStableRulesClassifier\nSIRUS\n\n\n52\nSubspaceLDA\nMultivariateStats\n\n\n53\nXGBoostClassifier\nXGBoost\n\n\n\n\n\n\nsimple way to look into model details:\ndoc($name, pkg=$package_name)\n\ndoc(\"AdaBoostStumpClassifier\", pkg=\"DecisionTree\")\n\nAdaBoostStumpClassifier\nA model type for constructing a Ada-boosted stump classifier, based on DecisionTree.jl, and implementing the MLJ model interface.\nFrom MLJ, the type can be imported using\nAdaBoostStumpClassifier = @load AdaBoostStumpClassifier pkg=DecisionTree\nDo model = AdaBoostStumpClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in AdaBoostStumpClassifier(n_iter=...).\n\n\nIn MLJ or MLJBase, bind an instance model to data with\nmach = machine(model, X, y)\nwhere:\n\nX: any table of input features (eg, a DataFrame) whose columns each have one of the following element scitypes: Continuous, Count, or &lt;:OrderedFactor; check column scitypes with schema(X)\ny: the target, which can be any AbstractVector whose element scitype is &lt;:OrderedFactor or &lt;:Multiclass; check the scitype with scitype(y)\n\nTrain the machine with fit!(mach, rows=...).\n\n\n\n\nn_iter=10: number of iterations of AdaBoost\nfeature_importance: method to use for computing feature importances. One of (:impurity, :split)\nrng=Random.GLOBAL_RNG: random number generator or seed\n\n\n\n\n\npredict(mach, Xnew): return predictions of the target given features Xnew having the same scitype as X above. Predictions are probabilistic, but uncalibrated.\npredict_mode(mach, Xnew): instead return the mode of each prediction above.\n\n\n\n\nThe fields of fitted_params(mach) are:\n\nstumps: the Ensemble object returned by the core DecisionTree.jl algorithm.\ncoefficients: the stump coefficients (one per stump)\n\n\n\n\n\nfeatures: the names of the features encountered in training\n\n\n\n\n\nfeature_importances(mach) returns a vector of (feature::Symbol =&gt; importance) pairs; the type of importance is determined by the hyperparameter feature_importance (see above)\n\n\n\n\nusing MLJ\nBooster = @load AdaBoostStumpClassifier pkg=DecisionTree\nbooster = Booster(n_iter=15)\n\nX, y = @load_iris\nmach = machine(booster, X, y) |&gt; fit!\n\nXnew = (sepal_length = [6.4, 7.2, 7.4],\n        sepal_width = [2.8, 3.0, 2.8],\n        petal_length = [5.6, 5.8, 6.1],\n        petal_width = [2.1, 1.6, 1.9],)\nyhat = predict(mach, Xnew) # probabilistic predictions\npredict_mode(mach, Xnew)   # point predictions\npdf.(yhat, \"virginica\")    # probabilities for the \"verginica\" class\n\nfitted_params(mach).stumps # raw `Ensemble` object from DecisionTree.jl\nfitted_params(mach).coefs  # coefficient associated with each stump\nfeature_importances(mach)\nSee also DecisionTree.jl and the unwrapped model type MLJDecisionTreeInterface.DecisionTree.AdaBoostStumpClassifier.\n\n\n\n\n\n\nthere are too many models, so you just add code if repel notice missing package , you just install packages.\n\nTree = @load DecisionTreeClassifier pkg=DecisionTree\n\nimport MLJDecisionTreeInterface âœ”\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n\n\nMLJDecisionTreeInterface.DecisionTreeClassifier\n\n\n\n\n\n\n tree=Tree()\n\nDecisionTreeClassifier(\n  max_depth = -1, \n  min_samples_leaf = 1, \n  min_samples_split = 2, \n  min_purity_increase = 0.0, \n  n_subfeatures = 0, \n  post_prune = false, \n  merge_purity_threshold = 1.0, \n  display_depth = 5, \n  feature_importance = :impurity, \n  rng = Random._GLOBAL_RNG())\n\n\n\n\n\n\nevaluate(tree, X, y,\n         resampling=CV(shuffle=true),\n         measures=[log_loss, accuracy],\n         verbosity=0)\n\n\nPerformanceEvaluation object with these fields:\n  model, measure, operation, measurement, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_rows, resampling, repeats\nExtract:\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”‚ measure              â”‚ operation    â”‚ measurement â”‚ 1.96*SE â”‚ per_fold       â‹¯\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”‚ LogLoss(             â”‚ predict      â”‚ 2.4         â”‚ 1.53    â”‚ [2.88, 2.22e-1 â‹¯\nâ”‚   tol = 2.22045e-16) â”‚              â”‚             â”‚         â”‚                â‹¯\nâ”‚ Accuracy()           â”‚ predict_mode â”‚ 0.933       â”‚ 0.0425  â”‚ [0.92, 1.0, 0. â‹¯\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n                                                                1 column omitted\n\n\n\n\n\n\n\nfill model and data to machine\n\nmach=machine(tree, X,y)\n\nuntrained Machine; caches model-specific representations of data\n  model: DecisionTreeClassifier(max_depth = -1, â€¦)\n  args: \n    1:  Source @124 â Table{AbstractVector{Continuous}}\n    2:  Source @591 â AbstractVector{Multiclass{3}}\n\n\n\n\n\n\ntrain, test = partition(eachindex(y), 0.7); \n\n\n\n\nuse train index to inform model to we must not use all data!!!\n\nfit!(mach, rows=train)\n\n[ Info: Training machine(DecisionTreeClassifier(max_depth = -1, â€¦), â€¦).\n\n\ntrained Machine; caches model-specific representations of data\n  model: DecisionTreeClassifier(max_depth = -1, â€¦)\n  args: \n    1:  Source @124 â Table{AbstractVector{Continuous}}\n    2:  Source @591 â AbstractVector{Multiclass{3}}\n\n\n\n\n\nsame with other julia stats model , X must be a matrix ,even if it is one row and one column data\n\nyhat = predict_mode(mach, X[test,:])\n\n45-element CategoricalArrays.CategoricalArray{String15,1,UInt32}:\n String15(\"versicolor\")\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"virginica\")\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"versicolor\")\n String15(\"versicolor\")\n String15(\"virginica\")\n String15(\"versicolor\")\n String15(\"virginica\")\n String15(\"versicolor\")\n String15(\"virginica\")\n â‹®\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"virginica\")\n String15(\"virginica\")\n String15(\"virginica\")\n String15(\"versicolor\")\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"virginica\")\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"setosa\")\n\n\n\n\n\n\naccuracy(yhat,y[test])\n\n0.9555555555555556\n\n\n\nlevels(yhat)\n\n3-element Vector{String15}:\n \"setosa\"\n \"versicolor\"\n \"virginica\"",
    "crumbs": [
      "Start",
      "build first model"
    ]
  },
  {
    "objectID": "packages/statesbase.html",
    "href": "packages/statesbase.html",
    "title": "StatsBase.jl",
    "section": "",
    "text": "StatsBase.jl"
  },
  {
    "objectID": "packages/glm.html",
    "href": "packages/glm.html",
    "title": "GLM.jl",
    "section": "",
    "text": "GLM.jl",
    "crumbs": [
      "GLM"
    ]
  },
  {
    "objectID": "learn/example.html",
    "href": "learn/example.html",
    "title": "Code-insertion Example",
    "section": "",
    "text": "quoted block added before the post"
  },
  {
    "objectID": "learn/example.html#heading",
    "href": "learn/example.html#heading",
    "title": "Code-insertion Example",
    "section": "Heading",
    "text": "Heading\nThis filter adds code immediately before and after the post."
  },
  {
    "objectID": "learn/example.html#appended-section-after-the-post",
    "href": "learn/example.html#appended-section-after-the-post",
    "title": "Code-insertion Example",
    "section": "Appended Section After the Post",
    "text": "Appended Section After the Post"
  },
  {
    "objectID": "dataset/iris.html",
    "href": "dataset/iris.html",
    "title": "ğŸŒºğŸŒ¸Iris dataset",
    "section": "",
    "text": "info\n\n\n\nIRIS is a an famous dataset for statistics and data science. t has long history. we will use it angain and agian.rery time wo use iris dataset we can get new knowledget is like exercies for running",
    "crumbs": [
      "Dataset",
      "1.iris"
    ]
  },
  {
    "objectID": "dataset/iris.html#load-csv",
    "href": "dataset/iris.html#load-csv",
    "title": "ğŸŒºğŸŒ¸Iris dataset",
    "section": "1. load csv",
    "text": "1. load csv\n\n using CSV,DataFrames,Tidier\n using CairoMakie,AlgebraOfGraphics\n\n #df=download(\"http://localhost:5220/data/iris.csv\")|&gt;CSV.File|&gt;DataFrame;\n df=CSV.File(\"../data/iris.csv\")|&gt;DataFrame\n first(df,5)\n\n5Ã—5 DataFrame\n\n\n\nRow\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\nFloat64\nFloat64\nFloat64\nFloat64\nString15\n\n\n\n\n1\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n2\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n3\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\nCSV,DataFrames,Tidier ç”¨äºæ•°æ®æ¡†å¤„ç†\nCairoMakie,AlgebraOfGraphicsç”¨äºç»˜å›¾",
    "crumbs": [
      "Dataset",
      "1.iris"
    ]
  },
  {
    "objectID": "dataset/iris.html#describe-dataframe",
    "href": "dataset/iris.html#describe-dataframe",
    "title": "ğŸŒºğŸŒ¸Iris dataset",
    "section": "2. describe dataframe",
    "text": "2. describe dataframe\n\ndescribe(df)\n\n5Ã—7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnionâ€¦\nAny\nUnionâ€¦\nAny\nInt64\nDataType\n\n\n\n\n1\nsepal_length\n5.84333\n4.3\n5.8\n7.9\n0\nFloat64\n\n\n2\nsepal_width\n3.054\n2.0\n3.0\n4.4\n0\nFloat64\n\n\n3\npetal_length\n3.75867\n1.0\n4.35\n6.9\n0\nFloat64\n\n\n4\npetal_width\n1.19867\n0.1\n1.3\n2.5\n0\nFloat64\n\n\n5\nspecies\n\nsetosa\n\nvirginica\n0\nString15",
    "crumbs": [
      "Dataset",
      "1.iris"
    ]
  },
  {
    "objectID": "dataset/iris.html#eda",
    "href": "dataset/iris.html#eda",
    "title": "ğŸŒºğŸŒ¸Iris dataset",
    "section": "3. EDA",
    "text": "3. EDA\n\n3.1 density plot\n\npalette = (color=tuple.([\"#FC7808\", \"#8C00EC\", \"#107A78\"], 0.65),\n           marker=[:circle, :utriangle, :rect])\ncycle = Cycle([:color, :marker], covary=true)\n\n\ndatalayer=data(df)\nmappinglayer1=mapping(:sepal_length,:sepal_width,color=:species)\nmappinglayer2=mapping(:sepal_length,color=:species)\nmappinglayer3=mapping(:sepal_width,color=:species)\n\nvisuallayer1=visual(Scatter,strokewidth=1,storkecolor=:black)\nvisuallayer2=visual(Density)\nvisuallayer3=visual(Density,direction=:y)\nplc=datalayer*mappinglayer1*visuallayer1  # top pic\nplt=datalayer*mappinglayer2*visuallayer2  # main pic\nplr=datalayer*mappinglayer3*visuallayer3  # right pic\n\nwith_theme(theme_light(),resolution = (600,400), palette=palette, Scatter=(cycle=cycle,)) do\n        fig = Figure()\n        axs = [Axis(fig[2,1], xlabel = \"sepal_length\", ylabel = \"sepal_width\"),\n            Axis(fig[1,1]), Axis(fig[2,2])]\n        dots = draw!(axs[1], plc)  # bject for lengend extract species info\n        draw!(axs[2], plt)\n        draw!(axs[3], plr)\n        # getting the right layout aspect\n        colsize!(fig.layout, 1, Auto(4.0))\n        rowsize!(fig.layout, 1, Auto(1/3))\n        colgap!(fig.layout,3)\n        rowgap!(fig.layout, 3)\n        linkxaxes!(axs[1], axs[2])\n        linkyaxes!(axs[1], axs[3])\n        hidedecorations!.(axs[2:3], grid=false)\n        \n        legend!(fig[1,2], dots)\n        fig\n    end\n\n\n\n\n\n\n\n\n\n\n3.2 pair plot\n\ncats=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\nlen=length(cats)\ncats=Symbol.(cats)\n\n\ndata_layer=data(df)\n\nfunction make_layer(sy1::Symbol,sy2::Symbol)\n   mapping_layer=sy1==sy2 ? mapping(sy1,color=:species) : mapping(sy1,sy2,color=:species)\n   vis_layer=sy1==sy2 ? visual(Density,color=:species) : visual(Scatter,color=:species,strokewidth=1,storkecolor=:black)\n   return  data_layer*mapping_layer*vis_layer\nend\n\nwith_theme(theme_minimal(),resolution = (800,800), palette=palette, Scatter=(cycle=cycle,)) do\n        cl=0.1\n        fig = Figure()\n        for i in 1:len\n            for  j in 1:len\n                ax=Axis(fig[i,j])\n                j==1 ? ax.ylabel= String(cats[i]) : nothing\n                i==len ? ax.xlabel= String(cats[j]) : nothing\n                layer=make_layer(cats[i],cats[j])\n              [i,j]==[len,len] ? cl=draw!(ax,layer) : draw!(ax,layer)\n            end\n        end\n        legend!(fig[2:3,5], cl)\n        fig\nend\n\n\n\n\n\n\n\n\n\n\n3.3 box plot\ntransform four feature to attribute and value\n\nlong_data=@pivot_longer(df, 1:5, names_to = attribute, values_to = value,-species)\nfirst(long_data,5)\n\n5Ã—3 DataFrame\n\n\n\nRow\nspecies\nattribute\nvalue\n\n\n\nString15\nString\nFloat64\n\n\n\n\n1\nsetosa\nsepal_length\n5.1\n\n\n2\nsetosa\nsepal_length\n4.9\n\n\n3\nsetosa\nsepal_length\n4.7\n\n\n4\nsetosa\nsepal_length\n4.6\n\n\n5\nsetosa\nsepal_length\n5.0\n\n\n\n\n\n\n\ndata(long_data) * visual(BoxPlot, show_notch=true) *\n    mapping(:attribute, :value, color=:species, dodge=:species) |&gt; draw\n\n\n\n\n\n\n\n\n\n\n3.4 correlation of numerical variables",
    "crumbs": [
      "Dataset",
      "1.iris"
    ]
  },
  {
    "objectID": "dataset/boston-house.html",
    "href": "dataset/boston-house.html",
    "title": "ğŸ ğŸ¡ğŸ£ Boston Housing dataset",
    "section": "",
    "text": "info\n\n\n\nBoston Housing is another famous dataset",
    "crumbs": [
      "Dataset",
      "3.boston housing"
    ]
  },
  {
    "objectID": "dataset/boston-house.html#load-csv",
    "href": "dataset/boston-house.html#load-csv",
    "title": "ğŸ ğŸ¡ğŸ£ Boston Housing dataset",
    "section": "1. load csv",
    "text": "1. load csv\n\n using CSV,DataFrames,Tidier\n using CairoMakie,AlgebraOfGraphics\n\ndf=CSV.File(\"../data/housing.csv\")|&gt;DataFrame\n first(df,5)\n\n5Ã—14 DataFrame\n\n\n\nRow\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nb\nlstat\nmedv\n\n\n\nFloat64\nFloat64\nFloat64\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\nInt64\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n0.00632\n18.0\n2.31\n0\n0.538\n6.575\n65.2\n4.09\n1\n296\n15.3\n396.9\n4.98\n24.0\n\n\n2\n0.02731\n0.0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242\n17.8\n396.9\n9.14\n21.6\n\n\n3\n0.02729\n0.0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242\n17.8\n392.83\n4.03\n34.7\n\n\n4\n0.03237\n0.0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222\n18.7\n394.63\n2.94\n33.4\n\n\n5\n0.06905\n0.0\n2.18\n0\n0.458\n7.147\n54.2\n6.0622\n3\n222\n18.7\n396.9\n5.33\n36.2",
    "crumbs": [
      "Dataset",
      "3.boston housing"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "dataset/index.html",
    "href": "dataset/index.html",
    "title": "Make friends with data!",
    "section": "",
    "text": "first step to start data science is recognize your data only you get familiar with the data , then make flow work",
    "crumbs": [
      "Dataset",
      "intro"
    ]
  },
  {
    "objectID": "dataset/penguins.html",
    "href": "dataset/penguins.html",
    "title": "ğŸ§ğŸ§ Penguins dataset",
    "section": "",
    "text": "info\n\n\n\nPenguins data set is another famous dataset for data science, compare with Iris, Penguins has more categorical feature.\nplease reference :\n\nPalmer penguins website\n[Aog Tutorial ğŸ§]",
    "crumbs": [
      "Dataset",
      "2.penguins"
    ]
  },
  {
    "objectID": "dataset/penguins.html#load-csv",
    "href": "dataset/penguins.html#load-csv",
    "title": "ğŸ§ğŸ§ Penguins dataset",
    "section": "1. load csv",
    "text": "1. load csv\n\nusing CSV,DataFrames,Tidier\nusing CairoMakie,AlgebraOfGraphics,MakieThemes\nMakie.set_theme!(ggthemr(:dust))\n\ndf=CSV.File(\"../data/penguine_data.csv\")|&gt;DataFrame\nfirst(df,5)\n\n5Ã—7 DataFrame\n\n\n\nRow\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\nString15\nString15\nString7\nString7\nString3\nString7\nString7\n\n\n\n\n1\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmale\n\n\n2\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nfemale\n\n\n3\nAdelie\nTorgersen\n40.3\n18\n195\n3250\nfemale\n\n\n4\nAdelie\nTorgersen\nNA\nNA\nNA\nNA\nNA\n\n\n5\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale",
    "crumbs": [
      "Dataset",
      "2.penguins"
    ]
  },
  {
    "objectID": "dataset/penguins.html#describe-dataframe",
    "href": "dataset/penguins.html#describe-dataframe",
    "title": "ğŸ§ğŸ§ Penguins dataset",
    "section": "2. describe dataframe",
    "text": "2. describe dataframe\n\ndescribe(df)\n\n7Ã—7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nNothing\nInlineStâ€¦\nNothing\nInlineStâ€¦\nInt64\nDataType\n\n\n\n\n1\nspecies\n\nAdelie\n\nGentoo\n0\nString15\n\n\n2\nisland\n\nBiscoe\n\nTorgersen\n0\nString15\n\n\n3\nbill_length_mm\n\n32.1\n\nNA\n0\nString7\n\n\n4\nbill_depth_mm\n\n13.1\n\nNA\n0\nString7\n\n\n5\nflipper_length_mm\n\n172\n\nNA\n0\nString3\n\n\n6\nbody_mass_g\n\n2700\n\nNA\n0\nString7\n\n\n7\nsex\n\nNA\n\nmale\n0\nString7",
    "crumbs": [
      "Dataset",
      "2.penguins"
    ]
  },
  {
    "objectID": "dataset/penguins.html#eda",
    "href": "dataset/penguins.html#eda",
    "title": "ğŸ§ğŸ§ Penguins dataset",
    "section": "3. EDA",
    "text": "3. EDA\n\n3.1 frequency\n\ndatalayer=data(df)\nmappinglayer=frequency()*mapping(:species,color = :island,dodge = :island)\naxis = (width = 225, height = 225)\ndraw(datalayer*mappinglayer,axis=axis)\n\n\n\n\n\n\n\nFigureÂ 1: fig-penguins-fequency\n\n\n\n\n\n\n\n3.2 correlation of two variables\n\n#show(names(df))\ncats=[\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\nlen=length(cats)\ncats=Symbol.(cats)\n\n4-element Vector{Symbol}:\n :bill_length_mm\n :bill_depth_mm\n :flipper_length_mm\n :body_mass_g\n\n\n\npalette = (color=tuple.([\"#FC7808\", \"#8C00EC\", \"#107A78\"], 0.65),\n           marker=[:circle, :utriangle, :rect])\ncycle = Cycle([:color, :marker], covary=true)\n\nfunction make_layer(sy1::Symbol,sy2::Symbol)\n   mapping_layer=sy1==sy2 ? mapping(sy1,color=:species) : mapping(sy1,sy2,color=:species)\n   vis_layer=sy1==sy2 ? visual(Density,color=:species) : visual(Scatter,color=:species,strokewidth=1,storkecolor=:black)\n   return  datalayer*mapping_layer*vis_layer\nend\n\nwith_theme(ggthemr(:dust),resolution = (800,800), palette=palette, Scatter=(cycle=cycle,)) do\n        cl=0.1\n        fig = Figure()\n        for i in 1:len\n            for  j in 1:len\n                ax=Axis(fig[i,j])\n                j==1 ? ax.ylabel= String(cats[i]) : nothing\n                i==len ? ax.xlabel= String(cats[j]) : nothing\n                layer=make_layer(cats[i],cats[j])\n              [i,j]==[len,len] ? cl=draw!(ax,layer) : draw!(ax,layer)\n            end\n        end\n        legend!(fig[2:3,5], cl)\n        fig\nend\n\n\n\n\n\n\n\nFigureÂ 2: fig-penguins-pairplot\n\n\n\n\n\n\n\n3.3 varying weights of the penguins\n\nax=(width = 225, height = 225)\nmappinglayer2=mapping(:species, :bill_depth_mm;color=:species)\nvislayer2=visual(BoxPlot,show_notch=true)\ndraw(datalayer*mappinglayer2*vislayer2,axis=ax)\n\n\n\n\n\n\n\nFigureÂ 3: fig-penguins-bodymass",
    "crumbs": [
      "Dataset",
      "2.penguins"
    ]
  },
  {
    "objectID": "learn/Julia-MachineLearning.html",
    "href": "learn/Julia-MachineLearning.html",
    "title": "Julia-MachineLearning",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "learn/Julia-MachineLearning.html#quarto",
    "href": "learn/Julia-MachineLearning.html#quarto",
    "title": "Julia-MachineLearning",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "learn/index.html",
    "href": "learn/index.html",
    "title": "Leanring",
    "section": "",
    "text": "Learn",
    "crumbs": [
      "Learn"
    ]
  },
  {
    "objectID": "packages/index.html",
    "href": "packages/index.html",
    "title": "Packages",
    "section": "",
    "text": "List\n\nGLM.jl"
  },
  {
    "objectID": "start/index.html",
    "href": "start/index.html",
    "title": "Start",
    "section": "",
    "text": "Start",
    "crumbs": [
      "Start",
      "GET STARTED"
    ]
  },
  {
    "objectID": "workflow/index.html",
    "href": "workflow/index.html",
    "title": "MLJ workflow",
    "section": "",
    "text": "graph TD\n    A[(Data)] --&gt;|Tidy Data| B(Ingestion)\n    B --&gt; C{Model Search}\n    C --&gt;|One| D[Regression]\n    C --&gt;|Two| E[Classification]\n    C --&gt;|Three| F[Dimension Reduction]\n    C --&gt;|Four| G[Ensemble Models]\n    C --&gt;|Five| H[Other Models]\n    D--&gt;J(Instantiating Model)\n    E--&gt;J\n    F--&gt;J\n    G--&gt;J\n    H--&gt;J\n    J--&gt;K(Evaluating Model)\n    K--&gt;L(Inspecting training results)\n    L--&gt;M(hyperparameter tuning)\n    M--&gt;N(Constructing linear pipelines)\n    N--&gt;O(ensemble of models) \n    O--&gt;P(Performance curves)\n\n\n\n\n\ngraph TD\n    A[(Data)] --&gt;|Tidy Data| B(Ingestion)\n    B --&gt; C{Model Search}\n    C --&gt;|One| D[Regression]\n    C --&gt;|Two| E[Classification]\n    C --&gt;|Three| F[Dimension Reduction]\n    C --&gt;|Four| G[Ensemble Models]\n    C --&gt;|Five| H[Other Models]\n    D--&gt;J(Instantiating Model)\n    E--&gt;J\n    F--&gt;J\n    G--&gt;J\n    H--&gt;J\n    J--&gt;K(Evaluating Model)\n    K--&gt;L(Inspecting training results)\n    L--&gt;M(hyperparameter tuning)\n    M--&gt;N(Constructing linear pipelines)\n    N--&gt;O(ensemble of models) \n    O--&gt;P(Performance curves)\n\n\n\n\nFigureÂ 1: MLJ main work flow",
    "crumbs": [
      "Workflow",
      "flowchart"
    ]
  },
  {
    "objectID": "workflow/index.html#common-workflow",
    "href": "workflow/index.html#common-workflow",
    "title": "MLJ workflow",
    "section": "",
    "text": "graph TD\n    A[(Data)] --&gt;|Tidy Data| B(Ingestion)\n    B --&gt; C{Model Search}\n    C --&gt;|One| D[Regression]\n    C --&gt;|Two| E[Classification]\n    C --&gt;|Three| F[Dimension Reduction]\n    C --&gt;|Four| G[Ensemble Models]\n    C --&gt;|Five| H[Other Models]\n    D--&gt;J(Instantiating Model)\n    E--&gt;J\n    F--&gt;J\n    G--&gt;J\n    H--&gt;J\n    J--&gt;K(Evaluating Model)\n    K--&gt;L(Inspecting training results)\n    L--&gt;M(hyperparameter tuning)\n    M--&gt;N(Constructing linear pipelines)\n    N--&gt;O(ensemble of models) \n    O--&gt;P(Performance curves)\n\n\n\n\n\ngraph TD\n    A[(Data)] --&gt;|Tidy Data| B(Ingestion)\n    B --&gt; C{Model Search}\n    C --&gt;|One| D[Regression]\n    C --&gt;|Two| E[Classification]\n    C --&gt;|Three| F[Dimension Reduction]\n    C --&gt;|Four| G[Ensemble Models]\n    C --&gt;|Five| H[Other Models]\n    D--&gt;J(Instantiating Model)\n    E--&gt;J\n    F--&gt;J\n    G--&gt;J\n    H--&gt;J\n    J--&gt;K(Evaluating Model)\n    K--&gt;L(Inspecting training results)\n    L--&gt;M(hyperparameter tuning)\n    M--&gt;N(Constructing linear pipelines)\n    N--&gt;O(ensemble of models) \n    O--&gt;P(Performance curves)\n\n\n\n\nFigureÂ 1: MLJ main work flow",
    "crumbs": [
      "Workflow",
      "flowchart"
    ]
  }
]
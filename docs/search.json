[
  {
    "objectID": "workflow/tailwindcss.html",
    "href": "workflow/tailwindcss.html",
    "title": "embedding tailwind css",
    "section": "",
    "text": "test tailwind css\n\nExample heading New\n\n\nNotifications 4\n\n\nâ¶ â· â¸ â¹ âº â… â† â‡ âˆ â‰\n\n\nâ¶ â· â¸ â¹ âº â… â† â‡ âˆ â‰\n\n\n\n\nFirst Name\n\n\nLast Name\n\n\nPoints\n\n\n\n\nJill\n\n\nSmith\n\n\n50\n\n\n\n\nJill\n\n\nSmith\n\n\n50\n\n\n\n\nJill\n\n\nSmith\n\n\n50"
  },
  {
    "objectID": "workflow/composing-models.html",
    "href": "workflow/composing-models.html",
    "title": "Composing Models",
    "section": "",
    "text": "MLJ ä¸­ç»„åˆæ¨¡å‹æœ‰ä¸‰ç§å½¢å¼\n\n\n\n\n\n    %%| label: fig-mlj-composing model\n    %%| fig-cap: \"MLJ  composing model\"\n    %%| fig-width: 6.5\n    %%| echo: true\n    graph TD\n        C(Composing Models)\n        C --&gt;|One| D([Pipeline Model ])\n        C --&gt;|Two| E([Ensemble Model])\n        C --&gt;|Three| F([Stack Model])",
    "crumbs": [
      "Workflow",
      "composing models"
    ]
  },
  {
    "objectID": "start/getting-start.html",
    "href": "start/getting-start.html",
    "title": "Build first MLJ machine learning model",
    "section": "",
    "text": "we followed workflow, step by step\n\n\nfor iris dataset ref: Iris Dataset EDA\n\nusing Pipe\nusing CSV,DataFrames\nusing MLJ\ndf=CSV.File(\"../data/iris.csv\")|&gt;DataFrame;\n\n\nschema(df)\n\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ names        â”‚ scitypes   â”‚ types    â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ sepal_length â”‚ Continuous â”‚ Float64  â”‚\nâ”‚ sepal_width  â”‚ Continuous â”‚ Float64  â”‚\nâ”‚ petal_length â”‚ Continuous â”‚ Float64  â”‚\nâ”‚ petal_width  â”‚ Continuous â”‚ Float64  â”‚\nâ”‚ species      â”‚ Textual    â”‚ String15 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n\nfor species need transform to Categorical type\n\ndf.species= coerce(df.species,Multiclass);\nschema(df)\n\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ names        â”‚ scitypes      â”‚ types                              â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ sepal_length â”‚ Continuous    â”‚ Float64                            â”‚\nâ”‚ sepal_width  â”‚ Continuous    â”‚ Float64                            â”‚\nâ”‚ petal_length â”‚ Continuous    â”‚ Float64                            â”‚\nâ”‚ petal_width  â”‚ Continuous    â”‚ Float64                            â”‚\nâ”‚ species      â”‚ Multiclass{3} â”‚ CategoricalValue{String15, UInt32} â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n\nnow we need to split data\n\ny, X = unpack(df, ==(:species); rng=123)\n\n\n(CategoricalArrays.CategoricalValue{String15, UInt32}[String15(\"virginica\"), String15(\"versicolor\"), String15(\"virginica\"), String15(\"setosa\"), String15(\"versicolor\"), String15(\"virginica\"), String15(\"setosa\"), String15(\"versicolor\"), String15(\"setosa\"), String15(\"virginica\")  â€¦  String15(\"virginica\"), String15(\"virginica\"), String15(\"virginica\"), String15(\"versicolor\"), String15(\"virginica\"), String15(\"setosa\"), String15(\"virginica\"), String15(\"virginica\"), String15(\"setosa\"), String15(\"setosa\")], 150Ã—4 DataFrame\n Row â”‚ sepal_length  sepal_width  petal_length  petal_width \n     â”‚ Float64       Float64      Float64       Float64     \nâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n   1 â”‚          6.7          3.3           5.7          2.1\n   2 â”‚          5.7          2.8           4.1          1.3\n   3 â”‚          7.2          3.0           5.8          1.6\n   4 â”‚          4.4          2.9           1.4          0.2\n   5 â”‚          5.6          2.5           3.9          1.1\n   6 â”‚          6.5          3.0           5.2          2.0\n   7 â”‚          4.4          3.0           1.3          0.2\n   8 â”‚          6.1          2.9           4.7          1.4\n   9 â”‚          5.4          3.9           1.7          0.4\n  10 â”‚          4.9          2.5           4.5          1.7\n  11 â”‚          6.3          2.5           4.9          1.5\n  â‹®  â”‚      â‹®             â‹®            â‹®             â‹®\n 141 â”‚          6.4          2.7           5.3          1.9\n 142 â”‚          6.8          3.2           5.9          2.3\n 143 â”‚          6.9          3.1           5.4          2.1\n 144 â”‚          6.1          2.8           4.0          1.3\n 145 â”‚          6.7          2.5           5.8          1.8\n 146 â”‚          5.0          3.5           1.3          0.3\n 147 â”‚          7.6          3.0           6.6          2.1\n 148 â”‚          6.3          2.5           5.0          1.9\n 149 â”‚          5.1          3.8           1.6          0.2\n 150 â”‚          5.0          3.6           1.4          0.2\n                                            129 rows omitted)\n\n\n\n\n\n\n\nres=@pipe models(matching(X,y))|&gt; DataFrame|&gt;select(_,[:name,:package_name])\n\n53Ã—2 DataFrame28 rows omitted\n\n\n\nRow\nname\npackage_name\n\n\n\nString\nString\n\n\n\n\n1\nAdaBoostClassifier\nMLJScikitLearnInterface\n\n\n2\nAdaBoostStumpClassifier\nDecisionTree\n\n\n3\nBaggingClassifier\nMLJScikitLearnInterface\n\n\n4\nBayesianLDA\nMLJScikitLearnInterface\n\n\n5\nBayesianLDA\nMultivariateStats\n\n\n6\nBayesianQDA\nMLJScikitLearnInterface\n\n\n7\nBayesianSubspaceLDA\nMultivariateStats\n\n\n8\nCatBoostClassifier\nCatBoost\n\n\n9\nConstantClassifier\nMLJModels\n\n\n10\nDecisionTreeClassifier\nBetaML\n\n\n11\nDecisionTreeClassifier\nDecisionTree\n\n\n12\nDeterministicConstantClassifier\nMLJModels\n\n\n13\nDummyClassifier\nMLJScikitLearnInterface\n\n\nâ‹®\nâ‹®\nâ‹®\n\n\n42\nRandomForestClassifier\nMLJScikitLearnInterface\n\n\n43\nRidgeCVClassifier\nMLJScikitLearnInterface\n\n\n44\nRidgeClassifier\nMLJScikitLearnInterface\n\n\n45\nSGDClassifier\nMLJScikitLearnInterface\n\n\n46\nSVC\nLIBSVM\n\n\n47\nSVMClassifier\nMLJScikitLearnInterface\n\n\n48\nSVMLinearClassifier\nMLJScikitLearnInterface\n\n\n49\nSVMNuClassifier\nMLJScikitLearnInterface\n\n\n50\nStableForestClassifier\nSIRUS\n\n\n51\nStableRulesClassifier\nSIRUS\n\n\n52\nSubspaceLDA\nMultivariateStats\n\n\n53\nXGBoostClassifier\nXGBoost\n\n\n\n\n\n\nsimple way to look into model details:\ndoc($name, pkg=$package_name)\n\ndoc(\"AdaBoostStumpClassifier\", pkg=\"DecisionTree\")\n\nAdaBoostStumpClassifier\nA model type for constructing a Ada-boosted stump classifier, based on DecisionTree.jl, and implementing the MLJ model interface.\nFrom MLJ, the type can be imported using\nAdaBoostStumpClassifier = @load AdaBoostStumpClassifier pkg=DecisionTree\nDo model = AdaBoostStumpClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in AdaBoostStumpClassifier(n_iter=...).\n\n\nIn MLJ or MLJBase, bind an instance model to data with\nmach = machine(model, X, y)\nwhere:\n\nX: any table of input features (eg, a DataFrame) whose columns each have one of the following element scitypes: Continuous, Count, or &lt;:OrderedFactor; check column scitypes with schema(X)\ny: the target, which can be any AbstractVector whose element scitype is &lt;:OrderedFactor or &lt;:Multiclass; check the scitype with scitype(y)\n\nTrain the machine with fit!(mach, rows=...).\n\n\n\n\nn_iter=10: number of iterations of AdaBoost\nfeature_importance: method to use for computing feature importances. One of (:impurity, :split)\nrng=Random.GLOBAL_RNG: random number generator or seed\n\n\n\n\n\npredict(mach, Xnew): return predictions of the target given features Xnew having the same scitype as X above. Predictions are probabilistic, but uncalibrated.\npredict_mode(mach, Xnew): instead return the mode of each prediction above.\n\n\n\n\nThe fields of fitted_params(mach) are:\n\nstumps: the Ensemble object returned by the core DecisionTree.jl algorithm.\ncoefficients: the stump coefficients (one per stump)\n\n\n\n\n\nfeatures: the names of the features encountered in training\n\n\n\n\n\nfeature_importances(mach) returns a vector of (feature::Symbol =&gt; importance) pairs; the type of importance is determined by the hyperparameter feature_importance (see above)\n\n\n\n\nusing MLJ\nBooster = @load AdaBoostStumpClassifier pkg=DecisionTree\nbooster = Booster(n_iter=15)\n\nX, y = @load_iris\nmach = machine(booster, X, y) |&gt; fit!\n\nXnew = (sepal_length = [6.4, 7.2, 7.4],\n        sepal_width = [2.8, 3.0, 2.8],\n        petal_length = [5.6, 5.8, 6.1],\n        petal_width = [2.1, 1.6, 1.9],)\nyhat = predict(mach, Xnew) # probabilistic predictions\npredict_mode(mach, Xnew)   # point predictions\npdf.(yhat, \"virginica\")    # probabilities for the \"verginica\" class\n\nfitted_params(mach).stumps # raw `Ensemble` object from DecisionTree.jl\nfitted_params(mach).coefs  # coefficient associated with each stump\nfeature_importances(mach)\nSee also DecisionTree.jl and the unwrapped model type MLJDecisionTreeInterface.DecisionTree.AdaBoostStumpClassifier.\n\n\n\n\n\n\nthere are too many models, so you just add code if repel notice missing package , you just install packages.\n\nTree = @load DecisionTreeClassifier pkg=DecisionTree\n\nimport MLJDecisionTreeInterface âœ”\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n\n\nMLJDecisionTreeInterface.DecisionTreeClassifier\n\n\n\n\n\n\n tree=Tree()\n\nDecisionTreeClassifier(\n  max_depth = -1, \n  min_samples_leaf = 1, \n  min_samples_split = 2, \n  min_purity_increase = 0.0, \n  n_subfeatures = 0, \n  post_prune = false, \n  merge_purity_threshold = 1.0, \n  display_depth = 5, \n  feature_importance = :impurity, \n  rng = Random._GLOBAL_RNG())\n\n\n\n\n\n\nevaluate(tree, X, y,\n         resampling=CV(shuffle=true),\n         measures=[log_loss, accuracy],\n         verbosity=0)\n\n\nPerformanceEvaluation object with these fields:\n  model, measure, operation, measurement, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_rows, resampling, repeats\nExtract:\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”‚ measure              â”‚ operation    â”‚ measurement â”‚ 1.96*SE â”‚ per_fold       â‹¯\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”‚ LogLoss(             â”‚ predict      â”‚ 2.4         â”‚ 1.53    â”‚ [2.88, 2.22e-1 â‹¯\nâ”‚   tol = 2.22045e-16) â”‚              â”‚             â”‚         â”‚                â‹¯\nâ”‚ Accuracy()           â”‚ predict_mode â”‚ 0.933       â”‚ 0.0425  â”‚ [0.92, 1.0, 0. â‹¯\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n                                                                1 column omitted\n\n\n\n\n\n\n\nfill model and data to machine\n\nmach=machine(tree, X,y)\n\nuntrained Machine; caches model-specific representations of data\n  model: DecisionTreeClassifier(max_depth = -1, â€¦)\n  args: \n    1:  Source @124 â Table{AbstractVector{Continuous}}\n    2:  Source @591 â AbstractVector{Multiclass{3}}\n\n\n\n\n\n\ntrain, test = partition(eachindex(y), 0.7); \n\n\n\n\nuse train index to inform model to we must not use all data!!!\n\nfit!(mach, rows=train)\n\n[ Info: Training machine(DecisionTreeClassifier(max_depth = -1, â€¦), â€¦).\n\n\ntrained Machine; caches model-specific representations of data\n  model: DecisionTreeClassifier(max_depth = -1, â€¦)\n  args: \n    1:  Source @124 â Table{AbstractVector{Continuous}}\n    2:  Source @591 â AbstractVector{Multiclass{3}}\n\n\n\n\n\nsame with other julia stats model , X must be a matrix ,even if it is one row and one column data\n\nyhat = predict_mode(mach, X[test,:])\n\n45-element CategoricalArrays.CategoricalArray{String15,1,UInt32}:\n String15(\"versicolor\")\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"virginica\")\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"versicolor\")\n String15(\"versicolor\")\n String15(\"virginica\")\n String15(\"versicolor\")\n String15(\"virginica\")\n String15(\"versicolor\")\n String15(\"virginica\")\n â‹®\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"virginica\")\n String15(\"virginica\")\n String15(\"virginica\")\n String15(\"versicolor\")\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"virginica\")\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"setosa\")\n\n\n\n\n\n\naccuracy(yhat,y[test])\n\n0.9555555555555556\n\n\n\nlevels(yhat)\n\n3-element Vector{String15}:\n \"setosa\"\n \"versicolor\"\n \"virginica\"",
    "crumbs": [
      "Start",
      "build first model"
    ]
  },
  {
    "objectID": "start/getting-start.html#getting-start",
    "href": "start/getting-start.html#getting-start",
    "title": "Build first MLJ machine learning model",
    "section": "",
    "text": "we followed workflow, step by step\n\n\nfor iris dataset ref: Iris Dataset EDA\n\nusing Pipe\nusing CSV,DataFrames\nusing MLJ\ndf=CSV.File(\"../data/iris.csv\")|&gt;DataFrame;\n\n\nschema(df)\n\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ names        â”‚ scitypes   â”‚ types    â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ sepal_length â”‚ Continuous â”‚ Float64  â”‚\nâ”‚ sepal_width  â”‚ Continuous â”‚ Float64  â”‚\nâ”‚ petal_length â”‚ Continuous â”‚ Float64  â”‚\nâ”‚ petal_width  â”‚ Continuous â”‚ Float64  â”‚\nâ”‚ species      â”‚ Textual    â”‚ String15 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n\nfor species need transform to Categorical type\n\ndf.species= coerce(df.species,Multiclass);\nschema(df)\n\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ names        â”‚ scitypes      â”‚ types                              â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ sepal_length â”‚ Continuous    â”‚ Float64                            â”‚\nâ”‚ sepal_width  â”‚ Continuous    â”‚ Float64                            â”‚\nâ”‚ petal_length â”‚ Continuous    â”‚ Float64                            â”‚\nâ”‚ petal_width  â”‚ Continuous    â”‚ Float64                            â”‚\nâ”‚ species      â”‚ Multiclass{3} â”‚ CategoricalValue{String15, UInt32} â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n\nnow we need to split data\n\ny, X = unpack(df, ==(:species); rng=123)\n\n\n(CategoricalArrays.CategoricalValue{String15, UInt32}[String15(\"virginica\"), String15(\"versicolor\"), String15(\"virginica\"), String15(\"setosa\"), String15(\"versicolor\"), String15(\"virginica\"), String15(\"setosa\"), String15(\"versicolor\"), String15(\"setosa\"), String15(\"virginica\")  â€¦  String15(\"virginica\"), String15(\"virginica\"), String15(\"virginica\"), String15(\"versicolor\"), String15(\"virginica\"), String15(\"setosa\"), String15(\"virginica\"), String15(\"virginica\"), String15(\"setosa\"), String15(\"setosa\")], 150Ã—4 DataFrame\n Row â”‚ sepal_length  sepal_width  petal_length  petal_width \n     â”‚ Float64       Float64      Float64       Float64     \nâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n   1 â”‚          6.7          3.3           5.7          2.1\n   2 â”‚          5.7          2.8           4.1          1.3\n   3 â”‚          7.2          3.0           5.8          1.6\n   4 â”‚          4.4          2.9           1.4          0.2\n   5 â”‚          5.6          2.5           3.9          1.1\n   6 â”‚          6.5          3.0           5.2          2.0\n   7 â”‚          4.4          3.0           1.3          0.2\n   8 â”‚          6.1          2.9           4.7          1.4\n   9 â”‚          5.4          3.9           1.7          0.4\n  10 â”‚          4.9          2.5           4.5          1.7\n  11 â”‚          6.3          2.5           4.9          1.5\n  â‹®  â”‚      â‹®             â‹®            â‹®             â‹®\n 141 â”‚          6.4          2.7           5.3          1.9\n 142 â”‚          6.8          3.2           5.9          2.3\n 143 â”‚          6.9          3.1           5.4          2.1\n 144 â”‚          6.1          2.8           4.0          1.3\n 145 â”‚          6.7          2.5           5.8          1.8\n 146 â”‚          5.0          3.5           1.3          0.3\n 147 â”‚          7.6          3.0           6.6          2.1\n 148 â”‚          6.3          2.5           5.0          1.9\n 149 â”‚          5.1          3.8           1.6          0.2\n 150 â”‚          5.0          3.6           1.4          0.2\n                                            129 rows omitted)\n\n\n\n\n\n\n\nres=@pipe models(matching(X,y))|&gt; DataFrame|&gt;select(_,[:name,:package_name])\n\n53Ã—2 DataFrame28 rows omitted\n\n\n\nRow\nname\npackage_name\n\n\n\nString\nString\n\n\n\n\n1\nAdaBoostClassifier\nMLJScikitLearnInterface\n\n\n2\nAdaBoostStumpClassifier\nDecisionTree\n\n\n3\nBaggingClassifier\nMLJScikitLearnInterface\n\n\n4\nBayesianLDA\nMLJScikitLearnInterface\n\n\n5\nBayesianLDA\nMultivariateStats\n\n\n6\nBayesianQDA\nMLJScikitLearnInterface\n\n\n7\nBayesianSubspaceLDA\nMultivariateStats\n\n\n8\nCatBoostClassifier\nCatBoost\n\n\n9\nConstantClassifier\nMLJModels\n\n\n10\nDecisionTreeClassifier\nBetaML\n\n\n11\nDecisionTreeClassifier\nDecisionTree\n\n\n12\nDeterministicConstantClassifier\nMLJModels\n\n\n13\nDummyClassifier\nMLJScikitLearnInterface\n\n\nâ‹®\nâ‹®\nâ‹®\n\n\n42\nRandomForestClassifier\nMLJScikitLearnInterface\n\n\n43\nRidgeCVClassifier\nMLJScikitLearnInterface\n\n\n44\nRidgeClassifier\nMLJScikitLearnInterface\n\n\n45\nSGDClassifier\nMLJScikitLearnInterface\n\n\n46\nSVC\nLIBSVM\n\n\n47\nSVMClassifier\nMLJScikitLearnInterface\n\n\n48\nSVMLinearClassifier\nMLJScikitLearnInterface\n\n\n49\nSVMNuClassifier\nMLJScikitLearnInterface\n\n\n50\nStableForestClassifier\nSIRUS\n\n\n51\nStableRulesClassifier\nSIRUS\n\n\n52\nSubspaceLDA\nMultivariateStats\n\n\n53\nXGBoostClassifier\nXGBoost\n\n\n\n\n\n\nsimple way to look into model details:\ndoc($name, pkg=$package_name)\n\ndoc(\"AdaBoostStumpClassifier\", pkg=\"DecisionTree\")\n\nAdaBoostStumpClassifier\nA model type for constructing a Ada-boosted stump classifier, based on DecisionTree.jl, and implementing the MLJ model interface.\nFrom MLJ, the type can be imported using\nAdaBoostStumpClassifier = @load AdaBoostStumpClassifier pkg=DecisionTree\nDo model = AdaBoostStumpClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in AdaBoostStumpClassifier(n_iter=...).\n\n\nIn MLJ or MLJBase, bind an instance model to data with\nmach = machine(model, X, y)\nwhere:\n\nX: any table of input features (eg, a DataFrame) whose columns each have one of the following element scitypes: Continuous, Count, or &lt;:OrderedFactor; check column scitypes with schema(X)\ny: the target, which can be any AbstractVector whose element scitype is &lt;:OrderedFactor or &lt;:Multiclass; check the scitype with scitype(y)\n\nTrain the machine with fit!(mach, rows=...).\n\n\n\n\nn_iter=10: number of iterations of AdaBoost\nfeature_importance: method to use for computing feature importances. One of (:impurity, :split)\nrng=Random.GLOBAL_RNG: random number generator or seed\n\n\n\n\n\npredict(mach, Xnew): return predictions of the target given features Xnew having the same scitype as X above. Predictions are probabilistic, but uncalibrated.\npredict_mode(mach, Xnew): instead return the mode of each prediction above.\n\n\n\n\nThe fields of fitted_params(mach) are:\n\nstumps: the Ensemble object returned by the core DecisionTree.jl algorithm.\ncoefficients: the stump coefficients (one per stump)\n\n\n\n\n\nfeatures: the names of the features encountered in training\n\n\n\n\n\nfeature_importances(mach) returns a vector of (feature::Symbol =&gt; importance) pairs; the type of importance is determined by the hyperparameter feature_importance (see above)\n\n\n\n\nusing MLJ\nBooster = @load AdaBoostStumpClassifier pkg=DecisionTree\nbooster = Booster(n_iter=15)\n\nX, y = @load_iris\nmach = machine(booster, X, y) |&gt; fit!\n\nXnew = (sepal_length = [6.4, 7.2, 7.4],\n        sepal_width = [2.8, 3.0, 2.8],\n        petal_length = [5.6, 5.8, 6.1],\n        petal_width = [2.1, 1.6, 1.9],)\nyhat = predict(mach, Xnew) # probabilistic predictions\npredict_mode(mach, Xnew)   # point predictions\npdf.(yhat, \"virginica\")    # probabilities for the \"verginica\" class\n\nfitted_params(mach).stumps # raw `Ensemble` object from DecisionTree.jl\nfitted_params(mach).coefs  # coefficient associated with each stump\nfeature_importances(mach)\nSee also DecisionTree.jl and the unwrapped model type MLJDecisionTreeInterface.DecisionTree.AdaBoostStumpClassifier.\n\n\n\n\n\n\nthere are too many models, so you just add code if repel notice missing package , you just install packages.\n\nTree = @load DecisionTreeClassifier pkg=DecisionTree\n\nimport MLJDecisionTreeInterface âœ”\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n\n\nMLJDecisionTreeInterface.DecisionTreeClassifier\n\n\n\n\n\n\n tree=Tree()\n\nDecisionTreeClassifier(\n  max_depth = -1, \n  min_samples_leaf = 1, \n  min_samples_split = 2, \n  min_purity_increase = 0.0, \n  n_subfeatures = 0, \n  post_prune = false, \n  merge_purity_threshold = 1.0, \n  display_depth = 5, \n  feature_importance = :impurity, \n  rng = Random._GLOBAL_RNG())\n\n\n\n\n\n\nevaluate(tree, X, y,\n         resampling=CV(shuffle=true),\n         measures=[log_loss, accuracy],\n         verbosity=0)\n\n\nPerformanceEvaluation object with these fields:\n  model, measure, operation, measurement, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_rows, resampling, repeats\nExtract:\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”‚ measure              â”‚ operation    â”‚ measurement â”‚ 1.96*SE â”‚ per_fold       â‹¯\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”‚ LogLoss(             â”‚ predict      â”‚ 2.4         â”‚ 1.53    â”‚ [2.88, 2.22e-1 â‹¯\nâ”‚   tol = 2.22045e-16) â”‚              â”‚             â”‚         â”‚                â‹¯\nâ”‚ Accuracy()           â”‚ predict_mode â”‚ 0.933       â”‚ 0.0425  â”‚ [0.92, 1.0, 0. â‹¯\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n                                                                1 column omitted\n\n\n\n\n\n\n\nfill model and data to machine\n\nmach=machine(tree, X,y)\n\nuntrained Machine; caches model-specific representations of data\n  model: DecisionTreeClassifier(max_depth = -1, â€¦)\n  args: \n    1:  Source @124 â Table{AbstractVector{Continuous}}\n    2:  Source @591 â AbstractVector{Multiclass{3}}\n\n\n\n\n\n\ntrain, test = partition(eachindex(y), 0.7); \n\n\n\n\nuse train index to inform model to we must not use all data!!!\n\nfit!(mach, rows=train)\n\n[ Info: Training machine(DecisionTreeClassifier(max_depth = -1, â€¦), â€¦).\n\n\ntrained Machine; caches model-specific representations of data\n  model: DecisionTreeClassifier(max_depth = -1, â€¦)\n  args: \n    1:  Source @124 â Table{AbstractVector{Continuous}}\n    2:  Source @591 â AbstractVector{Multiclass{3}}\n\n\n\n\n\nsame with other julia stats model , X must be a matrix ,even if it is one row and one column data\n\nyhat = predict_mode(mach, X[test,:])\n\n45-element CategoricalArrays.CategoricalArray{String15,1,UInt32}:\n String15(\"versicolor\")\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"virginica\")\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"versicolor\")\n String15(\"versicolor\")\n String15(\"virginica\")\n String15(\"versicolor\")\n String15(\"virginica\")\n String15(\"versicolor\")\n String15(\"virginica\")\n â‹®\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"virginica\")\n String15(\"virginica\")\n String15(\"virginica\")\n String15(\"versicolor\")\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"virginica\")\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"setosa\")\n\n\n\n\n\n\naccuracy(yhat,y[test])\n\n0.9555555555555556\n\n\n\nlevels(yhat)\n\n3-element Vector{String15}:\n \"setosa\"\n \"versicolor\"\n \"virginica\"",
    "crumbs": [
      "Start",
      "build first model"
    ]
  },
  {
    "objectID": "packages/statesbase.html",
    "href": "packages/statesbase.html",
    "title": "StatsBase.jl",
    "section": "",
    "text": "StatsBase.jl"
  },
  {
    "objectID": "packages/glm.html",
    "href": "packages/glm.html",
    "title": "GLM.jl",
    "section": "",
    "text": "GLM.jl",
    "crumbs": [
      "GLM"
    ]
  },
  {
    "objectID": "learn/example.html",
    "href": "learn/example.html",
    "title": "Code-insertion Example",
    "section": "",
    "text": "quoted block added before the post"
  },
  {
    "objectID": "learn/example.html#heading",
    "href": "learn/example.html#heading",
    "title": "Code-insertion Example",
    "section": "Heading",
    "text": "Heading\nThis filter adds code immediately before and after the post."
  },
  {
    "objectID": "learn/example.html#appended-section-after-the-post",
    "href": "learn/example.html#appended-section-after-the-post",
    "title": "Code-insertion Example",
    "section": "Appended Section After the Post",
    "text": "Appended Section After the Post"
  },
  {
    "objectID": "dataset/iris.html",
    "href": "dataset/iris.html",
    "title": "ğŸŒºğŸŒ¸Iris dataset",
    "section": "",
    "text": "info\n\n\n\nIRIS is a an famous dataset for statistics and data science. t has long history. we will use it angain and agian.rery time wo use iris dataset we can get new knowledget is like exercies for running",
    "crumbs": [
      "Dataset",
      "1.iris"
    ]
  },
  {
    "objectID": "dataset/iris.html#load-csv",
    "href": "dataset/iris.html#load-csv",
    "title": "ğŸŒºğŸŒ¸Iris dataset",
    "section": "1. load csv",
    "text": "1. load csv\n\n using CSV,DataFrames,Tidier\n using CairoMakie,AlgebraOfGraphics\n\n #df=download(\"http://localhost:5220/data/iris.csv\")|&gt;CSV.File|&gt;DataFrame;\n df=CSV.File(\"../data/iris.csv\")|&gt;DataFrame\n first(df,5)\n\n5Ã—5 DataFrame\n\n\n\nRow\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\nFloat64\nFloat64\nFloat64\nFloat64\nString15\n\n\n\n\n1\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n2\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n3\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\nCSV,DataFrames,Tidier ç”¨äºæ•°æ®æ¡†å¤„ç†\nCairoMakie,AlgebraOfGraphicsç”¨äºç»˜å›¾",
    "crumbs": [
      "Dataset",
      "1.iris"
    ]
  },
  {
    "objectID": "dataset/iris.html#describe-dataframe",
    "href": "dataset/iris.html#describe-dataframe",
    "title": "ğŸŒºğŸŒ¸Iris dataset",
    "section": "2. describe dataframe",
    "text": "2. describe dataframe\n\ndescribe(df)\n\n5Ã—7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnionâ€¦\nAny\nUnionâ€¦\nAny\nInt64\nDataType\n\n\n\n\n1\nsepal_length\n5.84333\n4.3\n5.8\n7.9\n0\nFloat64\n\n\n2\nsepal_width\n3.054\n2.0\n3.0\n4.4\n0\nFloat64\n\n\n3\npetal_length\n3.75867\n1.0\n4.35\n6.9\n0\nFloat64\n\n\n4\npetal_width\n1.19867\n0.1\n1.3\n2.5\n0\nFloat64\n\n\n5\nspecies\n\nsetosa\n\nvirginica\n0\nString15",
    "crumbs": [
      "Dataset",
      "1.iris"
    ]
  },
  {
    "objectID": "dataset/iris.html#eda",
    "href": "dataset/iris.html#eda",
    "title": "ğŸŒºğŸŒ¸Iris dataset",
    "section": "3. EDA",
    "text": "3. EDA\n\n3.1 density plot\n\npalette = (color=tuple.([\"#FC7808\", \"#8C00EC\", \"#107A78\"], 0.65),\n           marker=[:circle, :utriangle, :rect])\ncycle = Cycle([:color, :marker], covary=true)\n\n\ndatalayer=data(df)\nmappinglayer1=mapping(:sepal_length,:sepal_width,color=:species)\nmappinglayer2=mapping(:sepal_length,color=:species)\nmappinglayer3=mapping(:sepal_width,color=:species)\n\nvisuallayer1=visual(Scatter,strokewidth=1,storkecolor=:black)\nvisuallayer2=visual(Density)\nvisuallayer3=visual(Density,direction=:y)\nplc=datalayer*mappinglayer1*visuallayer1  # top pic\nplt=datalayer*mappinglayer2*visuallayer2  # main pic\nplr=datalayer*mappinglayer3*visuallayer3  # right pic\n\nwith_theme(theme_light(),resolution = (600,400), palette=palette, Scatter=(cycle=cycle,)) do\n        fig = Figure()\n        axs = [Axis(fig[2,1], xlabel = \"sepal_length\", ylabel = \"sepal_width\"),\n            Axis(fig[1,1]), Axis(fig[2,2])]\n        dots = draw!(axs[1], plc)  # bject for lengend extract species info\n        draw!(axs[2], plt)\n        draw!(axs[3], plr)\n        # getting the right layout aspect\n        colsize!(fig.layout, 1, Auto(4.0))\n        rowsize!(fig.layout, 1, Auto(1/3))\n        colgap!(fig.layout,3)\n        rowgap!(fig.layout, 3)\n        linkxaxes!(axs[1], axs[2])\n        linkyaxes!(axs[1], axs[3])\n        hidedecorations!.(axs[2:3], grid=false)\n        \n        legend!(fig[1,2], dots)\n        fig\n    end\n\n\n\n\n\n\n\n\n\n\n3.2 pair plot\n\ncats=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\nlen=length(cats)\ncats=Symbol.(cats)\n\n\ndata_layer=data(df)\n\nfunction make_layer(sy1::Symbol,sy2::Symbol)\n   mapping_layer=sy1==sy2 ? mapping(sy1,color=:species) : mapping(sy1,sy2,color=:species)\n   vis_layer=sy1==sy2 ? visual(Density,color=:species) : visual(Scatter,color=:species,strokewidth=1,storkecolor=:black)\n   return  data_layer*mapping_layer*vis_layer\nend\n\nwith_theme(theme_minimal(),resolution = (800,800), palette=palette, Scatter=(cycle=cycle,)) do\n        cl=0.1\n        fig = Figure()\n        for i in 1:len\n            for  j in 1:len\n                ax=Axis(fig[i,j])\n                j==1 ? ax.ylabel= String(cats[i]) : nothing\n                i==len ? ax.xlabel= String(cats[j]) : nothing\n                layer=make_layer(cats[i],cats[j])\n              [i,j]==[len,len] ? cl=draw!(ax,layer) : draw!(ax,layer)\n            end\n        end\n        legend!(fig[2:3,5], cl)\n        fig\nend\n\n\n\n\n\n\n\n\n\n\n3.3 box plot\ntransform four feature to attribute and value\n\nlong_data=@pivot_longer(df, 1:5, names_to = attribute, values_to = value,-species)\nfirst(long_data,5)\n\n5Ã—3 DataFrame\n\n\n\nRow\nspecies\nattribute\nvalue\n\n\n\nString15\nString\nFloat64\n\n\n\n\n1\nsetosa\nsepal_length\n5.1\n\n\n2\nsetosa\nsepal_length\n4.9\n\n\n3\nsetosa\nsepal_length\n4.7\n\n\n4\nsetosa\nsepal_length\n4.6\n\n\n5\nsetosa\nsepal_length\n5.0\n\n\n\n\n\n\n\ndata(long_data) * visual(BoxPlot, show_notch=true) *\n    mapping(:attribute, :value, color=:species, dodge=:species) |&gt; draw\n\n\n\n\n\n\n\n\n\n\n3.4 correlation of numerical variables",
    "crumbs": [
      "Dataset",
      "1.iris"
    ]
  },
  {
    "objectID": "dataset/boston-house.html",
    "href": "dataset/boston-house.html",
    "title": "ğŸ ğŸ¡ğŸ£ Boston Housing dataset",
    "section": "",
    "text": "info\n\n\n\nBoston Housing is another famous dataset",
    "crumbs": [
      "Dataset",
      "3.boston housing"
    ]
  },
  {
    "objectID": "dataset/boston-house.html#load-csv",
    "href": "dataset/boston-house.html#load-csv",
    "title": "ğŸ ğŸ¡ğŸ£ Boston Housing dataset",
    "section": "1. load csv",
    "text": "1. load csv\n\n using CSV,DataFrames,Tidier\n using CairoMakie,AlgebraOfGraphics\n\ndf=CSV.File(\"../data/housing.csv\")|&gt;DataFrame\n first(df,5)\n\n5Ã—14 DataFrame\n\n\n\nRow\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nb\nlstat\nmedv\n\n\n\nFloat64\nFloat64\nFloat64\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\nInt64\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n0.00632\n18.0\n2.31\n0\n0.538\n6.575\n65.2\n4.09\n1\n296\n15.3\n396.9\n4.98\n24.0\n\n\n2\n0.02731\n0.0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242\n17.8\n396.9\n9.14\n21.6\n\n\n3\n0.02729\n0.0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242\n17.8\n392.83\n4.03\n34.7\n\n\n4\n0.03237\n0.0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222\n18.7\n394.63\n2.94\n33.4\n\n\n5\n0.06905\n0.0\n2.18\n0\n0.458\n7.147\n54.2\n6.0622\n3\n222\n18.7\n396.9\n5.33\n36.2",
    "crumbs": [
      "Dataset",
      "3.boston housing"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "dataset/index.html",
    "href": "dataset/index.html",
    "title": "Make friends with data!",
    "section": "",
    "text": "first step to start data science is recognize your data only you get familiar with the data , then make flow work",
    "crumbs": [
      "Dataset",
      "intro"
    ]
  },
  {
    "objectID": "dataset/penguins.html",
    "href": "dataset/penguins.html",
    "title": "ğŸ§ğŸ§ Penguins dataset",
    "section": "",
    "text": "info\n\n\n\nPenguins data set is another famous dataset for data science, compare with Iris, Penguins has more categorical feature.\nplease reference :\n\nPalmer penguins website\n[Aog Tutorial ğŸ§]",
    "crumbs": [
      "Dataset",
      "2.penguins"
    ]
  },
  {
    "objectID": "dataset/penguins.html#load-csv",
    "href": "dataset/penguins.html#load-csv",
    "title": "ğŸ§ğŸ§ Penguins dataset",
    "section": "1. load csv",
    "text": "1. load csv\n\nusing CSV,DataFrames,Tidier\nusing CairoMakie,AlgebraOfGraphics,MakieThemes\nMakie.set_theme!(ggthemr(:dust))\n\ndf=CSV.File(\"../data/penguine_data.csv\")|&gt;DataFrame\nfirst(df,5)\n\n5Ã—7 DataFrame\n\n\n\nRow\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\nString15\nString15\nString7\nString7\nString3\nString7\nString7\n\n\n\n\n1\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmale\n\n\n2\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nfemale\n\n\n3\nAdelie\nTorgersen\n40.3\n18\n195\n3250\nfemale\n\n\n4\nAdelie\nTorgersen\nNA\nNA\nNA\nNA\nNA\n\n\n5\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale",
    "crumbs": [
      "Dataset",
      "2.penguins"
    ]
  },
  {
    "objectID": "dataset/penguins.html#describe-dataframe",
    "href": "dataset/penguins.html#describe-dataframe",
    "title": "ğŸ§ğŸ§ Penguins dataset",
    "section": "2. describe dataframe",
    "text": "2. describe dataframe\n\ndescribe(df)\n\n7Ã—7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nNothing\nInlineStâ€¦\nNothing\nInlineStâ€¦\nInt64\nDataType\n\n\n\n\n1\nspecies\n\nAdelie\n\nGentoo\n0\nString15\n\n\n2\nisland\n\nBiscoe\n\nTorgersen\n0\nString15\n\n\n3\nbill_length_mm\n\n32.1\n\nNA\n0\nString7\n\n\n4\nbill_depth_mm\n\n13.1\n\nNA\n0\nString7\n\n\n5\nflipper_length_mm\n\n172\n\nNA\n0\nString3\n\n\n6\nbody_mass_g\n\n2700\n\nNA\n0\nString7\n\n\n7\nsex\n\nNA\n\nmale\n0\nString7",
    "crumbs": [
      "Dataset",
      "2.penguins"
    ]
  },
  {
    "objectID": "dataset/penguins.html#eda",
    "href": "dataset/penguins.html#eda",
    "title": "ğŸ§ğŸ§ Penguins dataset",
    "section": "3. EDA",
    "text": "3. EDA\n\n3.1 frequency\n\ndatalayer=data(df)\nmappinglayer=frequency()*mapping(:species,color = :island,dodge = :island)\naxis = (width = 225, height = 225)\ndraw(datalayer*mappinglayer,axis=axis)\n\n\n\n\n\n\n\nFigureÂ 1: fig-penguins-fequency\n\n\n\n\n\n\n\n3.2 correlation of two variables\n\n#show(names(df))\ncats=[\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\nlen=length(cats)\ncats=Symbol.(cats)\n\n4-element Vector{Symbol}:\n :bill_length_mm\n :bill_depth_mm\n :flipper_length_mm\n :body_mass_g\n\n\n\npalette = (color=tuple.([\"#FC7808\", \"#8C00EC\", \"#107A78\"], 0.65),\n           marker=[:circle, :utriangle, :rect])\ncycle = Cycle([:color, :marker], covary=true)\n\nfunction make_layer(sy1::Symbol,sy2::Symbol)\n   mapping_layer=sy1==sy2 ? mapping(sy1,color=:species) : mapping(sy1,sy2,color=:species)\n   vis_layer=sy1==sy2 ? visual(Density,color=:species) : visual(Scatter,color=:species,strokewidth=1,storkecolor=:black)\n   return  datalayer*mapping_layer*vis_layer\nend\n\nwith_theme(ggthemr(:dust),resolution = (800,800), palette=palette, Scatter=(cycle=cycle,)) do\n        cl=0.1\n        fig = Figure()\n        for i in 1:len\n            for  j in 1:len\n                ax=Axis(fig[i,j])\n                j==1 ? ax.ylabel= String(cats[i]) : nothing\n                i==len ? ax.xlabel= String(cats[j]) : nothing\n                layer=make_layer(cats[i],cats[j])\n              [i,j]==[len,len] ? cl=draw!(ax,layer) : draw!(ax,layer)\n            end\n        end\n        legend!(fig[2:3,5], cl)\n        fig\nend\n\n\n\n\n\n\n\nFigureÂ 2: fig-penguins-pairplot\n\n\n\n\n\n\n\n3.3 varying weights of the penguins\n\nax=(width = 225, height = 225)\nmappinglayer2=mapping(:species, :bill_depth_mm;color=:species)\nvislayer2=visual(BoxPlot,show_notch=true)\ndraw(datalayer*mappinglayer2*vislayer2,axis=ax)\n\n\n\n\n\n\n\nFigureÂ 3: fig-penguins-bodymass",
    "crumbs": [
      "Dataset",
      "2.penguins"
    ]
  },
  {
    "objectID": "learn/Julia-MachineLearning.html",
    "href": "learn/Julia-MachineLearning.html",
    "title": "Julia-MachineLearning",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "learn/Julia-MachineLearning.html#quarto",
    "href": "learn/Julia-MachineLearning.html#quarto",
    "title": "Julia-MachineLearning",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "learn/index.html",
    "href": "learn/index.html",
    "title": "Leanring",
    "section": "",
    "text": "Learn",
    "crumbs": [
      "Learn"
    ]
  },
  {
    "objectID": "packages/index.html",
    "href": "packages/index.html",
    "title": "Packages",
    "section": "",
    "text": "List\n\nGLM.jl"
  },
  {
    "objectID": "start/index.html",
    "href": "start/index.html",
    "title": "Start",
    "section": "",
    "text": "Start",
    "crumbs": [
      "Start",
      "GET STARTED"
    ]
  },
  {
    "objectID": "workflow/index.html",
    "href": "workflow/index.html",
    "title": "MLJ workflow",
    "section": "",
    "text": "graph TD\n    A[(Data)] --&gt;|Tidy Data| B(Ingestion)\n    B --&gt; C{Model Search}\n    C --&gt;|One| D[Regression]\n    C --&gt;|Two| E[Classification]\n    C --&gt;|Three| F[Dimension Reduction]\n    C --&gt;|Four| G[Ensemble Models]\n    C --&gt;|Five| H[Other Models]\n    D--&gt;J(Instantiating Model)\n    E--&gt;J\n    F--&gt;J\n    G--&gt;J\n    H--&gt;J\n    J--&gt;K(Evaluating Model)\n    K--&gt;L(Inspecting training results)\n    L--&gt;M(hyperparameter tuning)\n    M--&gt;N(Constructing linear pipelines)\n    N--&gt;O(ensemble of models) \n    O--&gt;P(Performance curves)\n\n\n\n\n\ngraph TD\n    A[(Data)] --&gt;|Tidy Data| B(Ingestion)\n    B --&gt; C{Model Search}\n    C --&gt;|One| D[Regression]\n    C --&gt;|Two| E[Classification]\n    C --&gt;|Three| F[Dimension Reduction]\n    C --&gt;|Four| G[Ensemble Models]\n    C --&gt;|Five| H[Other Models]\n    D--&gt;J(Instantiating Model)\n    E--&gt;J\n    F--&gt;J\n    G--&gt;J\n    H--&gt;J\n    J--&gt;K(Evaluating Model)\n    K--&gt;L(Inspecting training results)\n    L--&gt;M(hyperparameter tuning)\n    M--&gt;N(Constructing linear pipelines)\n    N--&gt;O(ensemble of models) \n    O--&gt;P(Performance curves)\n\n\n\n\nFigureÂ 1: MLJ main work flow",
    "crumbs": [
      "Workflow",
      "flowchart"
    ]
  },
  {
    "objectID": "workflow/index.html#common-workflow",
    "href": "workflow/index.html#common-workflow",
    "title": "MLJ workflow",
    "section": "",
    "text": "graph TD\n    A[(Data)] --&gt;|Tidy Data| B(Ingestion)\n    B --&gt; C{Model Search}\n    C --&gt;|One| D[Regression]\n    C --&gt;|Two| E[Classification]\n    C --&gt;|Three| F[Dimension Reduction]\n    C --&gt;|Four| G[Ensemble Models]\n    C --&gt;|Five| H[Other Models]\n    D--&gt;J(Instantiating Model)\n    E--&gt;J\n    F--&gt;J\n    G--&gt;J\n    H--&gt;J\n    J--&gt;K(Evaluating Model)\n    K--&gt;L(Inspecting training results)\n    L--&gt;M(hyperparameter tuning)\n    M--&gt;N(Constructing linear pipelines)\n    N--&gt;O(ensemble of models) \n    O--&gt;P(Performance curves)\n\n\n\n\n\ngraph TD\n    A[(Data)] --&gt;|Tidy Data| B(Ingestion)\n    B --&gt; C{Model Search}\n    C --&gt;|One| D[Regression]\n    C --&gt;|Two| E[Classification]\n    C --&gt;|Three| F[Dimension Reduction]\n    C --&gt;|Four| G[Ensemble Models]\n    C --&gt;|Five| H[Other Models]\n    D--&gt;J(Instantiating Model)\n    E--&gt;J\n    F--&gt;J\n    G--&gt;J\n    H--&gt;J\n    J--&gt;K(Evaluating Model)\n    K--&gt;L(Inspecting training results)\n    L--&gt;M(hyperparameter tuning)\n    M--&gt;N(Constructing linear pipelines)\n    N--&gt;O(ensemble of models) \n    O--&gt;P(Performance curves)\n\n\n\n\nFigureÂ 1: MLJ main work flow",
    "crumbs": [
      "Workflow",
      "flowchart"
    ]
  },
  {
    "objectID": "dataset/boston-house.html#describe-dataframe",
    "href": "dataset/boston-house.html#describe-dataframe",
    "title": "ğŸ ğŸ¡ğŸ£ Boston Housing dataset",
    "section": "2. describe dataframe",
    "text": "2. describe dataframe\n\ndescribe(df)\n\n14Ã—7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nFloat64\nReal\nFloat64\nReal\nInt64\nDataType\n\n\n\n\n1\ncrim\n3.61352\n0.00632\n0.25651\n88.9762\n0\nFloat64\n\n\n2\nzn\n11.3636\n0.0\n0.0\n100.0\n0\nFloat64\n\n\n3\nindus\n11.1368\n0.46\n9.69\n27.74\n0\nFloat64\n\n\n4\nchas\n0.06917\n0\n0.0\n1\n0\nInt64\n\n\n5\nnox\n0.554695\n0.385\n0.538\n0.871\n0\nFloat64\n\n\n6\nrm\n6.28463\n3.561\n6.2085\n8.78\n0\nFloat64\n\n\n7\nage\n68.5749\n2.9\n77.5\n100.0\n0\nFloat64\n\n\n8\ndis\n3.79504\n1.1296\n3.20745\n12.1265\n0\nFloat64\n\n\n9\nrad\n9.54941\n1\n5.0\n24\n0\nInt64\n\n\n10\ntax\n408.237\n187\n330.0\n711\n0\nInt64\n\n\n11\nptratio\n18.4555\n12.6\n19.05\n22.0\n0\nFloat64\n\n\n12\nb\n356.674\n0.32\n391.44\n396.9\n0\nFloat64\n\n\n13\nlstat\n12.6531\n1.73\n11.36\n37.97\n0\nFloat64\n\n\n14\nmedv\n22.5328\n5.0\n21.2\n50.0\n0\nFloat64",
    "crumbs": [
      "Dataset",
      "3.boston housing"
    ]
  },
  {
    "objectID": "dataset/boston-house.html#eda",
    "href": "dataset/boston-house.html#eda",
    "title": "ğŸ ğŸ¡ğŸ£ Boston Housing dataset",
    "section": "3 EDA",
    "text": "3 EDA\n\n3.1 CHA\ncharles river is a famous river around harvard univ and mit so , we want to know by ther river side affect house price\n\ndf1=@chain df begin\n   @group_by(chas)\n   @summarize(mean=mean(medv),median=median(medv))\n\nend\n\nresolution = (500, 250)\nfig = Figure(; resolution)\nax1 = Axis(fig[1, 1],xlabel=\"chas\",ylabel=\"mean\")\nax2 = Axis(fig[1, 2],xlabel=\"chas\",ylabel=\"median\")\n\ndatalayer1=data(df1)\nmappinglayer1=mapping(:chas,:mean, color=:chas) \nmappinglayer2=mapping(:chas,:median, color=:chas)\nvislayer=visual(BarPlot,bar_labels=:y,flip_labels_at=23)\n\nplt1 = datalayer1* mappinglayer1*vislayer\nplt2 = datalayer1* mappinglayer2*vislayer\ndraw!(ax1,plt1)\ngrid=draw!(ax2,plt2)\nlegend!(fig[1, 3], grid)\nfig\n\n\n\n\n\n\n\n\n\n\n3.2 Rad count\n\ndf2=@chain df begin\n    @group_by(rad)\n    @summarize(count=n())\n    @ungroup\nend\n\n9Ã—2 DataFrame\n\n\n\nRow\nrad\ncount\n\n\n\nInt64\nInt64\n\n\n\n\n1\n1\n20\n\n\n2\n2\n24\n\n\n3\n3\n38\n\n\n4\n4\n110\n\n\n5\n5\n115\n\n\n6\n6\n26\n\n\n7\n7\n17\n\n\n8\n8\n24\n\n\n9\n24\n132\n\n\n\n\n\n\n\nlet \n    resolution = (400, 300)\n    fig = Figure(; resolution)\n    ax = Axis(fig[1, 1],xlabel=\"rad\",ylabel=\"count\")\n    \n    datalayer32=data(df2)\n    mappinglayer32=mapping(:rad,:count,color=:rad)\n    vislayer32=visual(BarPlot,bar_labels=:y,flip_labels_at=130)\n    plt=datalayer32*mappinglayer32*vislayer32\n    draw!(ax,plt)\n    fig\nend\n\n\n\n\n\n\n\n\n\n\n3.3 Rad price mean\n\ndf33=@chain df begin\n    @group_by(rad)\n    @summarize(mean=mean(medv))\n    @ungroup\nend\n\n9Ã—2 DataFrame\n\n\n\nRow\nrad\nmean\n\n\n\nInt64\nFloat64\n\n\n\n\n1\n1\n24.365\n\n\n2\n2\n26.8333\n\n\n3\n3\n27.9289\n\n\n4\n4\n21.3873\n\n\n5\n5\n25.707\n\n\n6\n6\n20.9769\n\n\n7\n7\n27.1059\n\n\n8\n8\n30.3583\n\n\n9\n24\n16.4038\n\n\n\n\n\n\n\n\n3.4 medv density\n\ndatalayer34=data(df)\nmappinglayer34=mapping(:medv)\nvislayer341=visual(Density,color=(:lightgreen,0.6),strokewidth=1,strokecolor=:black)\nvislayer342=visual(Hist,strokewidth=1,strokecolor=:black,normalization = :pdf,color=(:red,0.5))\ndraw(datalayer34*mappinglayer34*(vislayer341+vislayer342))",
    "crumbs": [
      "Dataset",
      "3.boston housing"
    ]
  },
  {
    "objectID": "dataset/mtcar.html",
    "href": "dataset/mtcar.html",
    "title": "ğŸš—ğŸš•ğŸš™ auto-mpg dataset",
    "section": "",
    "text": "using CSV,DataFrames,Tidier\nusing CairoMakie,AlgebraOfGraphics,MakieThemes\nusing StatsBase\nusing ScientificTypes\nMakie.set_theme!(ggthemr(:flat));\n\n\ndf=CSV.File(\"../data/auto-mpg.csv\")|&gt;DataFrame\ndf=@chain df begin\n   @clean_names\nend\nfirst(df,5)\n\n\n\n\n5Ã—7 DataFrame\n\n\n\nRow\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nmodel_year\n\n\n\nFloat64\nInt64\nFloat64\nInt64?\nInt64\nFloat64\nInt64\n\n\n\n\n1\n18.0\n8\n307.0\n130\n3504\n12.0\n70\n\n\n2\n15.0\n8\n350.0\n165\n3693\n11.5\n70\n\n\n3\n18.0\n8\n318.0\n150\n3436\n11.0\n70\n\n\n4\n16.0\n8\n304.0\n150\n3433\n12.0\n70\n\n\n5\n17.0\n8\n302.0\n140\n3449\n10.5\n70\n\n\n\n\n\n\n\nTableÂ 1: auto-mpg dataset",
    "crumbs": [
      "Dataset",
      "4.auto mpg"
    ]
  },
  {
    "objectID": "dataset/mtcar.html#load-csv",
    "href": "dataset/mtcar.html#load-csv",
    "title": "ğŸš—ğŸš•ğŸš™ auto-mpg dataset",
    "section": "",
    "text": "using CSV,DataFrames,Tidier\nusing CairoMakie,AlgebraOfGraphics,MakieThemes\nusing StatsBase\nusing ScientificTypes\nMakie.set_theme!(ggthemr(:flat));\n\n\ndf=CSV.File(\"../data/auto-mpg.csv\")|&gt;DataFrame\ndf=@chain df begin\n   @clean_names\nend\nfirst(df,5)\n\n\n\n\n5Ã—7 DataFrame\n\n\n\nRow\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nmodel_year\n\n\n\nFloat64\nInt64\nFloat64\nInt64?\nInt64\nFloat64\nInt64\n\n\n\n\n1\n18.0\n8\n307.0\n130\n3504\n12.0\n70\n\n\n2\n15.0\n8\n350.0\n165\n3693\n11.5\n70\n\n\n3\n18.0\n8\n318.0\n150\n3436\n11.0\n70\n\n\n4\n16.0\n8\n304.0\n150\n3433\n12.0\n70\n\n\n5\n17.0\n8\n302.0\n140\n3449\n10.5\n70\n\n\n\n\n\n\n\nTableÂ 1: auto-mpg dataset",
    "crumbs": [
      "Dataset",
      "4.auto mpg"
    ]
  },
  {
    "objectID": "dataset/mtcar.html#describe-data",
    "href": "dataset/mtcar.html#describe-data",
    "title": "ğŸš—ğŸš•ğŸš™ auto-mpg dataset",
    "section": "2. describe data",
    "text": "2. describe data\n\ndropmissing!(df)\n@show describe(df);\n\ndescribe(df) = 7Ã—7 DataFrame\n Row â”‚ variable      mean        min     median   max     nmissing  eltype\n     â”‚ Symbol        Float64     Real    Float64  Real    Int64     DataType\nâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n   1 â”‚ mpg             23.5172      9.0     23.0    46.6         0  Float64\n   2 â”‚ cylinders        5.45707     3        4.0     8           0  Int64\n   3 â”‚ displacement   193.65       68.0    148.5   455.0         0  Float64\n   4 â”‚ horsepower     104.189      46       92.0   230           0  Int64\n   5 â”‚ weight        2973.0      1613     2803.5  5140           0  Int64\n   6 â”‚ acceleration    15.5558      8.0     15.5    24.8         0  Float64\n   7 â”‚ model_year      76.0278     70       76.0    82           0  Int64",
    "crumbs": [
      "Dataset",
      "4.auto mpg"
    ]
  },
  {
    "objectID": "dataset/mtcar.html#heatmap-of-variables",
    "href": "dataset/mtcar.html#heatmap-of-variables",
    "title": "ğŸš—ğŸš•ğŸš™ auto-mpg dataset",
    "section": "3. heatmap of variables",
    "text": "3. heatmap of variables\n\ndf_cor = df|&gt;Matrix|&gt;cor.|&gt; d -&gt; round(d, digits=2)\nlabels=names(df)\nfunction plot_cov_cor()\n    fig = Figure(resolution=(800, 400)) \n    ax1 = Axis(fig[1, 1]; xticks=(1:7, labels), yticks=(1:7, labels), title=\"corr of mpg variables\",\n    xticklabelrotation = pi/8,\n    yreversed=true)\n    hm = heatmap!(ax1, df_cor)\n    Colorbar(fig[1, 2], hm)\n    [text!(ax1, x, y; text=string(df_cor[x, y]), color=:white, fontsize=18, align=(:center, :center)) for x in 1:7, y in 1:7]\n    fig\nend\nplot_cov_cor()\n\n\n\n\n\n\n\nFigureÂ 1: fig-automag-coorlation",
    "crumbs": [
      "Dataset",
      "4.auto mpg"
    ]
  },
  {
    "objectID": "dataset/mtcar.html#eda",
    "href": "dataset/mtcar.html#eda",
    "title": "ğŸš—ğŸš•ğŸš™ auto-mpg dataset",
    "section": "3. EDA",
    "text": "3. EDA\n\n3.1 heatmap of variables\n\ndf_cor = df|&gt;Matrix|&gt;cor.|&gt; d -&gt; round(d, digits=2)\nlabels=names(df)\nfunction plot_cov_cor()\n    fig = Figure(resolution=(800, 400)) \n    ax1 = Axis(fig[1, 1]; xticks=(1:7, labels), yticks=(1:7, labels), title=\"corr of mpg variables\",\n    xticklabelrotation = pi/8,\n    yreversed=true)\n    hm = heatmap!(ax1, df_cor)\n    Colorbar(fig[1, 2], hm)\n    [text!(ax1, x, y; text=string(df_cor[x, y]), color=:white, fontsize=18, align=(:center, :center)) for x in 1:7, y in 1:7]\n    fig\nend\nplot_cov_cor()\n\n\n\n\n\n\n\nFigureÂ 1: fig-automag-coorlation\n\n\n\n\n\n\n\n3.2 Univariate Analysis\n\n3.2.1 cylinders data\nplot_univariate plot univariate count\n\n    function plot_univariate(df::AbstractDataFrame,feature1::Symbol,feature2::Symbol,cats::Symbol)\n    ax=(width = 225, height = 225)\n    data_layer=data(df)\n    mappinglayer=mapping(feature1,feature2,color=cats)\n    vislayer=visual(BarPlot,bar_labels=:y,flip_labels_at=130)\n    plt=data_layer*mappinglayer*vislayer\n        draw(plt,axis=ax)\n    end\n\nplot_univariate (generic function with 1 method)\n\n\n\ndf321=@chain df begin\n    @group_by(cylinders)\n    @summarize(count=n())\n    @ungroup\nend\nplot_univariate(df321,:cylinders,:count,:cylinders)\n\n\n\n\n\n\n\n\n\n\n3.2.2 model_year count\n\ndf322=@chain df begin\n    @group_by(model_year)\n    @summarize(count=n())\n    @ungroup\nend\n\nplot_univariate(df322,:model_year,:count,:model_year)\n\n\n\n\n\n\n\n\n\n\n3.2.3 density of horsepower\n\nax=(width = 400, height = 300)\ndatalayer323=data(df)\nmappinglayer323=mapping(:horsepower)\nvislayer3231=visual(AlgebraOfGraphics.Density,color=(:lightgreen,0.6),strokewidth=1,strokecolor=:black)\nvislayer3232=visual(AlgebraOfGraphics.Hist,strokewidth=1,strokecolor=:black,normalization = :pdf,color=(:red,0.5))\ndraw(datalayer323*mappinglayer323*(vislayer3231+vislayer3232),axis=ax)\n\n\n\n\n\n\n\nFigureÂ 2: fig-automag-horsepower-density\n\n\n\n\n\n\n\n\n3.3 multivariate analysis\n\n3.3.1 mpg by cylinders\n\nlet\n    ax=(width =250, height = 250)\n    datalayer=data(df)\n    mappinglayer=mapping(:cylinders,:mpg,color=:cylinders)\n    vislayer=visual(BoxPlot)\n   data(df) * visual(BoxPlot) *\n    mapping(:cylinders, :mpg, color=:cylinders) |&gt;d-&gt;draw(d,axis=ax)\nend\n\n\n\n\n\n\n\n\n\n\n3.3.2 mpg by model_year\n\nlet\n    ax=(width =250, height = 250)\n    data(df) * visual(BoxPlot) *\n    mapping(:model_year, :mpg, color=:model_year) |&gt;d-&gt;draw(d,axis=ax)\nend\n\n\n\n\n\n\n\n\n\n\n3.3.3 horsepower-mpg relatiion\n\nax=(width =250, height = 250)\n plt1 = data(df)*mapping(:horsepower,:mpg) * linear()\n #plt2 = data(df)*mapping(:horsepower,:mpg) \n draw(plt1, axis=ax)",
    "crumbs": [
      "Dataset",
      "4.auto mpg"
    ]
  },
  {
    "objectID": "workflow/mlj-cheatsheet.html",
    "href": "workflow/mlj-cheatsheet.html",
    "title": "MLJ Cheatsheet",
    "section": "",
    "text": "Starting an interactive MLJ session\n\n\n\nSpecial title treatment\n\n\njulia&gt; using MLJ\njulia&gt; MLJ_VERSION # version of MLJ for this cheatsheet\nv\"0.20.2\"",
    "crumbs": [
      "Workflow",
      "mlj cheatsheet"
    ]
  },
  {
    "objectID": "workflow/mlj-cheatsheet.html#starting-an-interactive-mlj-session",
    "href": "workflow/mlj-cheatsheet.html#starting-an-interactive-mlj-session",
    "title": "MLJ Cheatsheet",
    "section": "",
    "text": "Starting an interactive MLJ session\n\n\n\nSpecial title treatment\n\n\njulia&gt; using MLJ\njulia&gt; MLJ_VERSION # version of MLJ for this cheatsheet\nv\"0.20.2\"",
    "crumbs": [
      "Workflow",
      "mlj cheatsheet"
    ]
  },
  {
    "objectID": "workflow/mlj-cheatsheet.html#model-search",
    "href": "workflow/mlj-cheatsheet.html#model-search",
    "title": "MLJ Cheatsheet",
    "section": "model search",
    "text": "model search\n\n\nmodel search\n\n\n\ninfo(\"RidgeRegressor\", pkg=\"MultivariateStats\")\n\n\ndoc(\"DecisionTreeClassifier\", pkg=\"DecisionTree\")\n\n\nmodels()\n\n\nmodels(Tree)\n\n\nmodels(matching(X))\n\n\nmodels(matching(X, y))",
    "crumbs": [
      "Workflow",
      "mlj cheatsheet"
    ]
  },
  {
    "objectID": "workflow/mlj-cheatsheet.html#scitypes-and-coercion",
    "href": "workflow/mlj-cheatsheet.html#scitypes-and-coercion",
    "title": "MLJ Cheatsheet",
    "section": "Scitypes and coercion",
    "text": "Scitypes and coercion\n\n\n\nScitypes and coercion\n\n\n\n\nschema(X)\n\n\ncoerce(y, Multiclass)\n\n\ncoerce(X, :x1 =&gt; Continuous, :x2 =&gt; OrderedFactor)\n\n\ncoerce(X, Count =&gt; Continuous)",
    "crumbs": [
      "Workflow",
      "mlj cheatsheet"
    ]
  },
  {
    "objectID": "workflow/mlj-cheatsheet.html#ingesting-data",
    "href": "workflow/mlj-cheatsheet.html#ingesting-data",
    "title": "MLJ Cheatsheet",
    "section": "Ingesting data",
    "text": "Ingesting data\n\n\n\nIngesting data\n\n\n\n\n\nusing RDatasets\nchanning = dataset(\"boot\", \"channing\")\ny, X =  unpack(channing, ==(:Exit); rng=123)\n\n\ntrain, valid, test = partition(eachindex(y), 0.7, 0.2, rng=1234) for 70:20:10 ratio\n\n\nXtrain, Xvalid, Xtest = partition(X, 0.5, 0.3, rng=123)\n\n\nX, y = make_blobs(100, 2) (also: make_moons, make_circles)\n\n\nX, y = make_regression(100, 2)",
    "crumbs": [
      "Workflow",
      "mlj cheatsheet"
    ]
  },
  {
    "objectID": "workflow/mlj-cheatsheet.html#blockquote",
    "href": "workflow/mlj-cheatsheet.html#blockquote",
    "title": "MLJ Cheatsheet",
    "section": "blockquote",
    "text": "blockquote\n\n\nQuote\n\n\n\n\nA well-known quote, contained in a blockquote element.\n\n\nSomeone famous in Source Title",
    "crumbs": [
      "Workflow",
      "mlj cheatsheet"
    ]
  },
  {
    "objectID": "workflow/mlj-cheatsheet.html#machine-co",
    "href": "workflow/mlj-cheatsheet.html#machine-co",
    "title": "MLJ Cheatsheet",
    "section": "Machine Co",
    "text": "Machine Co",
    "crumbs": [
      "Workflow",
      "mlj cheatsheet"
    ]
  },
  {
    "objectID": "workflow/mlj-cheatsheet.html#machine-construct",
    "href": "workflow/mlj-cheatsheet.html#machine-construct",
    "title": "MLJ Cheatsheet",
    "section": "Machine Construct",
    "text": "Machine Construct",
    "crumbs": [
      "Workflow",
      "mlj cheatsheet"
    ]
  },
  {
    "objectID": "workflow/mlj-cheatsheet.html#machine-construction",
    "href": "workflow/mlj-cheatsheet.html#machine-construction",
    "title": "MLJ Cheatsheet",
    "section": "Machine Construction",
    "text": "Machine Construction\n\n\n\nMachine Construction\n\n\n\n\nmodel = KNNRegressor(K=1) and mach = machine(model, X, y)\n\n\nmodel = OneHotEncoder() and mach = machine(model, X)",
    "crumbs": [
      "Workflow",
      "mlj cheatsheet"
    ]
  },
  {
    "objectID": "workflow/mlj-cheatsheet.html#fitting",
    "href": "workflow/mlj-cheatsheet.html#fitting",
    "title": "MLJ Cheatsheet",
    "section": "fitting",
    "text": "fitting\n\n\n\nfitting\n\n\n\n\n  fit!(mach, rows=1:100, verbosity=1, force=false) (defaults shown)",
    "crumbs": [
      "Workflow",
      "mlj cheatsheet"
    ]
  },
  {
    "objectID": "learn/ensemble-model.html",
    "href": "learn/ensemble-model.html",
    "title": "mlj Ensemble model",
    "section": "",
    "text": "using MLJ\nimport DataFrames: DataFrame\nusing PrettyPrinting\nusing StableRNGs\nusing CairoMakie,AlgebraOfGraphics,MakieThemes\nrng = StableRNG(512)\n\nStableRNGs.LehmerRNG(state=0x00000000000000000000000000000401)",
    "crumbs": [
      "Learn",
      "ensemble model"
    ]
  },
  {
    "objectID": "learn/ensemble-model.html#loading-package",
    "href": "learn/ensemble-model.html#loading-package",
    "title": "mlj Ensemble model",
    "section": "",
    "text": "using MLJ\nimport DataFrames: DataFrame\nusing PrettyPrinting\nusing StableRNGs\nusing CairoMakie,AlgebraOfGraphics,MakieThemes\nrng = StableRNG(512)\n\nStableRNGs.LehmerRNG(state=0x00000000000000000000000000000401)",
    "crumbs": [
      "Learn",
      "ensemble model"
    ]
  },
  {
    "objectID": "learn/ensemble-model.html#make-data",
    "href": "learn/ensemble-model.html#make-data",
    "title": "mlj Ensemble model",
    "section": "2. make data",
    "text": "2. make data\n\nXraw = rand(rng, 300, 3)\ny = exp.(Xraw[:,1] - Xraw[:,2] - 2Xraw[:,3] + 0.1*rand(rng, 300))\nX = DataFrame(Xraw, :auto)\ntrain, test = partition(eachindex(y), 0.7);",
    "crumbs": [
      "Learn",
      "ensemble model"
    ]
  },
  {
    "objectID": "learn/ensemble-model.html#load",
    "href": "learn/ensemble-model.html#load",
    "title": "mlj Ensemble model",
    "section": "3. load",
    "text": "3. load",
    "crumbs": [
      "Learn",
      "ensemble model"
    ]
  },
  {
    "objectID": "learn/ensemble-model.html#load-machine-model",
    "href": "learn/ensemble-model.html#load-machine-model",
    "title": "mlj Ensemble model",
    "section": "3. load machine model",
    "text": "3. load machine model\n\nKNNRegressor = @load KNNRegressor\nknn_model = KNNRegressor(K=10)\n\nimport NearestNeighborModels âœ”\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n\n\nKNNRegressor(\n  K = 10, \n  algorithm = :kdtree, \n  metric = Distances.Euclidean(0.0), \n  leafsize = 10, \n  reorder = true, \n  weights = NearestNeighborModels.Uniform())",
    "crumbs": [
      "Learn",
      "ensemble model"
    ]
  },
  {
    "objectID": "learn/ensemble-model.html#section",
    "href": "learn/ensemble-model.html#section",
    "title": "mlj Ensemble model",
    "section": "4",
    "text": "4",
    "crumbs": [
      "Learn",
      "ensemble model"
    ]
  },
  {
    "objectID": "learn/ensemble-model.html#instantiate-model",
    "href": "learn/ensemble-model.html#instantiate-model",
    "title": "mlj Ensemble model",
    "section": "4. instantiate model",
    "text": "4. instantiate model\n\nknn=machine(knn_model,X,y)\n\nuntrained Machine; caches model-specific representations of data\n  model: KNNRegressor(K = 10, â€¦)\n  args: \n    1:  Source @472 â Table{AbstractVector{Continuous}}\n    2:  Source @452 â AbstractVector{Continuous}",
    "crumbs": [
      "Learn",
      "ensemble model"
    ]
  },
  {
    "objectID": "learn/ensemble-model.html#fit-model",
    "href": "learn/ensemble-model.html#fit-model",
    "title": "mlj Ensemble model",
    "section": "5. fit model",
    "text": "5. fit model\n\nfit!(knn, rows=train)\n\n[ Info: Training machine(KNNRegressor(K = 10, â€¦), â€¦).\n\n\ntrained Machine; caches model-specific representations of data\n  model: KNNRegressor(K = 10, â€¦)\n  args: \n    1:  Source @646 â Table{AbstractVector{Continuous}}\n    2:  Source @252 â AbstractVector{Continuous}\n\n\npreidict\n\nyÌ‚ = predict(knn, rows=test)\n\n90-element Vector{Float64}:\n 0.34467109322355544\n 0.47021416109338776\n 0.3605963245828601\n 0.7952383260523377\n 0.8175983630884535\n 0.2787449628967793\n 1.376995501664209\n 0.6335451619415926\n 0.24132363918064742\n 0.48242991564636817\n 1.006001124602383\n 0.4619208719838038\n 0.21005488086190915\n â‹®\n 0.17746741478027778\n 1.26371952798802\n 0.8821595000833542\n 1.3986725366131116\n 0.33686536328173955\n 0.28638455204396673\n 0.5602321105178957\n 0.8161827139978965\n 0.1998143885279288\n 0.8256665297053332\n 0.44366704202674506\n 0.7669517046982802\n\n\nevaluate quality\n\nrms(yÌ‚, y[test])\n\n0.06389980172436369",
    "crumbs": [
      "Learn",
      "ensemble model"
    ]
  },
  {
    "objectID": "learn/ensemble-model.html#single-model",
    "href": "learn/ensemble-model.html#single-model",
    "title": "mlj Ensemble model",
    "section": "3. single model",
    "text": "3. single model\n\n3.1 load machine model\n\nKNNRegressor = @load KNNRegressor\nknn_model = KNNRegressor(K=10)\n\nimport NearestNeighborModels âœ”\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n\n\nKNNRegressor(\n  K = 10, \n  algorithm = :kdtree, \n  metric = Distances.Euclidean(0.0), \n  leafsize = 10, \n  reorder = true, \n  weights = NearestNeighborModels.Uniform())\n\n\n\n\n3.2 instantiate model\n\nknn=machine(knn_model,X,y)\n\nuntrained Machine; caches model-specific representations of data\n  model: KNNRegressor(K = 10, â€¦)\n  args: \n    1:  Source @724 â Table{AbstractVector{Continuous}}\n    2:  Source @606 â AbstractVector{Continuous}\n\n\n\n\n3.3 fit model\n\nfit!(knn, rows=train)\n\n[ Info: Training machine(KNNRegressor(K = 10, â€¦), â€¦).\n\n\ntrained Machine; caches model-specific representations of data\n  model: KNNRegressor(K = 10, â€¦)\n  args: \n    1:  Source @724 â Table{AbstractVector{Continuous}}\n    2:  Source @606 â AbstractVector{Continuous}\n\n\npreidict\n\nyÌ‚ = predict(knn, rows=test)\n\n90-element Vector{Float64}:\n 0.34467109322355544\n 0.47021416109338776\n 0.3605963245828601\n 0.7952383260523377\n 0.8175983630884535\n 0.2787449628967793\n 1.376995501664209\n 0.6335451619415926\n 0.24132363918064742\n 0.48242991564636817\n 1.006001124602383\n 0.4619208719838038\n 0.21005488086190915\n â‹®\n 0.17746741478027778\n 1.26371952798802\n 0.8821595000833542\n 1.3986725366131116\n 0.33686536328173955\n 0.28638455204396673\n 0.5602321105178957\n 0.8161827139978965\n 0.1998143885279288\n 0.8256665297053332\n 0.44366704202674506\n 0.7669517046982802\n\n\nevaluate quality\n\nrms(yÌ‚, y[test])\n\n0.06389980172436369",
    "crumbs": [
      "Learn",
      "ensemble model"
    ]
  },
  {
    "objectID": "learn/ensemble-model.html#homogengous-models",
    "href": "learn/ensemble-model.html#homogengous-models",
    "title": "mlj Ensemble model",
    "section": "4 homogengous models",
    "text": "4 homogengous models",
    "crumbs": [
      "Learn",
      "ensemble model"
    ]
  },
  {
    "objectID": "learn/ensemble-model.html#homogengous-ens",
    "href": "learn/ensemble-model.html#homogengous-ens",
    "title": "mlj Ensemble model",
    "section": "4 homogengous ens",
    "text": "4 homogengous ens",
    "crumbs": [
      "Learn",
      "ensemble model"
    ]
  },
  {
    "objectID": "learn/ensemble-model.html#homogengous-ensembles",
    "href": "learn/ensemble-model.html#homogengous-ensembles",
    "title": "mlj Ensemble model",
    "section": "4 homogengous ensembles",
    "text": "4 homogengous ensembles\n\nensemble_model = EnsembleModel(model=knn_model, n=20);\n\n\nensemble = machine(ensemble_model, X, y)\n\nuntrained Machine; caches model-specific representations of data\n  model: DeterministicEnsembleModel(model = KNNRegressor(K = 10, â€¦), â€¦)\n  args: \n    1:  Source @720 â Table{AbstractVector{Continuous}}\n    2:  Source @516 â AbstractVector{Continuous}\n\n\nevaluate ensemble models\n\nestimates = evaluate!(ensemble, resampling=CV())\n\n\nPerformanceEvaluation object with these fields:\n  model, measure, operation, measurement, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_rows, resampling, repeats\nExtract:\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”‚ measure  â”‚ operation â”‚ measurement â”‚ 1.96*SE â”‚ per_fold                      â‹¯\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”‚ LPLoss(  â”‚ predict   â”‚ 0.00725     â”‚ 0.00277 â”‚ [0.00786, 0.0127, 0.0054, 0.0 â‹¯\nâ”‚   p = 2) â”‚           â”‚             â”‚         â”‚                               â‹¯\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n                                                                1 column omitted\n\n\n\n\n\n4.1 Systme tuning Params\nfirst show params:\n\nparams(ensemble_model) |&gt; pprint\n\n(model = (K = 10,\n          algorithm = :kdtree,\n          metric = Distances.Euclidean(0.0),\n          leafsize = 10,\n          reorder = true,\n          weights = NearestNeighborModels.Uniform()),\n atomic_weights = [],\n bagging_fraction = 0.8,\n rng = Random._GLOBAL_RNG(),\n n = 20,\n acceleration = CPU1{Nothing}(nothing),\n out_of_bag_measure = [])\n\n\nconstruct range of params\n\nB_range = range(ensemble_model, :bagging_fraction,\n                lower=0.5, upper=1.0)\nK_range = range(ensemble_model, :(model.K),\n                lower=1, upper=20)\n\nNumericRange(1 â‰¤ model.K â‰¤ 20; origin=10.5, unit=9.5)\n\n\nconstruct tune models\n\ntm = TunedModel(model=ensemble_model,\n                tuning=Grid(resolution=10), # 10x10 grid\n                resampling=Holdout(fraction_train=0.8, rng=StableRNG(42)),\n                ranges=[B_range, K_range])\n\n[ Info: No measure specified. Setting measure=LPLoss(p = 2). \n\n\nDeterministicTunedModel(\n  model = DeterministicEnsembleModel(\n        model = KNNRegressor(K = 10, â€¦), \n        atomic_weights = Float64[], \n        bagging_fraction = 0.8, \n        rng = Random._GLOBAL_RNG(), \n        n = 20, \n        acceleration = CPU1{Nothing}(nothing), \n        out_of_bag_measure = Any[]), \n  tuning = Grid(\n        goal = nothing, \n        resolution = 10, \n        shuffle = true, \n        rng = Random._GLOBAL_RNG()), \n  resampling = Holdout(\n        fraction_train = 0.8, \n        shuffle = true, \n        rng = StableRNGs.LehmerRNG(state=0x00000000000000000000000000000055)), \n  measure = LPLoss(p = 2), \n  weights = nothing, \n  class_weights = nothing, \n  operation = nothing, \n  range = MLJBase.NumericRange{T, MLJBase.Bounded, Symbol} where T[NumericRange(0.5 â‰¤ bagging_fraction â‰¤ 1.0; origin=0.75, unit=0.25), NumericRange(1 â‰¤ model.K â‰¤ 20; origin=10.5, unit=9.5)], \n  selection_heuristic = MLJTuning.NaiveSelection(nothing), \n  train_best = true, \n  repeats = 1, \n  n = nothing, \n  acceleration = CPU1{Nothing}(nothing), \n  acceleration_resampling = CPU1{Nothing}(nothing), \n  check_measure = true, \n  cache = true)\n\n\nfit model\n\ntuned_ensemble = machine(tm, X, y)\nfit!(tuned_ensemble, rows=train);\n\n[ Info: Training machine(DeterministicTunedModel(model = DeterministicEnsembleModel(model = KNNRegressor(K = 10, â€¦), â€¦), â€¦), â€¦).\n[ Info: Attempting to evaluate 100 models.\nEvaluating over 100 metamodels:   0%[&gt;                        ]  ETA: N/AEvaluating over 100 metamodels:   1%[&gt;                        ]  ETA: 0:00:00Evaluating over 100 metamodels:   2%[&gt;                        ]  ETA: 0:00:00Evaluating over 100 metamodels:   3%[&gt;                        ]  ETA: 0:00:00Evaluating over 100 metamodels:   4%[=&gt;                       ]  ETA: 0:00:00Evaluating over 100 metamodels:   5%[=&gt;                       ]  ETA: 0:00:00Evaluating over 100 metamodels:   6%[=&gt;                       ]  ETA: 0:00:00Evaluating over 100 metamodels:   7%[=&gt;                       ]  ETA: 0:00:00Evaluating over 100 metamodels:   8%[==&gt;                      ]  ETA: 0:00:00Evaluating over 100 metamodels:   9%[==&gt;                      ]  ETA: 0:00:00Evaluating over 100 metamodels:  10%[==&gt;                      ]  ETA: 0:00:00Evaluating over 100 metamodels:  11%[==&gt;                      ]  ETA: 0:00:00Evaluating over 100 metamodels:  12%[===&gt;                     ]  ETA: 0:00:00Evaluating over 100 metamodels:  13%[===&gt;                     ]  ETA: 0:00:00Evaluating over 100 metamodels:  14%[===&gt;                     ]  ETA: 0:00:00Evaluating over 100 metamodels:  15%[===&gt;                     ]  ETA: 0:00:00Evaluating over 100 metamodels:  16%[====&gt;                    ]  ETA: 0:00:00Evaluating over 100 metamodels:  17%[====&gt;                    ]  ETA: 0:00:00Evaluating over 100 metamodels:  18%[====&gt;                    ]  ETA: 0:00:00Evaluating over 100 metamodels:  19%[====&gt;                    ]  ETA: 0:00:00Evaluating over 100 metamodels:  20%[=====&gt;                   ]  ETA: 0:00:00Evaluating over 100 metamodels:  21%[=====&gt;                   ]  ETA: 0:00:00Evaluating over 100 metamodels:  22%[=====&gt;                   ]  ETA: 0:00:00Evaluating over 100 metamodels:  23%[=====&gt;                   ]  ETA: 0:00:00Evaluating over 100 metamodels:  24%[======&gt;                  ]  ETA: 0:00:00Evaluating over 100 metamodels:  25%[======&gt;                  ]  ETA: 0:00:00Evaluating over 100 metamodels:  26%[======&gt;                  ]  ETA: 0:00:00Evaluating over 100 metamodels:  27%[======&gt;                  ]  ETA: 0:00:00Evaluating over 100 metamodels:  28%[=======&gt;                 ]  ETA: 0:00:00Evaluating over 100 metamodels:  29%[=======&gt;                 ]  ETA: 0:00:00Evaluating over 100 metamodels:  30%[=======&gt;                 ]  ETA: 0:00:00Evaluating over 100 metamodels:  31%[=======&gt;                 ]  ETA: 0:00:00Evaluating over 100 metamodels:  32%[========&gt;                ]  ETA: 0:00:00Evaluating over 100 metamodels:  33%[========&gt;                ]  ETA: 0:00:00Evaluating over 100 metamodels:  34%[========&gt;                ]  ETA: 0:00:00Evaluating over 100 metamodels:  35%[========&gt;                ]  ETA: 0:00:00Evaluating over 100 metamodels:  36%[=========&gt;               ]  ETA: 0:00:00Evaluating over 100 metamodels:  37%[=========&gt;               ]  ETA: 0:00:00Evaluating over 100 metamodels:  38%[=========&gt;               ]  ETA: 0:00:00Evaluating over 100 metamodels:  39%[=========&gt;               ]  ETA: 0:00:00Evaluating over 100 metamodels:  40%[==========&gt;              ]  ETA: 0:00:00Evaluating over 100 metamodels:  41%[==========&gt;              ]  ETA: 0:00:00Evaluating over 100 metamodels:  42%[==========&gt;              ]  ETA: 0:00:00Evaluating over 100 metamodels:  43%[==========&gt;              ]  ETA: 0:00:00Evaluating over 100 metamodels:  44%[===========&gt;             ]  ETA: 0:00:00Evaluating over 100 metamodels:  45%[===========&gt;             ]  ETA: 0:00:00Evaluating over 100 metamodels:  46%[===========&gt;             ]  ETA: 0:00:00Evaluating over 100 metamodels:  47%[===========&gt;             ]  ETA: 0:00:00Evaluating over 100 metamodels:  48%[============&gt;            ]  ETA: 0:00:00Evaluating over 100 metamodels:  49%[============&gt;            ]  ETA: 0:00:00Evaluating over 100 metamodels:  50%[============&gt;            ]  ETA: 0:00:00Evaluating over 100 metamodels:  51%[============&gt;            ]  ETA: 0:00:00Evaluating over 100 metamodels:  52%[=============&gt;           ]  ETA: 0:00:00Evaluating over 100 metamodels:  53%[=============&gt;           ]  ETA: 0:00:00Evaluating over 100 metamodels:  54%[=============&gt;           ]  ETA: 0:00:00Evaluating over 100 metamodels:  55%[=============&gt;           ]  ETA: 0:00:00Evaluating over 100 metamodels:  56%[==============&gt;          ]  ETA: 0:00:00Evaluating over 100 metamodels:  57%[==============&gt;          ]  ETA: 0:00:00Evaluating over 100 metamodels:  58%[==============&gt;          ]  ETA: 0:00:00Evaluating over 100 metamodels:  59%[==============&gt;          ]  ETA: 0:00:00Evaluating over 100 metamodels:  60%[===============&gt;         ]  ETA: 0:00:00Evaluating over 100 metamodels:  61%[===============&gt;         ]  ETA: 0:00:00Evaluating over 100 metamodels:  62%[===============&gt;         ]  ETA: 0:00:00Evaluating over 100 metamodels:  63%[===============&gt;         ]  ETA: 0:00:00Evaluating over 100 metamodels:  64%[================&gt;        ]  ETA: 0:00:00Evaluating over 100 metamodels:  65%[================&gt;        ]  ETA: 0:00:00Evaluating over 100 metamodels:  66%[================&gt;        ]  ETA: 0:00:00Evaluating over 100 metamodels:  67%[================&gt;        ]  ETA: 0:00:00Evaluating over 100 metamodels:  68%[=================&gt;       ]  ETA: 0:00:00Evaluating over 100 metamodels:  69%[=================&gt;       ]  ETA: 0:00:00Evaluating over 100 metamodels:  70%[=================&gt;       ]  ETA: 0:00:00Evaluating over 100 metamodels:  71%[=================&gt;       ]  ETA: 0:00:00Evaluating over 100 metamodels:  72%[==================&gt;      ]  ETA: 0:00:00Evaluating over 100 metamodels:  73%[==================&gt;      ]  ETA: 0:00:00Evaluating over 100 metamodels:  74%[==================&gt;      ]  ETA: 0:00:00Evaluating over 100 metamodels:  75%[==================&gt;      ]  ETA: 0:00:00Evaluating over 100 metamodels:  76%[===================&gt;     ]  ETA: 0:00:00Evaluating over 100 metamodels:  77%[===================&gt;     ]  ETA: 0:00:00Evaluating over 100 metamodels:  78%[===================&gt;     ]  ETA: 0:00:00Evaluating over 100 metamodels:  79%[===================&gt;     ]  ETA: 0:00:00Evaluating over 100 metamodels:  80%[====================&gt;    ]  ETA: 0:00:00Evaluating over 100 metamodels:  81%[====================&gt;    ]  ETA: 0:00:00Evaluating over 100 metamodels:  82%[====================&gt;    ]  ETA: 0:00:00Evaluating over 100 metamodels:  83%[====================&gt;    ]  ETA: 0:00:00Evaluating over 100 metamodels:  84%[=====================&gt;   ]  ETA: 0:00:00Evaluating over 100 metamodels:  85%[=====================&gt;   ]  ETA: 0:00:00Evaluating over 100 metamodels:  86%[=====================&gt;   ]  ETA: 0:00:00Evaluating over 100 metamodels:  87%[=====================&gt;   ]  ETA: 0:00:00Evaluating over 100 metamodels:  88%[======================&gt;  ]  ETA: 0:00:00Evaluating over 100 metamodels:  89%[======================&gt;  ]  ETA: 0:00:00Evaluating over 100 metamodels:  90%[======================&gt;  ]  ETA: 0:00:00Evaluating over 100 metamodels:  91%[======================&gt;  ]  ETA: 0:00:00Evaluating over 100 metamodels:  92%[=======================&gt; ]  ETA: 0:00:00Evaluating over 100 metamodels:  93%[=======================&gt; ]  ETA: 0:00:00Evaluating over 100 metamodels:  94%[=======================&gt; ]  ETA: 0:00:00Evaluating over 100 metamodels:  95%[=======================&gt; ]  ETA: 0:00:00Evaluating over 100 metamodels:  96%[========================&gt;]  ETA: 0:00:00Evaluating over 100 metamodels:  97%[========================&gt;]  ETA: 0:00:00Evaluating over 100 metamodels:  98%[========================&gt;]  ETA: 0:00:00Evaluating over 100 metamodels:  99%[========================&gt;]  ETA: 0:00:00Evaluating over 100 metamodels: 100%[=========================] Time: 0:00:00\n\n\n\n\n4.2 report tunning results\n\nbest_ensemble = fitted_params(tuned_ensemble).best_model\n@show best_ensemble.model.K\n@show best_ensemble.bagging_fraction\n\nbest_ensemble.model.K = 3\nbest_ensemble.bagging_fraction = 0.6111111111111112\n\n\n0.6111111111111112\n\n\nget detail report\n\nr = report(tuned_ensemble)\n\n(best_model = DeterministicEnsembleModel(model = KNNRegressor(K = 3, â€¦), â€¦),\n best_history_entry = (model = DeterministicEnsembleModel(model = KNNRegressor(K = 3, â€¦), â€¦),\n                       measure = StatisticalMeasuresBase.RobustMeasure{StatisticalMeasuresBase.FussyMeasure{StatisticalMeasuresBase.RobustMeasure{StatisticalMeasuresBase.Multimeasure{StatisticalMeasuresBase.SupportsMissingsMeasure{StatisticalMeasures.LPLossOnScalars{Int64}}, Nothing, StatisticalMeasuresBase.Mean, typeof(identity)}}, Nothing}}[LPLoss(p = 2)],\n                       measurement = [0.0014622515676628268],\n                       per_fold = [[0.0014622515676628268]],),\n history = NamedTuple{(:model, :measure, :measurement, :per_fold), Tuple{MLJEnsembles.DeterministicEnsembleModel{NearestNeighborModels.KNNRegressor}, Vector{StatisticalMeasuresBase.RobustMeasure{StatisticalMeasuresBase.FussyMeasure{StatisticalMeasuresBase.RobustMeasure{StatisticalMeasuresBase.Multimeasure{StatisticalMeasuresBase.SupportsMissingsMeasure{StatisticalMeasures.LPLossOnScalars{Int64}}, Nothing, StatisticalMeasuresBase.Mean, typeof(identity)}}, Nothing}}}, Vector{Float64}, Vector{Vector{Float64}}}}[(model = DeterministicEnsembleModel(model = KNNRegressor(K = 18, â€¦), â€¦), measure = [LPLoss(p = 2)], measurement = [0.006563470407815136], per_fold = [[0.006563470407815136]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 16, â€¦), â€¦), measure = [LPLoss(p = 2)], measurement = [0.0122246059374603], per_fold = [[0.0122246059374603]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 1, â€¦), â€¦), measure = [LPLoss(p = 2)], measurement = [0.0036628015206465734], per_fold = [[0.0036628015206465734]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 14, â€¦), â€¦), measure = [LPLoss(p = 2)], measurement = [0.008020213502274467], per_fold = [[0.008020213502274467]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 5, â€¦), â€¦), measure = [LPLoss(p = 2)], measurement = [0.001787953402180091], per_fold = [[0.001787953402180091]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 7, â€¦), â€¦), measure = [LPLoss(p = 2)], measurement = [0.0025945479977448303], per_fold = [[0.0025945479977448303]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 18, â€¦), â€¦), measure = [LPLoss(p = 2)], measurement = [0.020351582333467805], per_fold = [[0.020351582333467805]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 12, â€¦), â€¦), measure = [LPLoss(p = 2)], measurement = [0.009536648688229333], per_fold = [[0.009536648688229333]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 12, â€¦), â€¦), measure = [LPLoss(p = 2)], measurement = [0.009536648688229333], per_fold = [[0.009536648688229333]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 12, â€¦), â€¦), measure = [LPLoss(p = 2)], measurement = [0.009536648688229333], per_fold = [[0.009536648688229333]])  â€¦  (model = DeterministicEnsembleModel(model = KNNRegressor(K = 20, â€¦), â€¦), measure = [LPLoss(p = 2)], measurement = [0.01646454995922476], per_fold = [[0.01646454995922476]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 14, â€¦), â€¦), measure = [LPLoss(p = 2)], measurement = [0.006456471195446319], per_fold = [[0.006456471195446319]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 1, â€¦), â€¦), measure = [LPLoss(p = 2)], measurement = [0.003151889979016522], per_fold = [[0.003151889979016522]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 3, â€¦), â€¦), measure = [LPLoss(p = 2)], measurement = [0.0030046273651692227], per_fold = [[0.0030046273651692227]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 14, â€¦), â€¦), measure = [LPLoss(p = 2)], measurement = [0.004745533719078165], per_fold = [[0.004745533719078165]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 7, â€¦), â€¦), measure = [LPLoss(p = 2)], measurement = [0.004590011170145359], per_fold = [[0.004590011170145359]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 7, â€¦), â€¦), measure = [LPLoss(p = 2)], measurement = [0.004590011170145359], per_fold = [[0.004590011170145359]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 3, â€¦), â€¦), measure = [LPLoss(p = 2)], measurement = [0.0036225306262139788], per_fold = [[0.0036225306262139788]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 1, â€¦), â€¦), measure = [LPLoss(p = 2)], measurement = [0.0031016792824338077], per_fold = [[0.0031016792824338077]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 12, â€¦), â€¦), measure = [LPLoss(p = 2)], measurement = [0.007526080413070994], per_fold = [[0.007526080413070994]])],\n best_report = (measures = Any[],\n                oob_measurements = missing,),\n plotting = (parameter_names = [\"bagging_fraction\", \"model.K\"],\n             parameter_scales = [:linear, :linear],\n             parameter_values = Any[0.8888888888888888 18; 0.6111111111111112 16; â€¦ ; 0.5555555555555556 1; 0.6111111111111112 12],\n             measurements = [0.006563470407815136, 0.0122246059374603, 0.0036628015206465734, 0.008020213502274467, 0.001787953402180091, 0.0025945479977448303, 0.020351582333467805, 0.009536648688229333, 0.009536648688229333, 0.009536648688229333  â€¦  0.01646454995922476, 0.006456471195446319, 0.003151889979016522, 0.0030046273651692227, 0.004745533719078165, 0.004590011170145359, 0.004590011170145359, 0.0036225306262139788, 0.0031016792824338077, 0.007526080413070994],),)\n\n\n\n\n4.3 plot tunning results\n\nres = r.plotting\ntable=(vals_b = res.parameter_values[:, 1],\n            vals_k = res.parameter_values[:, 2],\n            measurement=res.measurements\n)\n\ndatalayer=data(table)\nmappinglayer=mapping(:vals_b,:vals_k,:measurement,color=:measurement)\nvislayler=visual(Tricontourf,colormap = :batlow)\nax=(width = 400, height = 400)\nplt=datalayer*mappinglayer*vislayler\ndraw(plt,axis=ax)\n\n\n\n\n\n\n\nFigureÂ 1: fig-ensemble-params-tunning\n\n\n\n\n\n\n\n4.4 predict with ensemble model\n\nyÌ‚ = predict(tuned_ensemble, rows=test)\n@show rms(yÌ‚, y[test])\n\nrms(Å·, y[test]) = 0.05120320246985217\n\n\n0.05120320246985217",
    "crumbs": [
      "Learn",
      "ensemble model"
    ]
  }
]
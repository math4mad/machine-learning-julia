[
  {
    "objectID": "workflow/tailwindcss.html",
    "href": "workflow/tailwindcss.html",
    "title": "embedding tailwind css",
    "section": "",
    "text": "test tailwind css\n\nExample heading New\n\n\nNotifications 4\n\n\n❶ ❷ ❸ ❹ ❺ ➅ ➆ ➇ ➈ ➉\n\n\n❶ ❷ ❸ ❹ ❺ ➅ ➆ ➇ ➈ ➉\n\n\n\n\nFirst Name\n\n\nLast Name\n\n\nPoints\n\n\n\n\nJill\n\n\nSmith\n\n\n50\n\n\n\n\nJill\n\n\nSmith\n\n\n50\n\n\n\n\nJill\n\n\nSmith\n\n\n50"
  },
  {
    "objectID": "workflow/mlj-cheatsheet.html",
    "href": "workflow/mlj-cheatsheet.html",
    "title": "MLJ Cheatsheet",
    "section": "",
    "text": "Special title treatment\n\n\njulia&gt; using MLJ\njulia&gt; MLJ_VERSION # version of MLJ for this cheatsheet\nv\"0.20.2\"",
    "crumbs": [
      "Workflow",
      "mlj cheatsheet"
    ]
  },
  {
    "objectID": "workflow/mlj-cheatsheet.html#starting-an-interactive-mlj-session",
    "href": "workflow/mlj-cheatsheet.html#starting-an-interactive-mlj-session",
    "title": "MLJ Cheatsheet",
    "section": "",
    "text": "Special title treatment\n\n\njulia&gt; using MLJ\njulia&gt; MLJ_VERSION # version of MLJ for this cheatsheet\nv\"0.20.2\"",
    "crumbs": [
      "Workflow",
      "mlj cheatsheet"
    ]
  },
  {
    "objectID": "workflow/mlj-cheatsheet.html#model-search",
    "href": "workflow/mlj-cheatsheet.html#model-search",
    "title": "MLJ Cheatsheet",
    "section": "model search",
    "text": "model search\n\n\nmodel search\n\n\n\ninfo(\"RidgeRegressor\", pkg=\"MultivariateStats\")\n\n\ndoc(\"DecisionTreeClassifier\", pkg=\"DecisionTree\")\n\n\nmodels()\n\n\nmodels(Tree)\n\n\nmodels(matching(X))\n\n\nmodels(matching(X, y))",
    "crumbs": [
      "Workflow",
      "mlj cheatsheet"
    ]
  },
  {
    "objectID": "workflow/mlj-cheatsheet.html#scitypes-and-coercion",
    "href": "workflow/mlj-cheatsheet.html#scitypes-and-coercion",
    "title": "MLJ Cheatsheet",
    "section": "Scitypes and coercion",
    "text": "Scitypes and coercion\n\n\n\nScitypes and coercion\n\n\n\n\nschema(X)\n\n\ncoerce(y, Multiclass)\n\n\ncoerce(X, :x1 =&gt; Continuous, :x2 =&gt; OrderedFactor)\n\n\ncoerce(X, Count =&gt; Continuous)",
    "crumbs": [
      "Workflow",
      "mlj cheatsheet"
    ]
  },
  {
    "objectID": "workflow/mlj-cheatsheet.html#ingesting-data",
    "href": "workflow/mlj-cheatsheet.html#ingesting-data",
    "title": "MLJ Cheatsheet",
    "section": "Ingesting data",
    "text": "Ingesting data\n\n\n\nIngesting data\n\n\n\n\nusing RDatasets\nchanning = dataset(\"boot\", \"channing\")\ny, X =  unpack(channing, ==(:Exit); rng=123)\n\n\ntrain, valid, test = partition(eachindex(y), 0.7, 0.2, rng=1234) for 70:20:10 ratio\n\n\nXtrain, Xvalid, Xtest = partition(X, 0.5, 0.3, rng=123)\n\n\nX, y = make_blobs(100, 2) (also: make_moons, make_circles)\n\n\nX, y = make_regression(100, 2)",
    "crumbs": [
      "Workflow",
      "mlj cheatsheet"
    ]
  },
  {
    "objectID": "workflow/mlj-cheatsheet.html#machine-construction",
    "href": "workflow/mlj-cheatsheet.html#machine-construction",
    "title": "MLJ Cheatsheet",
    "section": "Machine Construction",
    "text": "Machine Construction\n\n\n\nMachine Construction\n\n\n\n\nmodel = KNNRegressor(K=1) and mach = machine(model, X, y)\n\n\nmodel = OneHotEncoder() and mach = machine(model, X)",
    "crumbs": [
      "Workflow",
      "mlj cheatsheet"
    ]
  },
  {
    "objectID": "workflow/mlj-cheatsheet.html#fitting",
    "href": "workflow/mlj-cheatsheet.html#fitting",
    "title": "MLJ Cheatsheet",
    "section": "fitting",
    "text": "fitting\n\n\n\nfitting\n\n\n\n\n  fit!(mach, rows=1:100, verbosity=1, force=false) (defaults shown)",
    "crumbs": [
      "Workflow",
      "mlj cheatsheet"
    ]
  },
  {
    "objectID": "workflow/composing-models.html",
    "href": "workflow/composing-models.html",
    "title": "Composing Models",
    "section": "",
    "text": "MLJ 中组合模型有三种形式\n\n\n\n\n\n    %%| label: fig-mlj-composing model\n    %%| fig-cap: \"MLJ  composing model\"\n    %%| fig-width: 6.5\n    %%| echo: true\n    graph TD\n        C(Composing Models)\n        C --&gt;|One| D([Pipeline Model ])\n        C --&gt;|Two| E([Ensemble Model])\n        C --&gt;|Three| F([Stack Model])",
    "crumbs": [
      "Workflow",
      "composing models"
    ]
  },
  {
    "objectID": "start/getting-start.html",
    "href": "start/getting-start.html",
    "title": "Build first MLJ machine learning model",
    "section": "",
    "text": "we followed workflow, step by step\n\n\nfor iris dataset ref: Iris Dataset EDA\n\nusing Pipe\nusing CSV,DataFrames\nusing MLJ\ndf=CSV.File(\"../data/iris.csv\")|&gt;DataFrame;\n\n\nschema(df)\n\n\n┌──────────────┬────────────┬──────────┐\n│ names        │ scitypes   │ types    │\n├──────────────┼────────────┼──────────┤\n│ sepal_length │ Continuous │ Float64  │\n│ sepal_width  │ Continuous │ Float64  │\n│ petal_length │ Continuous │ Float64  │\n│ petal_width  │ Continuous │ Float64  │\n│ species      │ Textual    │ String15 │\n└──────────────┴────────────┴──────────┘\n\n\n\n\nfor species need transform to Categorical type\n\ndf.species= coerce(df.species,Multiclass);\nschema(df)\n\n\n┌──────────────┬───────────────┬────────────────────────────────────┐\n│ names        │ scitypes      │ types                              │\n├──────────────┼───────────────┼────────────────────────────────────┤\n│ sepal_length │ Continuous    │ Float64                            │\n│ sepal_width  │ Continuous    │ Float64                            │\n│ petal_length │ Continuous    │ Float64                            │\n│ petal_width  │ Continuous    │ Float64                            │\n│ species      │ Multiclass{3} │ CategoricalValue{String15, UInt32} │\n└──────────────┴───────────────┴────────────────────────────────────┘\n\n\n\n\nnow we need to split data\n\ny, X = unpack(df, ==(:species); rng=123)\n\n\n(CategoricalArrays.CategoricalValue{String15, UInt32}[String15(\"virginica\"), String15(\"versicolor\"), String15(\"virginica\"), String15(\"setosa\"), String15(\"versicolor\"), String15(\"virginica\"), String15(\"setosa\"), String15(\"versicolor\"), String15(\"setosa\"), String15(\"virginica\")  …  String15(\"virginica\"), String15(\"virginica\"), String15(\"virginica\"), String15(\"versicolor\"), String15(\"virginica\"), String15(\"setosa\"), String15(\"virginica\"), String15(\"virginica\"), String15(\"setosa\"), String15(\"setosa\")], 150×4 DataFrame\n Row │ sepal_length  sepal_width  petal_length  petal_width \n     │ Float64       Float64      Float64       Float64     \n─────┼──────────────────────────────────────────────────────\n   1 │          6.7          3.3           5.7          2.1\n   2 │          5.7          2.8           4.1          1.3\n   3 │          7.2          3.0           5.8          1.6\n   4 │          4.4          2.9           1.4          0.2\n   5 │          5.6          2.5           3.9          1.1\n   6 │          6.5          3.0           5.2          2.0\n   7 │          4.4          3.0           1.3          0.2\n   8 │          6.1          2.9           4.7          1.4\n   9 │          5.4          3.9           1.7          0.4\n  10 │          4.9          2.5           4.5          1.7\n  11 │          6.3          2.5           4.9          1.5\n  ⋮  │      ⋮             ⋮            ⋮             ⋮\n 141 │          6.4          2.7           5.3          1.9\n 142 │          6.8          3.2           5.9          2.3\n 143 │          6.9          3.1           5.4          2.1\n 144 │          6.1          2.8           4.0          1.3\n 145 │          6.7          2.5           5.8          1.8\n 146 │          5.0          3.5           1.3          0.3\n 147 │          7.6          3.0           6.6          2.1\n 148 │          6.3          2.5           5.0          1.9\n 149 │          5.1          3.8           1.6          0.2\n 150 │          5.0          3.6           1.4          0.2\n                                            129 rows omitted)\n\n\n\n\n\n\n\nres=@pipe models(matching(X,y))|&gt; DataFrame|&gt;select(_,[:name,:package_name])\n\n53×2 DataFrame28 rows omitted\n\n\n\nRow\nname\npackage_name\n\n\n\nString\nString\n\n\n\n\n1\nAdaBoostClassifier\nMLJScikitLearnInterface\n\n\n2\nAdaBoostStumpClassifier\nDecisionTree\n\n\n3\nBaggingClassifier\nMLJScikitLearnInterface\n\n\n4\nBayesianLDA\nMLJScikitLearnInterface\n\n\n5\nBayesianLDA\nMultivariateStats\n\n\n6\nBayesianQDA\nMLJScikitLearnInterface\n\n\n7\nBayesianSubspaceLDA\nMultivariateStats\n\n\n8\nCatBoostClassifier\nCatBoost\n\n\n9\nConstantClassifier\nMLJModels\n\n\n10\nDecisionTreeClassifier\nBetaML\n\n\n11\nDecisionTreeClassifier\nDecisionTree\n\n\n12\nDeterministicConstantClassifier\nMLJModels\n\n\n13\nDummyClassifier\nMLJScikitLearnInterface\n\n\n⋮\n⋮\n⋮\n\n\n42\nRandomForestClassifier\nMLJScikitLearnInterface\n\n\n43\nRidgeCVClassifier\nMLJScikitLearnInterface\n\n\n44\nRidgeClassifier\nMLJScikitLearnInterface\n\n\n45\nSGDClassifier\nMLJScikitLearnInterface\n\n\n46\nSVC\nLIBSVM\n\n\n47\nSVMClassifier\nMLJScikitLearnInterface\n\n\n48\nSVMLinearClassifier\nMLJScikitLearnInterface\n\n\n49\nSVMNuClassifier\nMLJScikitLearnInterface\n\n\n50\nStableForestClassifier\nSIRUS\n\n\n51\nStableRulesClassifier\nSIRUS\n\n\n52\nSubspaceLDA\nMultivariateStats\n\n\n53\nXGBoostClassifier\nXGBoost\n\n\n\n\n\n\nsimple way to look into model details:\ndoc($name, pkg=$package_name)\n\ndoc(\"AdaBoostStumpClassifier\", pkg=\"DecisionTree\")\n\nAdaBoostStumpClassifier\nA model type for constructing a Ada-boosted stump classifier, based on DecisionTree.jl, and implementing the MLJ model interface.\nFrom MLJ, the type can be imported using\nAdaBoostStumpClassifier = @load AdaBoostStumpClassifier pkg=DecisionTree\nDo model = AdaBoostStumpClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in AdaBoostStumpClassifier(n_iter=...).\n\n\nIn MLJ or MLJBase, bind an instance model to data with\nmach = machine(model, X, y)\nwhere:\n\nX: any table of input features (eg, a DataFrame) whose columns each have one of the following element scitypes: Continuous, Count, or &lt;:OrderedFactor; check column scitypes with schema(X)\ny: the target, which can be any AbstractVector whose element scitype is &lt;:OrderedFactor or &lt;:Multiclass; check the scitype with scitype(y)\n\nTrain the machine with fit!(mach, rows=...).\n\n\n\n\nn_iter=10: number of iterations of AdaBoost\nfeature_importance: method to use for computing feature importances. One of (:impurity, :split)\nrng=Random.GLOBAL_RNG: random number generator or seed\n\n\n\n\n\npredict(mach, Xnew): return predictions of the target given features Xnew having the same scitype as X above. Predictions are probabilistic, but uncalibrated.\npredict_mode(mach, Xnew): instead return the mode of each prediction above.\n\n\n\n\nThe fields of fitted_params(mach) are:\n\nstumps: the Ensemble object returned by the core DecisionTree.jl algorithm.\ncoefficients: the stump coefficients (one per stump)\n\n\n\n\n\nfeatures: the names of the features encountered in training\n\n\n\n\n\nfeature_importances(mach) returns a vector of (feature::Symbol =&gt; importance) pairs; the type of importance is determined by the hyperparameter feature_importance (see above)\n\n\n\n\nusing MLJ\nBooster = @load AdaBoostStumpClassifier pkg=DecisionTree\nbooster = Booster(n_iter=15)\n\nX, y = @load_iris\nmach = machine(booster, X, y) |&gt; fit!\n\nXnew = (sepal_length = [6.4, 7.2, 7.4],\n        sepal_width = [2.8, 3.0, 2.8],\n        petal_length = [5.6, 5.8, 6.1],\n        petal_width = [2.1, 1.6, 1.9],)\nyhat = predict(mach, Xnew) # probabilistic predictions\npredict_mode(mach, Xnew)   # point predictions\npdf.(yhat, \"virginica\")    # probabilities for the \"verginica\" class\n\nfitted_params(mach).stumps # raw `Ensemble` object from DecisionTree.jl\nfitted_params(mach).coefs  # coefficient associated with each stump\nfeature_importances(mach)\nSee also DecisionTree.jl and the unwrapped model type MLJDecisionTreeInterface.DecisionTree.AdaBoostStumpClassifier.\n\n\n\n\n\n\nthere are too many models, so you just add code if repel notice missing package , you just install packages.\n\nTree = @load DecisionTreeClassifier pkg=DecisionTree\n\nimport MLJDecisionTreeInterface ✔\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n\n\nMLJDecisionTreeInterface.DecisionTreeClassifier\n\n\n\n\n\n\n tree=Tree()\n\nDecisionTreeClassifier(\n  max_depth = -1, \n  min_samples_leaf = 1, \n  min_samples_split = 2, \n  min_purity_increase = 0.0, \n  n_subfeatures = 0, \n  post_prune = false, \n  merge_purity_threshold = 1.0, \n  display_depth = 5, \n  feature_importance = :impurity, \n  rng = Random._GLOBAL_RNG())\n\n\n\n\n\n\nevaluate(tree, X, y,\n         resampling=CV(shuffle=true),\n         measures=[log_loss, accuracy],\n         verbosity=0)\n\n\nPerformanceEvaluation object with these fields:\n  model, measure, operation, measurement, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_rows, resampling, repeats\nExtract:\n┌──────────────────────┬──────────────┬─────────────┬─────────┬─────────────────\n│ measure              │ operation    │ measurement │ 1.96*SE │ per_fold       ⋯\n├──────────────────────┼──────────────┼─────────────┼─────────┼─────────────────\n│ LogLoss(             │ predict      │ 2.4         │ 1.31    │ [1.44, 2.88, 2 ⋯\n│   tol = 2.22045e-16) │              │             │         │                ⋯\n│ Accuracy()           │ predict_mode │ 0.933       │ 0.0362  │ [0.96, 0.92, 1 ⋯\n└──────────────────────┴──────────────┴─────────────┴─────────┴─────────────────\n                                                                1 column omitted\n\n\n\n\n\n\n\nfill model and data to machine\n\nmach=machine(tree, X,y)\n\nuntrained Machine; caches model-specific representations of data\n  model: DecisionTreeClassifier(max_depth = -1, …)\n  args: \n    1:  Source @757 ⏎ Table{AbstractVector{Continuous}}\n    2:  Source @003 ⏎ AbstractVector{Multiclass{3}}\n\n\n\n\n\n\ntrain, test = partition(eachindex(y), 0.7); \n\n\n\n\nuse train index to inform model to we must not use all data!!!\n\nfit!(mach, rows=train)\n\n[ Info: Training machine(DecisionTreeClassifier(max_depth = -1, …), …).\n\n\ntrained Machine; caches model-specific representations of data\n  model: DecisionTreeClassifier(max_depth = -1, …)\n  args: \n    1:  Source @757 ⏎ Table{AbstractVector{Continuous}}\n    2:  Source @003 ⏎ AbstractVector{Multiclass{3}}\n\n\n\n\n\nsame with other julia stats model , X must be a matrix ,even if it is one row and one column data\n\nyhat = predict_mode(mach, X[test,:])\n\n45-element CategoricalArrays.CategoricalArray{String15,1,UInt32}:\n String15(\"versicolor\")\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"virginica\")\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"versicolor\")\n String15(\"versicolor\")\n String15(\"virginica\")\n String15(\"versicolor\")\n String15(\"virginica\")\n String15(\"versicolor\")\n String15(\"virginica\")\n ⋮\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"virginica\")\n String15(\"virginica\")\n String15(\"virginica\")\n String15(\"versicolor\")\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"virginica\")\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"setosa\")\n\n\n\n\n\n\naccuracy(yhat,y[test])\n\n0.9555555555555556\n\n\n\nlevels(yhat)\n\n3-element Vector{String15}:\n \"setosa\"\n \"versicolor\"\n \"virginica\"",
    "crumbs": [
      "Start",
      "build first model"
    ]
  },
  {
    "objectID": "start/getting-start.html#getting-start",
    "href": "start/getting-start.html#getting-start",
    "title": "Build first MLJ machine learning model",
    "section": "",
    "text": "we followed workflow, step by step\n\n\nfor iris dataset ref: Iris Dataset EDA\n\nusing Pipe\nusing CSV,DataFrames\nusing MLJ\ndf=CSV.File(\"../data/iris.csv\")|&gt;DataFrame;\n\n\nschema(df)\n\n\n┌──────────────┬────────────┬──────────┐\n│ names        │ scitypes   │ types    │\n├──────────────┼────────────┼──────────┤\n│ sepal_length │ Continuous │ Float64  │\n│ sepal_width  │ Continuous │ Float64  │\n│ petal_length │ Continuous │ Float64  │\n│ petal_width  │ Continuous │ Float64  │\n│ species      │ Textual    │ String15 │\n└──────────────┴────────────┴──────────┘\n\n\n\n\nfor species need transform to Categorical type\n\ndf.species= coerce(df.species,Multiclass);\nschema(df)\n\n\n┌──────────────┬───────────────┬────────────────────────────────────┐\n│ names        │ scitypes      │ types                              │\n├──────────────┼───────────────┼────────────────────────────────────┤\n│ sepal_length │ Continuous    │ Float64                            │\n│ sepal_width  │ Continuous    │ Float64                            │\n│ petal_length │ Continuous    │ Float64                            │\n│ petal_width  │ Continuous    │ Float64                            │\n│ species      │ Multiclass{3} │ CategoricalValue{String15, UInt32} │\n└──────────────┴───────────────┴────────────────────────────────────┘\n\n\n\n\nnow we need to split data\n\ny, X = unpack(df, ==(:species); rng=123)\n\n\n(CategoricalArrays.CategoricalValue{String15, UInt32}[String15(\"virginica\"), String15(\"versicolor\"), String15(\"virginica\"), String15(\"setosa\"), String15(\"versicolor\"), String15(\"virginica\"), String15(\"setosa\"), String15(\"versicolor\"), String15(\"setosa\"), String15(\"virginica\")  …  String15(\"virginica\"), String15(\"virginica\"), String15(\"virginica\"), String15(\"versicolor\"), String15(\"virginica\"), String15(\"setosa\"), String15(\"virginica\"), String15(\"virginica\"), String15(\"setosa\"), String15(\"setosa\")], 150×4 DataFrame\n Row │ sepal_length  sepal_width  petal_length  petal_width \n     │ Float64       Float64      Float64       Float64     \n─────┼──────────────────────────────────────────────────────\n   1 │          6.7          3.3           5.7          2.1\n   2 │          5.7          2.8           4.1          1.3\n   3 │          7.2          3.0           5.8          1.6\n   4 │          4.4          2.9           1.4          0.2\n   5 │          5.6          2.5           3.9          1.1\n   6 │          6.5          3.0           5.2          2.0\n   7 │          4.4          3.0           1.3          0.2\n   8 │          6.1          2.9           4.7          1.4\n   9 │          5.4          3.9           1.7          0.4\n  10 │          4.9          2.5           4.5          1.7\n  11 │          6.3          2.5           4.9          1.5\n  ⋮  │      ⋮             ⋮            ⋮             ⋮\n 141 │          6.4          2.7           5.3          1.9\n 142 │          6.8          3.2           5.9          2.3\n 143 │          6.9          3.1           5.4          2.1\n 144 │          6.1          2.8           4.0          1.3\n 145 │          6.7          2.5           5.8          1.8\n 146 │          5.0          3.5           1.3          0.3\n 147 │          7.6          3.0           6.6          2.1\n 148 │          6.3          2.5           5.0          1.9\n 149 │          5.1          3.8           1.6          0.2\n 150 │          5.0          3.6           1.4          0.2\n                                            129 rows omitted)\n\n\n\n\n\n\n\nres=@pipe models(matching(X,y))|&gt; DataFrame|&gt;select(_,[:name,:package_name])\n\n53×2 DataFrame28 rows omitted\n\n\n\nRow\nname\npackage_name\n\n\n\nString\nString\n\n\n\n\n1\nAdaBoostClassifier\nMLJScikitLearnInterface\n\n\n2\nAdaBoostStumpClassifier\nDecisionTree\n\n\n3\nBaggingClassifier\nMLJScikitLearnInterface\n\n\n4\nBayesianLDA\nMLJScikitLearnInterface\n\n\n5\nBayesianLDA\nMultivariateStats\n\n\n6\nBayesianQDA\nMLJScikitLearnInterface\n\n\n7\nBayesianSubspaceLDA\nMultivariateStats\n\n\n8\nCatBoostClassifier\nCatBoost\n\n\n9\nConstantClassifier\nMLJModels\n\n\n10\nDecisionTreeClassifier\nBetaML\n\n\n11\nDecisionTreeClassifier\nDecisionTree\n\n\n12\nDeterministicConstantClassifier\nMLJModels\n\n\n13\nDummyClassifier\nMLJScikitLearnInterface\n\n\n⋮\n⋮\n⋮\n\n\n42\nRandomForestClassifier\nMLJScikitLearnInterface\n\n\n43\nRidgeCVClassifier\nMLJScikitLearnInterface\n\n\n44\nRidgeClassifier\nMLJScikitLearnInterface\n\n\n45\nSGDClassifier\nMLJScikitLearnInterface\n\n\n46\nSVC\nLIBSVM\n\n\n47\nSVMClassifier\nMLJScikitLearnInterface\n\n\n48\nSVMLinearClassifier\nMLJScikitLearnInterface\n\n\n49\nSVMNuClassifier\nMLJScikitLearnInterface\n\n\n50\nStableForestClassifier\nSIRUS\n\n\n51\nStableRulesClassifier\nSIRUS\n\n\n52\nSubspaceLDA\nMultivariateStats\n\n\n53\nXGBoostClassifier\nXGBoost\n\n\n\n\n\n\nsimple way to look into model details:\ndoc($name, pkg=$package_name)\n\ndoc(\"AdaBoostStumpClassifier\", pkg=\"DecisionTree\")\n\nAdaBoostStumpClassifier\nA model type for constructing a Ada-boosted stump classifier, based on DecisionTree.jl, and implementing the MLJ model interface.\nFrom MLJ, the type can be imported using\nAdaBoostStumpClassifier = @load AdaBoostStumpClassifier pkg=DecisionTree\nDo model = AdaBoostStumpClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in AdaBoostStumpClassifier(n_iter=...).\n\n\nIn MLJ or MLJBase, bind an instance model to data with\nmach = machine(model, X, y)\nwhere:\n\nX: any table of input features (eg, a DataFrame) whose columns each have one of the following element scitypes: Continuous, Count, or &lt;:OrderedFactor; check column scitypes with schema(X)\ny: the target, which can be any AbstractVector whose element scitype is &lt;:OrderedFactor or &lt;:Multiclass; check the scitype with scitype(y)\n\nTrain the machine with fit!(mach, rows=...).\n\n\n\n\nn_iter=10: number of iterations of AdaBoost\nfeature_importance: method to use for computing feature importances. One of (:impurity, :split)\nrng=Random.GLOBAL_RNG: random number generator or seed\n\n\n\n\n\npredict(mach, Xnew): return predictions of the target given features Xnew having the same scitype as X above. Predictions are probabilistic, but uncalibrated.\npredict_mode(mach, Xnew): instead return the mode of each prediction above.\n\n\n\n\nThe fields of fitted_params(mach) are:\n\nstumps: the Ensemble object returned by the core DecisionTree.jl algorithm.\ncoefficients: the stump coefficients (one per stump)\n\n\n\n\n\nfeatures: the names of the features encountered in training\n\n\n\n\n\nfeature_importances(mach) returns a vector of (feature::Symbol =&gt; importance) pairs; the type of importance is determined by the hyperparameter feature_importance (see above)\n\n\n\n\nusing MLJ\nBooster = @load AdaBoostStumpClassifier pkg=DecisionTree\nbooster = Booster(n_iter=15)\n\nX, y = @load_iris\nmach = machine(booster, X, y) |&gt; fit!\n\nXnew = (sepal_length = [6.4, 7.2, 7.4],\n        sepal_width = [2.8, 3.0, 2.8],\n        petal_length = [5.6, 5.8, 6.1],\n        petal_width = [2.1, 1.6, 1.9],)\nyhat = predict(mach, Xnew) # probabilistic predictions\npredict_mode(mach, Xnew)   # point predictions\npdf.(yhat, \"virginica\")    # probabilities for the \"verginica\" class\n\nfitted_params(mach).stumps # raw `Ensemble` object from DecisionTree.jl\nfitted_params(mach).coefs  # coefficient associated with each stump\nfeature_importances(mach)\nSee also DecisionTree.jl and the unwrapped model type MLJDecisionTreeInterface.DecisionTree.AdaBoostStumpClassifier.\n\n\n\n\n\n\nthere are too many models, so you just add code if repel notice missing package , you just install packages.\n\nTree = @load DecisionTreeClassifier pkg=DecisionTree\n\nimport MLJDecisionTreeInterface ✔\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n\n\nMLJDecisionTreeInterface.DecisionTreeClassifier\n\n\n\n\n\n\n tree=Tree()\n\nDecisionTreeClassifier(\n  max_depth = -1, \n  min_samples_leaf = 1, \n  min_samples_split = 2, \n  min_purity_increase = 0.0, \n  n_subfeatures = 0, \n  post_prune = false, \n  merge_purity_threshold = 1.0, \n  display_depth = 5, \n  feature_importance = :impurity, \n  rng = Random._GLOBAL_RNG())\n\n\n\n\n\n\nevaluate(tree, X, y,\n         resampling=CV(shuffle=true),\n         measures=[log_loss, accuracy],\n         verbosity=0)\n\n\nPerformanceEvaluation object with these fields:\n  model, measure, operation, measurement, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_rows, resampling, repeats\nExtract:\n┌──────────────────────┬──────────────┬─────────────┬─────────┬─────────────────\n│ measure              │ operation    │ measurement │ 1.96*SE │ per_fold       ⋯\n├──────────────────────┼──────────────┼─────────────┼─────────┼─────────────────\n│ LogLoss(             │ predict      │ 2.4         │ 1.31    │ [1.44, 2.88, 2 ⋯\n│   tol = 2.22045e-16) │              │             │         │                ⋯\n│ Accuracy()           │ predict_mode │ 0.933       │ 0.0362  │ [0.96, 0.92, 1 ⋯\n└──────────────────────┴──────────────┴─────────────┴─────────┴─────────────────\n                                                                1 column omitted\n\n\n\n\n\n\n\nfill model and data to machine\n\nmach=machine(tree, X,y)\n\nuntrained Machine; caches model-specific representations of data\n  model: DecisionTreeClassifier(max_depth = -1, …)\n  args: \n    1:  Source @757 ⏎ Table{AbstractVector{Continuous}}\n    2:  Source @003 ⏎ AbstractVector{Multiclass{3}}\n\n\n\n\n\n\ntrain, test = partition(eachindex(y), 0.7); \n\n\n\n\nuse train index to inform model to we must not use all data!!!\n\nfit!(mach, rows=train)\n\n[ Info: Training machine(DecisionTreeClassifier(max_depth = -1, …), …).\n\n\ntrained Machine; caches model-specific representations of data\n  model: DecisionTreeClassifier(max_depth = -1, …)\n  args: \n    1:  Source @757 ⏎ Table{AbstractVector{Continuous}}\n    2:  Source @003 ⏎ AbstractVector{Multiclass{3}}\n\n\n\n\n\nsame with other julia stats model , X must be a matrix ,even if it is one row and one column data\n\nyhat = predict_mode(mach, X[test,:])\n\n45-element CategoricalArrays.CategoricalArray{String15,1,UInt32}:\n String15(\"versicolor\")\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"virginica\")\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"versicolor\")\n String15(\"versicolor\")\n String15(\"virginica\")\n String15(\"versicolor\")\n String15(\"virginica\")\n String15(\"versicolor\")\n String15(\"virginica\")\n ⋮\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"virginica\")\n String15(\"virginica\")\n String15(\"virginica\")\n String15(\"versicolor\")\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"virginica\")\n String15(\"virginica\")\n String15(\"setosa\")\n String15(\"setosa\")\n\n\n\n\n\n\naccuracy(yhat,y[test])\n\n0.9555555555555556\n\n\n\nlevels(yhat)\n\n3-element Vector{String15}:\n \"setosa\"\n \"versicolor\"\n \"virginica\"",
    "crumbs": [
      "Start",
      "build first model"
    ]
  },
  {
    "objectID": "packages/statesbase.html",
    "href": "packages/statesbase.html",
    "title": "StatsBase.jl",
    "section": "",
    "text": "StatsBase.jl"
  },
  {
    "objectID": "learn/pca-explained.html",
    "href": "learn/pca-explained.html",
    "title": "PCA explained",
    "section": "",
    "text": "PCA explained\n\n\n\n我们通过一个简单实例来直观介绍一下主成分分析(PCA).主成分分析对高维数据分析非常有用, 不失一般性, 在二维数据上仍然可以工作. 在数学中,最基本的方法是度量, 例如度量人的身高. 度量身高时我们需要一个工具, 直尺,卷尺,甚至是一根树枝都可以作为度量工具. 如果要度量两个属性, 例如同时度量身高和体重, 我们还需要一个称量工具. 对于二维数据,度量不同属性的工具本不能交叉使用. 在大多数情况下,属性都不可或缺,但是在有些特殊情况下, 有些属性就显得不那么重要. 例如有一条公路, 度量公路的属性有长度和宽度. 但是当我们在地图上观察公路的时候, 发现道路的宽度经常没有,出现的只是长度曲线. 所以在地图应用中, 道路的宽度信息不那么重要,在地图中, 很容易观察到长度信息比宽度信息重要. 从数值上看, 长度的跨度比宽度跨度大. 因此我们可以舍去宽度信息. 主成分分析实际就是上面这段直观解释的算法实现.\n\n\nLet’s take a brief example to visualize Principal Component Analysis (PCA), which is very useful for high-dimensional data analysis, without losing generality, and can still work on two-dimensional data. In mathematics, the most basic method is to measure, for example to measure a person’s height. We need a tool when measuring height, a ruler, a tape measure and even a branch can be used as a measurement tool. If we want to measure two attributes, such as measuring height and weight at the same time, we also need a weighing tool. For 2D data, tools to measure different properties cannot be used interchangeably. In most cases, attributes are indispensable, but in some special cases, some attributes are less important. For example, if there is a highway, the properties of the highway are measured by length and width. But when we look at the road on the map, we often don’t have the width of the road, but only the length curve. So in the map application, the width information of the road is not so important, and in the map, it is easy to observe that the length information is more important than the width information. Numerically, the span of the length is larger than the span of the width. So we can discard the width information. Principal component analysis is actually the algorithm implementation of the above intuitive explanation.\n\n\n using CSV,DataFrames,Tidier,Pipe\n using CairoMakie,AlgebraOfGraphics\n using Random,Distributions\n dist=Normal()\n Random.seed!(34343)\n\nTaskLocalRNG()\n\n\n\n  xs=range(0,4pi, 300)\n  fy(x)=5*sin(x)+3*x+2*rand(dist)\n  ys=fy.(xs)\n  df=DataFrame(xs=xs,ys=ys)\n\n300×2 DataFrame275 rows omitted\n\n\n\nRow\nxs\nys\n\n\n\nFloat64\nFloat64\n\n\n\n\n1\n0.0\n-0.272431\n\n\n2\n0.042028\n-1.63477\n\n\n3\n0.084056\n0.367143\n\n\n4\n0.126084\n1.39357\n\n\n5\n0.168112\n-4.05051\n\n\n6\n0.21014\n0.794208\n\n\n7\n0.252168\n0.94975\n\n\n8\n0.294196\n3.93994\n\n\n9\n0.336224\n3.71181\n\n\n10\n0.378252\n5.31367\n\n\n11\n0.42028\n1.89908\n\n\n12\n0.462308\n5.11653\n\n\n13\n0.504336\n7.07824\n\n\n⋮\n⋮\n⋮\n\n\n289\n12.1041\n33.2352\n\n\n290\n12.1461\n33.5851\n\n\n291\n12.1881\n33.8266\n\n\n292\n12.2301\n36.0369\n\n\n293\n12.2722\n36.9264\n\n\n294\n12.3142\n35.2095\n\n\n295\n12.3562\n36.0006\n\n\n296\n12.3983\n35.3022\n\n\n297\n12.4403\n33.5829\n\n\n298\n12.4823\n38.3843\n\n\n299\n12.5243\n38.147\n\n\n300\n12.5664\n36.3962\n\n\n\n\n\n\n\nax=(width=500,height=250)\ndlayer=data(df)\nmlayer=mapping(:xs,:ys)\nvlayer=visual(Scatter,markersize=6, color=(:blue,0.5),strokewidth=1,strokecolor=:black)\nplt1=dlayer*mlayer*vlayer\ndraw(plt1,axis=ax)\n\n\n\n\n\n\n\n\n\n对于二维数据, 理论上可以占满坐标系中所有的位置,但是实际的数据可能只出现在有限的位置, 我们可以说真实的数据是嵌入在二维空间中的\n\n\nFor two-dimensional data, theoretically all positions in the coordinate system can be filled, but the actual data may only appear in a limited number of positions, and we can say that the real data is embedded in the two-dimensional space\n\n\nmatrix=Matrix(df)|&gt;transpose\n\n2×300 transpose(::Matrix{Float64}) with eltype Float64:\n  0.0        0.042028  0.084056  …  12.4403  12.4823  12.5243  12.5664\n -0.272431  -1.63477   0.367143     33.5829  38.3843  38.147   36.3962\n\n\n\nSVD decomposition\n\nusing  LinearAlgebra\nfunction svd_block(A::Matrix)\n        U,Σ,V=svd(A)\n        return (i)-&gt;U[:,i]*Σ[i]*V[:,i]'\nend\n\nd=@pipe df|&gt;Matrix|&gt;svd_block\ndf2=@pipe d(1)|&gt;DataFrame(_,:auto)\n\ndlayer2=data(df2)\nmlayer2=mapping(:x1,:x2)\nplt2=dlayer2*mlayer2*visual(Scatter,markersize=6, color=(:yellow,0.5),strokewidth=1,strokecolor=:black)\ndraw(plt2)\n\n\n\n\n\n\n\n\n\ndraw(plt1+plt2)\n\n\n\n\n\n\n\n\n\ndf3=@pipe d(2)|&gt;DataFrame(_,:auto)\n\ndlayer3=data(df3)\nmlayer3=mapping(:x1,:x2)\nplt3=dlayer3*mlayer3*visual(Scatter,markersize=6, color=(:yellow,0.5),strokewidth=1,strokecolor=:black)\ndraw(plt3)\n\n\n\n\n\n\n\n\n\nax=(width=500,height=500)\ndraw(plt2+plt3,axis=ax)",
    "crumbs": [
      "Learn",
      "PCA  explained"
    ]
  },
  {
    "objectID": "learn/learning-network-1.html",
    "href": "learn/learning-network-1.html",
    "title": "Learning Network 1",
    "section": "",
    "text": "using MLJ\n    using MLJ\n    X, y = make_blobs(cluster_std=10.0, rng=123) \n    Xnew, _ = make_blobs(3) \n\n(Tables.MatrixTable{Matrix{Float64}} with 3 rows, 2 columns, and schema:\n :x1  Float64\n :x2  Float64, CategoricalArrays.CategoricalValue{Int64, UInt32}[3, 2, 1])",
    "crumbs": [
      "Learn",
      "learnig network 1"
    ]
  },
  {
    "objectID": "learn/learning-network-1.html#load-package",
    "href": "learn/learning-network-1.html#load-package",
    "title": "Learning Network 1",
    "section": "",
    "text": "using MLJ\n    using MLJ\n    X, y = make_blobs(cluster_std=10.0, rng=123) \n    Xnew, _ = make_blobs(3) \n\n(Tables.MatrixTable{Matrix{Float64}} with 3 rows, 2 columns, and schema:\n :x1  Float64\n :x2  Float64, CategoricalArrays.CategoricalValue{Int64, UInt32}[3, 2, 1])",
    "crumbs": [
      "Learn",
      "learnig network 1"
    ]
  },
  {
    "objectID": "learn/learning-network-1.html#load-models",
    "href": "learn/learning-network-1.html#load-models",
    "title": "Learning Network 1",
    "section": "2. load models",
    "text": "2. load models\n\n    pca = (@load PCA pkg=MultivariateStats verbosity=0)()\n    tree = (@load DecisionTreeClassifier pkg=DecisionTree verbosity=0)()\n\nDecisionTreeClassifier(\n  max_depth = -1, \n  min_samples_leaf = 1, \n  min_samples_split = 2, \n  min_purity_increase = 0.0, \n  n_subfeatures = 0, \n  post_prune = false, \n  merge_purity_threshold = 1.0, \n  display_depth = 5, \n  feature_importance = :impurity, \n  rng = Random._GLOBAL_RNG())",
    "crumbs": [
      "Learn",
      "learnig network 1"
    ]
  },
  {
    "objectID": "learn/learning-network-1.html#training-data-as-source",
    "href": "learn/learning-network-1.html#training-data-as-source",
    "title": "Learning Network 1",
    "section": "3. training data as source",
    "text": "3. training data as source\n\n    Xs = source(X)\n    ys = source(y)\n\nSource @773 ⏎ `AbstractVector{Multiclass{3}}`\n\n\n\nmach1 = machine(pca, Xs)\nx = transform(mach1, Xs)\n\n\nNode @412 → PCA(…)\n  args:\n    1:  Source @532\n  formula:\n    transform(\n      machine(PCA(maxoutdim = 0, …), …), \n      Source @532)\n\n\n\n\nmach2 = machine(tree, x, ys)\nyhat = predict(mach2, x)\n\n\nNode @878 → DecisionTreeClassifier(…)\n  args:\n    1:  Node @412 → PCA(…)\n  formula:\n    predict(\n      machine(DecisionTreeClassifier(max_depth = -1, …), …), \n      transform(\n        machine(PCA(maxoutdim = 0, …), …), \n        Source @532))\n\n\n\n\nfit!(yhat)\n\n[ Info: Training machine(PCA(maxoutdim = 0, …), …).\n[ Info: Training machine(DecisionTreeClassifier(max_depth = -1, …), …).\n\n\n\nNode @878 → DecisionTreeClassifier(…)\n  args:\n    1:  Node @412 → PCA(…)\n  formula:\n    predict(\n      machine(DecisionTreeClassifier(max_depth = -1, …), …), \n      transform(\n        machine(PCA(maxoutdim = 0, …), …), \n        Source @532))",
    "crumbs": [
      "Learn",
      "learnig network 1"
    ]
  },
  {
    "objectID": "learn/example.html",
    "href": "learn/example.html",
    "title": "Code-insertion Example",
    "section": "",
    "text": "quoted block added before the post"
  },
  {
    "objectID": "learn/example.html#heading",
    "href": "learn/example.html#heading",
    "title": "Code-insertion Example",
    "section": "Heading",
    "text": "Heading\nThis filter adds code immediately before and after the post."
  },
  {
    "objectID": "learn/example.html#appended-section-after-the-post",
    "href": "learn/example.html#appended-section-after-the-post",
    "title": "Code-insertion Example",
    "section": "Appended Section After the Post",
    "text": "Appended Section After the Post"
  },
  {
    "objectID": "learn/ensemble-model.html",
    "href": "learn/ensemble-model.html",
    "title": "mlj Ensemble model",
    "section": "",
    "text": "using MLJ\nimport DataFrames: DataFrame\nusing PrettyPrinting\nusing StableRNGs\nusing CairoMakie,AlgebraOfGraphics,MakieThemes\nrng = StableRNG(512)\n\nStableRNGs.LehmerRNG(state=0x00000000000000000000000000000401)",
    "crumbs": [
      "Learn",
      "ensemble model"
    ]
  },
  {
    "objectID": "learn/ensemble-model.html#loading-package",
    "href": "learn/ensemble-model.html#loading-package",
    "title": "mlj Ensemble model",
    "section": "",
    "text": "using MLJ\nimport DataFrames: DataFrame\nusing PrettyPrinting\nusing StableRNGs\nusing CairoMakie,AlgebraOfGraphics,MakieThemes\nrng = StableRNG(512)\n\nStableRNGs.LehmerRNG(state=0x00000000000000000000000000000401)",
    "crumbs": [
      "Learn",
      "ensemble model"
    ]
  },
  {
    "objectID": "learn/ensemble-model.html#make-data",
    "href": "learn/ensemble-model.html#make-data",
    "title": "mlj Ensemble model",
    "section": "2. make data",
    "text": "2. make data\n\nXraw = rand(rng, 300, 3)\ny = exp.(Xraw[:,1] - Xraw[:,2] - 2Xraw[:,3] + 0.1*rand(rng, 300))\nX = DataFrame(Xraw, :auto)\ntrain, test = partition(eachindex(y), 0.7);",
    "crumbs": [
      "Learn",
      "ensemble model"
    ]
  },
  {
    "objectID": "learn/ensemble-model.html#single-model",
    "href": "learn/ensemble-model.html#single-model",
    "title": "mlj Ensemble model",
    "section": "3. single model",
    "text": "3. single model\n\n3.1 load machine model\n\nKNNRegressor = @load KNNRegressor\nknn_model = KNNRegressor(K=10)\n\nimport NearestNeighborModels ✔\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n\n\nKNNRegressor(\n  K = 10, \n  algorithm = :kdtree, \n  metric = Distances.Euclidean(0.0), \n  leafsize = 10, \n  reorder = true, \n  weights = NearestNeighborModels.Uniform())\n\n\n\n\n3.2 instantiate model\n\nknn=machine(knn_model,X,y)\n\nuntrained Machine; caches model-specific representations of data\n  model: KNNRegressor(K = 10, …)\n  args: \n    1:  Source @724 ⏎ Table{AbstractVector{Continuous}}\n    2:  Source @606 ⏎ AbstractVector{Continuous}\n\n\n\n\n3.3 fit model\n\nfit!(knn, rows=train)\n\n[ Info: Training machine(KNNRegressor(K = 10, …), …).\n\n\ntrained Machine; caches model-specific representations of data\n  model: KNNRegressor(K = 10, …)\n  args: \n    1:  Source @724 ⏎ Table{AbstractVector{Continuous}}\n    2:  Source @606 ⏎ AbstractVector{Continuous}\n\n\npreidict\n\nŷ = predict(knn, rows=test)\n\n90-element Vector{Float64}:\n 0.34467109322355544\n 0.47021416109338776\n 0.3605963245828601\n 0.7952383260523377\n 0.8175983630884535\n 0.2787449628967793\n 1.376995501664209\n 0.6335451619415926\n 0.24132363918064742\n 0.48242991564636817\n 1.006001124602383\n 0.4619208719838038\n 0.21005488086190915\n ⋮\n 0.17746741478027778\n 1.26371952798802\n 0.8821595000833542\n 1.3986725366131116\n 0.33686536328173955\n 0.28638455204396673\n 0.5602321105178957\n 0.8161827139978965\n 0.1998143885279288\n 0.8256665297053332\n 0.44366704202674506\n 0.7669517046982802\n\n\nevaluate quality\n\nrms(ŷ, y[test])\n\n0.06389980172436369",
    "crumbs": [
      "Learn",
      "ensemble model"
    ]
  },
  {
    "objectID": "learn/ensemble-model.html#homogengous-ensembles",
    "href": "learn/ensemble-model.html#homogengous-ensembles",
    "title": "mlj Ensemble model",
    "section": "4 homogengous ensembles",
    "text": "4 homogengous ensembles\n\nensemble_model = EnsembleModel(model=knn_model, n=20);\n\n\nensemble = machine(ensemble_model, X, y)\n\nuntrained Machine; caches model-specific representations of data\n  model: DeterministicEnsembleModel(model = KNNRegressor(K = 10, …), …)\n  args: \n    1:  Source @720 ⏎ Table{AbstractVector{Continuous}}\n    2:  Source @516 ⏎ AbstractVector{Continuous}\n\n\nevaluate ensemble models\n\nestimates = evaluate!(ensemble, resampling=CV())\n\n\nPerformanceEvaluation object with these fields:\n  model, measure, operation, measurement, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_rows, resampling, repeats\nExtract:\n┌──────────┬───────────┬─────────────┬─────────┬────────────────────────────────\n│ measure  │ operation │ measurement │ 1.96*SE │ per_fold                      ⋯\n├──────────┼───────────┼─────────────┼─────────┼────────────────────────────────\n│ LPLoss(  │ predict   │ 0.00725     │ 0.00277 │ [0.00786, 0.0127, 0.0054, 0.0 ⋯\n│   p = 2) │           │             │         │                               ⋯\n└──────────┴───────────┴─────────────┴─────────┴────────────────────────────────\n                                                                1 column omitted\n\n\n\n\n\n4.1 Systme tuning Params\nfirst show params:\n\nparams(ensemble_model) |&gt; pprint\n\n(model = (K = 10,\n          algorithm = :kdtree,\n          metric = Distances.Euclidean(0.0),\n          leafsize = 10,\n          reorder = true,\n          weights = NearestNeighborModels.Uniform()),\n atomic_weights = [],\n bagging_fraction = 0.8,\n rng = Random._GLOBAL_RNG(),\n n = 20,\n acceleration = CPU1{Nothing}(nothing),\n out_of_bag_measure = [])\n\n\nconstruct range of params\n\nB_range = range(ensemble_model, :bagging_fraction,\n                lower=0.5, upper=1.0)\nK_range = range(ensemble_model, :(model.K),\n                lower=1, upper=20)\n\nNumericRange(1 ≤ model.K ≤ 20; origin=10.5, unit=9.5)\n\n\nconstruct tune models\n\ntm = TunedModel(model=ensemble_model,\n                tuning=Grid(resolution=10), # 10x10 grid\n                resampling=Holdout(fraction_train=0.8, rng=StableRNG(42)),\n                ranges=[B_range, K_range])\n\n[ Info: No measure specified. Setting measure=LPLoss(p = 2). \n\n\nDeterministicTunedModel(\n  model = DeterministicEnsembleModel(\n        model = KNNRegressor(K = 10, …), \n        atomic_weights = Float64[], \n        bagging_fraction = 0.8, \n        rng = Random._GLOBAL_RNG(), \n        n = 20, \n        acceleration = CPU1{Nothing}(nothing), \n        out_of_bag_measure = Any[]), \n  tuning = Grid(\n        goal = nothing, \n        resolution = 10, \n        shuffle = true, \n        rng = Random._GLOBAL_RNG()), \n  resampling = Holdout(\n        fraction_train = 0.8, \n        shuffle = true, \n        rng = StableRNGs.LehmerRNG(state=0x00000000000000000000000000000055)), \n  measure = LPLoss(p = 2), \n  weights = nothing, \n  class_weights = nothing, \n  operation = nothing, \n  range = MLJBase.NumericRange{T, MLJBase.Bounded, Symbol} where T[NumericRange(0.5 ≤ bagging_fraction ≤ 1.0; origin=0.75, unit=0.25), NumericRange(1 ≤ model.K ≤ 20; origin=10.5, unit=9.5)], \n  selection_heuristic = MLJTuning.NaiveSelection(nothing), \n  train_best = true, \n  repeats = 1, \n  n = nothing, \n  acceleration = CPU1{Nothing}(nothing), \n  acceleration_resampling = CPU1{Nothing}(nothing), \n  check_measure = true, \n  cache = true)\n\n\nfit model\n\ntuned_ensemble = machine(tm, X, y)\nfit!(tuned_ensemble, rows=train);\n\n[ Info: Training machine(DeterministicTunedModel(model = DeterministicEnsembleModel(model = KNNRegressor(K = 10, …), …), …), …).\n[ Info: Attempting to evaluate 100 models.\nEvaluating over 100 metamodels:   0%[&gt;                        ]  ETA: N/AEvaluating over 100 metamodels:   1%[&gt;                        ]  ETA: 0:00:00Evaluating over 100 metamodels:   2%[&gt;                        ]  ETA: 0:00:00Evaluating over 100 metamodels:   3%[&gt;                        ]  ETA: 0:00:00Evaluating over 100 metamodels:   4%[=&gt;                       ]  ETA: 0:00:00Evaluating over 100 metamodels:   5%[=&gt;                       ]  ETA: 0:00:00Evaluating over 100 metamodels:   6%[=&gt;                       ]  ETA: 0:00:00Evaluating over 100 metamodels:   7%[=&gt;                       ]  ETA: 0:00:00Evaluating over 100 metamodels:   8%[==&gt;                      ]  ETA: 0:00:00Evaluating over 100 metamodels:   9%[==&gt;                      ]  ETA: 0:00:00Evaluating over 100 metamodels:  10%[==&gt;                      ]  ETA: 0:00:00Evaluating over 100 metamodels:  11%[==&gt;                      ]  ETA: 0:00:00Evaluating over 100 metamodels:  12%[===&gt;                     ]  ETA: 0:00:00Evaluating over 100 metamodels:  13%[===&gt;                     ]  ETA: 0:00:00Evaluating over 100 metamodels:  14%[===&gt;                     ]  ETA: 0:00:00Evaluating over 100 metamodels:  15%[===&gt;                     ]  ETA: 0:00:00Evaluating over 100 metamodels:  16%[====&gt;                    ]  ETA: 0:00:00Evaluating over 100 metamodels:  17%[====&gt;                    ]  ETA: 0:00:00Evaluating over 100 metamodels:  18%[====&gt;                    ]  ETA: 0:00:00Evaluating over 100 metamodels:  19%[====&gt;                    ]  ETA: 0:00:00Evaluating over 100 metamodels:  20%[=====&gt;                   ]  ETA: 0:00:00Evaluating over 100 metamodels:  21%[=====&gt;                   ]  ETA: 0:00:00Evaluating over 100 metamodels:  22%[=====&gt;                   ]  ETA: 0:00:00Evaluating over 100 metamodels:  23%[=====&gt;                   ]  ETA: 0:00:00Evaluating over 100 metamodels:  24%[======&gt;                  ]  ETA: 0:00:00Evaluating over 100 metamodels:  25%[======&gt;                  ]  ETA: 0:00:00Evaluating over 100 metamodels:  26%[======&gt;                  ]  ETA: 0:00:00Evaluating over 100 metamodels:  27%[======&gt;                  ]  ETA: 0:00:00Evaluating over 100 metamodels:  28%[=======&gt;                 ]  ETA: 0:00:00Evaluating over 100 metamodels:  29%[=======&gt;                 ]  ETA: 0:00:00Evaluating over 100 metamodels:  30%[=======&gt;                 ]  ETA: 0:00:00Evaluating over 100 metamodels:  31%[=======&gt;                 ]  ETA: 0:00:00Evaluating over 100 metamodels:  32%[========&gt;                ]  ETA: 0:00:00Evaluating over 100 metamodels:  33%[========&gt;                ]  ETA: 0:00:00Evaluating over 100 metamodels:  34%[========&gt;                ]  ETA: 0:00:00Evaluating over 100 metamodels:  35%[========&gt;                ]  ETA: 0:00:00Evaluating over 100 metamodels:  36%[=========&gt;               ]  ETA: 0:00:00Evaluating over 100 metamodels:  37%[=========&gt;               ]  ETA: 0:00:00Evaluating over 100 metamodels:  38%[=========&gt;               ]  ETA: 0:00:00Evaluating over 100 metamodels:  39%[=========&gt;               ]  ETA: 0:00:00Evaluating over 100 metamodels:  40%[==========&gt;              ]  ETA: 0:00:00Evaluating over 100 metamodels:  41%[==========&gt;              ]  ETA: 0:00:00Evaluating over 100 metamodels:  42%[==========&gt;              ]  ETA: 0:00:00Evaluating over 100 metamodels:  43%[==========&gt;              ]  ETA: 0:00:00Evaluating over 100 metamodels:  44%[===========&gt;             ]  ETA: 0:00:00Evaluating over 100 metamodels:  45%[===========&gt;             ]  ETA: 0:00:00Evaluating over 100 metamodels:  46%[===========&gt;             ]  ETA: 0:00:00Evaluating over 100 metamodels:  47%[===========&gt;             ]  ETA: 0:00:00Evaluating over 100 metamodels:  48%[============&gt;            ]  ETA: 0:00:00Evaluating over 100 metamodels:  49%[============&gt;            ]  ETA: 0:00:00Evaluating over 100 metamodels:  50%[============&gt;            ]  ETA: 0:00:00Evaluating over 100 metamodels:  51%[============&gt;            ]  ETA: 0:00:00Evaluating over 100 metamodels:  52%[=============&gt;           ]  ETA: 0:00:00Evaluating over 100 metamodels:  53%[=============&gt;           ]  ETA: 0:00:00Evaluating over 100 metamodels:  54%[=============&gt;           ]  ETA: 0:00:00Evaluating over 100 metamodels:  55%[=============&gt;           ]  ETA: 0:00:00Evaluating over 100 metamodels:  56%[==============&gt;          ]  ETA: 0:00:00Evaluating over 100 metamodels:  57%[==============&gt;          ]  ETA: 0:00:00Evaluating over 100 metamodels:  58%[==============&gt;          ]  ETA: 0:00:00Evaluating over 100 metamodels:  59%[==============&gt;          ]  ETA: 0:00:00Evaluating over 100 metamodels:  60%[===============&gt;         ]  ETA: 0:00:00Evaluating over 100 metamodels:  61%[===============&gt;         ]  ETA: 0:00:00Evaluating over 100 metamodels:  62%[===============&gt;         ]  ETA: 0:00:00Evaluating over 100 metamodels:  63%[===============&gt;         ]  ETA: 0:00:00Evaluating over 100 metamodels:  64%[================&gt;        ]  ETA: 0:00:00Evaluating over 100 metamodels:  65%[================&gt;        ]  ETA: 0:00:00Evaluating over 100 metamodels:  66%[================&gt;        ]  ETA: 0:00:00Evaluating over 100 metamodels:  67%[================&gt;        ]  ETA: 0:00:00Evaluating over 100 metamodels:  68%[=================&gt;       ]  ETA: 0:00:00Evaluating over 100 metamodels:  69%[=================&gt;       ]  ETA: 0:00:00Evaluating over 100 metamodels:  70%[=================&gt;       ]  ETA: 0:00:00Evaluating over 100 metamodels:  71%[=================&gt;       ]  ETA: 0:00:00Evaluating over 100 metamodels:  72%[==================&gt;      ]  ETA: 0:00:00Evaluating over 100 metamodels:  73%[==================&gt;      ]  ETA: 0:00:00Evaluating over 100 metamodels:  74%[==================&gt;      ]  ETA: 0:00:00Evaluating over 100 metamodels:  75%[==================&gt;      ]  ETA: 0:00:00Evaluating over 100 metamodels:  76%[===================&gt;     ]  ETA: 0:00:00Evaluating over 100 metamodels:  77%[===================&gt;     ]  ETA: 0:00:00Evaluating over 100 metamodels:  78%[===================&gt;     ]  ETA: 0:00:00Evaluating over 100 metamodels:  79%[===================&gt;     ]  ETA: 0:00:00Evaluating over 100 metamodels:  80%[====================&gt;    ]  ETA: 0:00:00Evaluating over 100 metamodels:  81%[====================&gt;    ]  ETA: 0:00:00Evaluating over 100 metamodels:  82%[====================&gt;    ]  ETA: 0:00:00Evaluating over 100 metamodels:  83%[====================&gt;    ]  ETA: 0:00:00Evaluating over 100 metamodels:  84%[=====================&gt;   ]  ETA: 0:00:00Evaluating over 100 metamodels:  85%[=====================&gt;   ]  ETA: 0:00:00Evaluating over 100 metamodels:  86%[=====================&gt;   ]  ETA: 0:00:00Evaluating over 100 metamodels:  87%[=====================&gt;   ]  ETA: 0:00:00Evaluating over 100 metamodels:  88%[======================&gt;  ]  ETA: 0:00:00Evaluating over 100 metamodels:  89%[======================&gt;  ]  ETA: 0:00:00Evaluating over 100 metamodels:  90%[======================&gt;  ]  ETA: 0:00:00Evaluating over 100 metamodels:  91%[======================&gt;  ]  ETA: 0:00:00Evaluating over 100 metamodels:  92%[=======================&gt; ]  ETA: 0:00:00Evaluating over 100 metamodels:  93%[=======================&gt; ]  ETA: 0:00:00Evaluating over 100 metamodels:  94%[=======================&gt; ]  ETA: 0:00:00Evaluating over 100 metamodels:  95%[=======================&gt; ]  ETA: 0:00:00Evaluating over 100 metamodels:  96%[========================&gt;]  ETA: 0:00:00Evaluating over 100 metamodels:  97%[========================&gt;]  ETA: 0:00:00Evaluating over 100 metamodels:  98%[========================&gt;]  ETA: 0:00:00Evaluating over 100 metamodels:  99%[========================&gt;]  ETA: 0:00:00Evaluating over 100 metamodels: 100%[=========================] Time: 0:00:00\n\n\n\n\n4.2 report tunning results\n\nbest_ensemble = fitted_params(tuned_ensemble).best_model\n@show best_ensemble.model.K\n@show best_ensemble.bagging_fraction\n\nbest_ensemble.model.K = 3\nbest_ensemble.bagging_fraction = 0.6111111111111112\n\n\n0.6111111111111112\n\n\nget detail report\n\nr = report(tuned_ensemble)\n\n(best_model = DeterministicEnsembleModel(model = KNNRegressor(K = 3, …), …),\n best_history_entry = (model = DeterministicEnsembleModel(model = KNNRegressor(K = 3, …), …),\n                       measure = StatisticalMeasuresBase.RobustMeasure{StatisticalMeasuresBase.FussyMeasure{StatisticalMeasuresBase.RobustMeasure{StatisticalMeasuresBase.Multimeasure{StatisticalMeasuresBase.SupportsMissingsMeasure{StatisticalMeasures.LPLossOnScalars{Int64}}, Nothing, StatisticalMeasuresBase.Mean, typeof(identity)}}, Nothing}}[LPLoss(p = 2)],\n                       measurement = [0.0014622515676628268],\n                       per_fold = [[0.0014622515676628268]],),\n history = NamedTuple{(:model, :measure, :measurement, :per_fold), Tuple{MLJEnsembles.DeterministicEnsembleModel{NearestNeighborModels.KNNRegressor}, Vector{StatisticalMeasuresBase.RobustMeasure{StatisticalMeasuresBase.FussyMeasure{StatisticalMeasuresBase.RobustMeasure{StatisticalMeasuresBase.Multimeasure{StatisticalMeasuresBase.SupportsMissingsMeasure{StatisticalMeasures.LPLossOnScalars{Int64}}, Nothing, StatisticalMeasuresBase.Mean, typeof(identity)}}, Nothing}}}, Vector{Float64}, Vector{Vector{Float64}}}}[(model = DeterministicEnsembleModel(model = KNNRegressor(K = 18, …), …), measure = [LPLoss(p = 2)], measurement = [0.006563470407815136], per_fold = [[0.006563470407815136]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 16, …), …), measure = [LPLoss(p = 2)], measurement = [0.0122246059374603], per_fold = [[0.0122246059374603]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 1, …), …), measure = [LPLoss(p = 2)], measurement = [0.0036628015206465734], per_fold = [[0.0036628015206465734]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 14, …), …), measure = [LPLoss(p = 2)], measurement = [0.008020213502274467], per_fold = [[0.008020213502274467]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 5, …), …), measure = [LPLoss(p = 2)], measurement = [0.001787953402180091], per_fold = [[0.001787953402180091]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 7, …), …), measure = [LPLoss(p = 2)], measurement = [0.0025945479977448303], per_fold = [[0.0025945479977448303]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 18, …), …), measure = [LPLoss(p = 2)], measurement = [0.020351582333467805], per_fold = [[0.020351582333467805]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 12, …), …), measure = [LPLoss(p = 2)], measurement = [0.009536648688229333], per_fold = [[0.009536648688229333]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 12, …), …), measure = [LPLoss(p = 2)], measurement = [0.009536648688229333], per_fold = [[0.009536648688229333]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 12, …), …), measure = [LPLoss(p = 2)], measurement = [0.009536648688229333], per_fold = [[0.009536648688229333]])  …  (model = DeterministicEnsembleModel(model = KNNRegressor(K = 20, …), …), measure = [LPLoss(p = 2)], measurement = [0.01646454995922476], per_fold = [[0.01646454995922476]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 14, …), …), measure = [LPLoss(p = 2)], measurement = [0.006456471195446319], per_fold = [[0.006456471195446319]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 1, …), …), measure = [LPLoss(p = 2)], measurement = [0.003151889979016522], per_fold = [[0.003151889979016522]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 3, …), …), measure = [LPLoss(p = 2)], measurement = [0.0030046273651692227], per_fold = [[0.0030046273651692227]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 14, …), …), measure = [LPLoss(p = 2)], measurement = [0.004745533719078165], per_fold = [[0.004745533719078165]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 7, …), …), measure = [LPLoss(p = 2)], measurement = [0.004590011170145359], per_fold = [[0.004590011170145359]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 7, …), …), measure = [LPLoss(p = 2)], measurement = [0.004590011170145359], per_fold = [[0.004590011170145359]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 3, …), …), measure = [LPLoss(p = 2)], measurement = [0.0036225306262139788], per_fold = [[0.0036225306262139788]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 1, …), …), measure = [LPLoss(p = 2)], measurement = [0.0031016792824338077], per_fold = [[0.0031016792824338077]]), (model = DeterministicEnsembleModel(model = KNNRegressor(K = 12, …), …), measure = [LPLoss(p = 2)], measurement = [0.007526080413070994], per_fold = [[0.007526080413070994]])],\n best_report = (measures = Any[],\n                oob_measurements = missing,),\n plotting = (parameter_names = [\"bagging_fraction\", \"model.K\"],\n             parameter_scales = [:linear, :linear],\n             parameter_values = Any[0.8888888888888888 18; 0.6111111111111112 16; … ; 0.5555555555555556 1; 0.6111111111111112 12],\n             measurements = [0.006563470407815136, 0.0122246059374603, 0.0036628015206465734, 0.008020213502274467, 0.001787953402180091, 0.0025945479977448303, 0.020351582333467805, 0.009536648688229333, 0.009536648688229333, 0.009536648688229333  …  0.01646454995922476, 0.006456471195446319, 0.003151889979016522, 0.0030046273651692227, 0.004745533719078165, 0.004590011170145359, 0.004590011170145359, 0.0036225306262139788, 0.0031016792824338077, 0.007526080413070994],),)\n\n\n\n\n4.3 plot tunning results\n\nres = r.plotting\ntable=(vals_b = res.parameter_values[:, 1],\n            vals_k = res.parameter_values[:, 2],\n            measurement=res.measurements\n)\n\ndatalayer=data(table)\nmappinglayer=mapping(:vals_b,:vals_k,:measurement,color=:measurement)\nvislayler=visual(Tricontourf,colormap = :batlow)\nax=(width = 400, height = 400)\nplt=datalayer*mappinglayer*vislayler\ndraw(plt,axis=ax)\n\n\n\n\n\n\n\nFigure 1: fig-ensemble-params-tunning\n\n\n\n\n\n\n\n4.4 predict with ensemble model\n\nŷ = predict(tuned_ensemble, rows=test)\n@show rms(ŷ, y[test])\n\nrms(ŷ, y[test]) = 0.05120320246985217\n\n\n0.05120320246985217",
    "crumbs": [
      "Learn",
      "ensemble model"
    ]
  },
  {
    "objectID": "learn/Regression/index.html",
    "href": "learn/Regression/index.html",
    "title": "Regression Model",
    "section": "",
    "text": "using MLJ \nmodels(\"Regression\")\n\n2-element Vector{NamedTuple{(:name, :package_name, :is_supervised, :abstract_type, :deep_properties, :docstring, :fit_data_scitype, :human_name, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :inverse_transform_scitype, :is_pure_julia, :is_wrapper, :iteration_parameter, :load_path, :package_license, :package_url, :package_uuid, :predict_scitype, :prediction_type, :reporting_operations, :reports_feature_importances, :supports_class_weights, :supports_online, :supports_training_losses, :supports_weights, :transform_scitype, :input_scitype, :target_scitype, :output_scitype)}}:\n (name = MultitargetSRRegressor, package_name = SymbolicRegression, ... )\n (name = SRRegressor, package_name = SymbolicRegression, ... )",
    "crumbs": [
      "Regression Intro"
    ]
  },
  {
    "objectID": "learn/Regression/4-salary-linear-regression.html",
    "href": "learn/Regression/4-salary-linear-regression.html",
    "title": "4 Salary dataset Linear Regression",
    "section": "",
    "text": "info\n\n\n\ndataset: Salary Dataset - Simple linear regression",
    "crumbs": [
      "4-salary-dataset-linear-regression"
    ]
  },
  {
    "objectID": "learn/Regression/4-salary-linear-regression.html#load-package",
    "href": "learn/Regression/4-salary-linear-regression.html#load-package",
    "title": "4 Salary dataset Linear Regression",
    "section": "1. load package",
    "text": "1. load package\n\n import MLJ:fit!,match,predict,table,fitted_params\n using CSV,DataFrames,Tidier,Pipe\n using CairoMakie,AlgebraOfGraphics,MakieThemes\n using MLJ,StatsBase",
    "crumbs": [
      "4-salary-dataset-linear-regression"
    ]
  },
  {
    "objectID": "learn/Regression/4-salary-linear-regression.html#load-data",
    "href": "learn/Regression/4-salary-linear-regression.html#load-data",
    "title": "4 Salary dataset Linear Regression",
    "section": "2. load data",
    "text": "2. load data\n\ndf=CSV.File(\"../../data/salary_dataset.csv\")|&gt;DataFrame\nfirst(df,5)\n\n5×3 DataFrame\n\n\n\nRow\nColumn1\nYearsExperience\nSalary\n\n\n\nInt64\nFloat64\nFloat64\n\n\n\n\n1\n0\n1.2\n39344.0\n\n\n2\n1\n1.4\n46206.0\n\n\n3\n2\n1.6\n37732.0\n\n\n4\n3\n2.1\n43526.0\n\n\n5\n4\n2.3\n39892.0",
    "crumbs": [
      "4-salary-dataset-linear-regression"
    ]
  },
  {
    "objectID": "learn/Regression/4-salary-linear-regression.html#data-processing",
    "href": "learn/Regression/4-salary-linear-regression.html#data-processing",
    "title": "4 Salary dataset Linear Regression",
    "section": "3. data processing",
    "text": "3. data processing\n\ndf=@chain df begin\n  @clean_names\n  @select(Not(1))\n  #coerce(_,:years_experience=&gt;Continuous,:salary=&gt;Continuous)\nend\nschema(df)\n\n\n┌──────────────────┬────────────┬─────────┐\n│ names            │ scitypes   │ types   │\n├──────────────────┼────────────┼─────────┤\n│ years_experience │ Continuous │ Float64 │\n│ salary           │ Continuous │ Float64 │\n└──────────────────┴────────────┴─────────┘\n\n\n\n\n\nax=(width=400,height=250)\nplt1=data(df)*mapping(:years_experience,:salary)*visual(Scatter,color=:lightgreen,strokewidth=1,strokecolor=:black)\nfg=draw(plt1,axis=ax)\n\n\n\n\n\n\n\n\n\nX=MLJ.table(reshape(df[:,1],30,1))\ny=Vector(df[:,2])\nshow(y)\n\n[39344.0, 46206.0, 37732.0, 43526.0, 39892.0, 56643.0, 60151.0, 54446.0, 64446.0, 57190.0, 63219.0, 55795.0, 56958.0, 57082.0, 61112.0, 67939.0, 66030.0, 83089.0, 81364.0, 93941.0, 91739.0, 98274.0, 101303.0, 113813.0, 109432.0, 105583.0, 116970.0, 112636.0, 122392.0, 121873.0]",
    "crumbs": [
      "4-salary-dataset-linear-regression"
    ]
  },
  {
    "objectID": "learn/Regression/4-salary-linear-regression.html#mlj-workflow",
    "href": "learn/Regression/4-salary-linear-regression.html#mlj-workflow",
    "title": "4 Salary dataset Linear Regression",
    "section": "4. MLJ workflow",
    "text": "4. MLJ workflow\n\n4.1 load model\n\nLinearRegressor = @load LinearRegressor pkg=MLJLinearModels\nmodel=LinearRegressor()\n\nimport MLJLinearModels ✔\n\n\nLinearRegressor(\n  fit_intercept = true, \n  solver = nothing)\n\n\n\n\n4.2 fitting model\n\nmach = MLJ.fit!(machine(model,X,y))\n\ntrained Machine; caches model-specific representations of data\n  model: LinearRegressor(fit_intercept = true, …)\n  args: \n    1:  Source @172 ⏎ Table{AbstractVector{Continuous}}\n    2:  Source @583 ⏎ AbstractVector{Continuous}\n\n\n\n\n4.3 getting params\n\nfp=MLJ.fitted_params(mach)\na=fp.coefs[1,1][2]\nb=fp.intercept\nf(t)=a*t+b\n\nf (generic function with 1 method)",
    "crumbs": [
      "4-salary-dataset-linear-regression"
    ]
  },
  {
    "objectID": "learn/Regression/4-salary-linear-regression.html#plot-fit-line",
    "href": "learn/Regression/4-salary-linear-regression.html#plot-fit-line",
    "title": "4 Salary dataset Linear Regression",
    "section": "5 plot fit line",
    "text": "5 plot fit line\n\nxspan=range(extrema(df[:,1])...,200)\nys=f.(xspan)\n\ndatalayer2=data((years_experience=xspan,salary=ys))\nmaplayer2=mapping(:years_experience,:salary)\nvislayer2=visual(Lines,color=:red,linewidth=2)\nplt2=datalayer2*maplayer2*vislayer2\ndraw(plt1+plt2,axis=ax)",
    "crumbs": [
      "4-salary-dataset-linear-regression"
    ]
  },
  {
    "objectID": "learn/Regression/2-florida-lakes.html",
    "href": "learn/Regression/2-florida-lakes.html",
    "title": "Florida Lakes",
    "section": "",
    "text": "info\n\n\n\nref : FloridaLakes Dataset\n\n数学的基本用途是用来解决问题. 我们总是尝试利用简单方法解决复杂问题. 在 Florida Lakes 数据集中,测量了湖泊水体的 PH值, 重金属元素的含量. 当我们使用回归模型建立 PH值和重金属含量之间的映射关系以后, 数学的威力就显现出来. 我们可以以非常简单的方法来间接测量湖水的重金属浓度. 这些重金属的浓度如果直接测量,会非常复杂,价格也非常昂贵. 由于有非常好的在线 PH值检测传感器, 我们甚至可以实时检测重金属浓度. 但是必须要注意的是模型并没有完全反映出实际的浓度. 这是近似模型 .\n\n\nThe basic purpose of mathematics is to solve problems. We always try to solve complex problems with simple methods. In the Florida Lakes dataset, the PH value of the lake water body, the content of heavy metal elements, was measured. When we use regression models to establish a mapping between PH and heavy metal content, the power of mathematics becomes apparent. We can indirectly measure the concentration of heavy metals in lake water in a very simple way. The concentration of these heavy metals can be very complex and expensive if measured directly. Since there are very good online PH detection sensors, we can even detect heavy metal concentrations in real time. However, it is important to note that the model does not fully reflect the actual concentrations. This is an approximate model .",
    "crumbs": [
      "2.FloridaLakes"
    ]
  },
  {
    "objectID": "learn/Regression/2-florida-lakes.html#load-package",
    "href": "learn/Regression/2-florida-lakes.html#load-package",
    "title": "Florida Lakes",
    "section": "1. load package",
    "text": "1. load package\n\n #| eval: false\n using CSV,DataFrames,Tidier,Pipe\n using CairoMakie,AlgebraOfGraphics,MakieThemes\n using StatsBase\n #Makie.set_theme!(ggthemr(:flat))",
    "crumbs": [
      "2.FloridaLakes"
    ]
  },
  {
    "objectID": "learn/Regression/2-florida-lakes.html#load-csv",
    "href": "learn/Regression/2-florida-lakes.html#load-csv",
    "title": "Florida Lakes",
    "section": "2. load csv",
    "text": "2. load csv\n\ndf=CSV.File(\"../../data/FloridaLakes.csv\")|&gt;DataFrame\ndf=@chain  df  begin\n    @clean_names\nend\nfirst(df,5)\n\n5×12 DataFrame\n\n\n\nRow\ni_d\nlake\nalkalinity\np_h\ncalcium\nchlorophyll\navg_mercury\nnum_samples\nmin_mercury\nmax_mercury\nthree_yr_std_mercury\nage_data\n\n\n\nInt64\nString31\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nInt64\nFloat64\nFloat64\nFloat64\nInt64\n\n\n\n\n1\n1\nAlligator\n5.9\n6.1\n3.0\n0.7\n1.23\n5\n0.85\n1.43\n1.53\n1\n\n\n2\n2\nAnnie\n3.5\n5.1\n1.9\n3.2\n1.33\n7\n0.92\n1.9\n1.33\n0\n\n\n3\n3\nApopka\n116.0\n9.1\n44.1\n128.3\n0.04\n6\n0.04\n0.06\n0.04\n0\n\n\n4\n4\nBlue Cypress\n39.4\n6.9\n16.4\n3.5\n0.44\n12\n0.13\n0.84\n0.44\n0\n\n\n5\n5\nBrick\n2.5\n4.6\n2.9\n1.8\n1.2\n12\n0.69\n1.5\n1.33\n1",
    "crumbs": [
      "2.FloridaLakes"
    ]
  },
  {
    "objectID": "learn/Regression/2-florida-lakes.html#describe-df",
    "href": "learn/Regression/2-florida-lakes.html#describe-df",
    "title": "Florida Lakes",
    "section": "3. describe df",
    "text": "3. describe df\n\ndescribe(df)\n\n12×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nDataType\n\n\n\n\n1\ni_d\n27.0\n1\n27.0\n53\n0\nInt64\n\n\n2\nlake\n\nAlligator\n\nYale\n0\nString31\n\n\n3\nalkalinity\n37.5302\n1.2\n19.6\n128.0\n0\nFloat64\n\n\n4\np_h\n6.59057\n3.6\n6.8\n9.1\n0\nFloat64\n\n\n5\ncalcium\n22.2019\n1.1\n12.6\n90.7\n0\nFloat64\n\n\n6\nchlorophyll\n23.117\n0.7\n12.8\n152.4\n0\nFloat64\n\n\n7\navg_mercury\n0.52717\n0.04\n0.48\n1.33\n0\nFloat64\n\n\n8\nnum_samples\n13.0566\n4\n12.0\n44\n0\nInt64\n\n\n9\nmin_mercury\n0.279811\n0.04\n0.25\n0.92\n0\nFloat64\n\n\n10\nmax_mercury\n0.874528\n0.06\n0.84\n2.04\n0\nFloat64\n\n\n11\nthree_yr_std_mercury\n0.513208\n0.04\n0.45\n1.53\n0\nFloat64\n\n\n12\nage_data\n0.811321\n0\n1.0\n1\n0\nInt64",
    "crumbs": [
      "2.FloridaLakes"
    ]
  },
  {
    "objectID": "learn/LearningNetwork/index.html",
    "href": "learn/LearningNetwork/index.html",
    "title": "Learning Network",
    "section": "",
    "text": "Learning Network",
    "crumbs": [
      "Learning Network  Intro"
    ]
  },
  {
    "objectID": "learn/Ensemble/index.html",
    "href": "learn/Ensemble/index.html",
    "title": "Ensemble model",
    "section": "",
    "text": "using HypertextLiteral\nusing MLJ \nLinearRegressor = @load LinearRegressor pkg=MLJLinearModels\nstr=@doc(LinearRegressor);\n\nimport MLJLinearModels ✔\n\n\n[ Info: For silent loading, specify `verbosity=0`.\nGaussian Mixture Model Regression doc  \n  \n \n GMR\n \n \n  \n  LinearRegressor\nA model type for constructing a linear regressor, based on MLJLinearModels.jl, and implementing the MLJ model interface.\nFrom MLJ, the type can be imported using\nLinearRegressor = @load LinearRegressor pkg=MLJLinearModels\nDo model = LinearRegressor() to construct an instance with default hyper-parameters.\nThis model provides standard linear regression with objective function\n$|Xθ - y|₂²/2$\nDifferent solver options exist, as indicated under \"Hyperparameters\" below. \nTraining data\nIn MLJ or MLJBase, bind an instance model to data with\nmach = machine(model, X, y)\nwhere:\n\nX is any table of input features (eg, a DataFrame) whose columns have Continuous scitype; check column scitypes with schema(X)\n\ny is the target, which can be any AbstractVector whose element scitype is Continuous; check the scitype with scitype(y)\n\n\nTrain the machine using fit!(mach, rows=...).\nHyperparameters\n\nfit_intercept::Bool: whether to fit the intercept or not. Default: true\n\nsolver::Union{Nothing, MLJLinearModels.Solver}: \"any instance of MLJLinearModels.Analytical. Use Analytical() for Cholesky and CG()=Analytical(iterative=true) for conjugate-gradient.\nIf solver = nothing (default) then Analytical() is used.  Default: nothing\n\n\nExample\nusing MLJ\nX, y = make_regression()\nmach = fit!(machine(LinearRegressor(), X, y))\npredict(mach, X)\nfitted_params(mach)",
    "crumbs": [
      "Ensemble  Intro"
    ]
  },
  {
    "objectID": "learn/Ensemble/index.html#tailwind-and-daisyui",
    "href": "learn/Ensemble/index.html#tailwind-and-daisyui",
    "title": "Ensemble model",
    "section": "tailwind and daisyui",
    "text": "tailwind and daisyui\n\n\n  \n  \n\n\n\n\nneutral \nprimary \nsecondary \naccent \nghost\n\n\n\n\n  在这里拷贝样式模板 \n  组件样式在这里有介绍",
    "crumbs": [
      "Ensemble  Intro"
    ]
  },
  {
    "objectID": "learn/Dimension/3-iris-pca.html",
    "href": "learn/Dimension/3-iris-pca.html",
    "title": "3 IRIS PCA",
    "section": "",
    "text": "using CSV,DataFrames,Tidier,Pipe\n using CairoMakie,AlgebraOfGraphics,MakieThemes\n using MLJ,MLJModelInterface,Random\n import MLJ:transform,predict\n #Makie.set_theme!(ggthemr(:flat))",
    "crumbs": [
      "3.IRIS  PCA "
    ]
  },
  {
    "objectID": "learn/Dimension/3-iris-pca.html#load-package",
    "href": "learn/Dimension/3-iris-pca.html#load-package",
    "title": "3 IRIS PCA",
    "section": "",
    "text": "using CSV,DataFrames,Tidier,Pipe\n using CairoMakie,AlgebraOfGraphics,MakieThemes\n using MLJ,MLJModelInterface,Random\n import MLJ:transform,predict\n #Makie.set_theme!(ggthemr(:flat))",
    "crumbs": [
      "3.IRIS  PCA "
    ]
  },
  {
    "objectID": "learn/Dimension/3-iris-pca.html#load-csv",
    "href": "learn/Dimension/3-iris-pca.html#load-csv",
    "title": "3 IRIS PCA",
    "section": "2. load csv",
    "text": "2. load csv\n\ndf=CSV.File(\"../../data/iris.csv\")|&gt;DataFrame\ndf=@chain  df  begin\n    @clean_names\nend\nfirst(df,5)\n\n5×5 DataFrame\n\n\n\nRow\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\nFloat64\nFloat64\nFloat64\nFloat64\nString15\n\n\n\n\n1\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n2\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n3\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5\n5.0\n3.6\n1.4\n0.2\nsetosa",
    "crumbs": [
      "3.IRIS  PCA "
    ]
  },
  {
    "objectID": "learn/Dimension/3-iris-pca.html#describe-df",
    "href": "learn/Dimension/3-iris-pca.html#describe-df",
    "title": "3 IRIS PCA",
    "section": "3. describe df",
    "text": "3. describe df\n\ndescribe(df)\n\n5×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nDataType\n\n\n\n\n1\nsepal_length\n5.84333\n4.3\n5.8\n7.9\n0\nFloat64\n\n\n2\nsepal_width\n3.054\n2.0\n3.0\n4.4\n0\nFloat64\n\n\n3\npetal_length\n3.75867\n1.0\n4.35\n6.9\n0\nFloat64\n\n\n4\npetal_width\n1.19867\n0.1\n1.3\n2.5\n0\nFloat64\n\n\n5\nspecies\n\nsetosa\n\nvirginica\n0\nString15",
    "crumbs": [
      "3.IRIS  PCA "
    ]
  },
  {
    "objectID": "learn/Dimension/3-iris-pca.html#data-processing",
    "href": "learn/Dimension/3-iris-pca.html#data-processing",
    "title": "3 IRIS PCA",
    "section": "4. data processing",
    "text": "4. data processing\n\n    coerce!(df,:labels=&gt;Multiclass)\n    ytrain, Xtrain=  unpack(df, ==(:species), rng=123)\n    cat=levels(ytrain)\n    rows,cols=size(Xtrain)\n\n(150, 4)",
    "crumbs": [
      "3.IRIS  PCA "
    ]
  },
  {
    "objectID": "learn/Dimension/3-iris-pca.html#pca-workflow",
    "href": "learn/Dimension/3-iris-pca.html#pca-workflow",
    "title": "3 IRIS PCA",
    "section": "5. pca workflow",
    "text": "5. pca workflow\n\nPCA = @load PCA pkg=MultivariateStats\nmaxdim=6\nmodel=PCA(maxoutdim=maxdim)\nmach = machine(model, Xtrain) |&gt; fit!\nYtr =transform(mach, Xtrain)\nfirst(Ytr,10)\n\nimport MLJMultivariateStatsInterface ✔\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: Training machine(PCA(maxoutdim = 6, …), …).\n\n\n10×3 DataFrame\n\n\n\nRow\nx1\nx2\nx3\n\n\n\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n-2.27585\n0.333387\n-0.284678\n\n\n2\n-0.297808\n-0.347017\n-0.0121791\n\n\n3\n-2.38756\n0.462519\n0.452024\n\n\n4\n2.88796\n-0.570798\n-0.0273354\n\n\n5\n-0.0432464\n-0.581489\n0.232964\n\n\n6\n-1.76405\n0.0785192\n-0.130784\n\n\n7\n2.98184\n-0.48025\n-0.0797248\n\n\n8\n-0.984045\n-0.12436\n0.0621574\n\n\n9\n2.2799\n0.747783\n-0.174326\n\n\n10\n-0.519383\n-1.19135\n-0.546686",
    "crumbs": [
      "3.IRIS  PCA "
    ]
  },
  {
    "objectID": "learn/Dimension/3-iris-pca.html#report-mach",
    "href": "learn/Dimension/3-iris-pca.html#report-mach",
    "title": "3 IRIS PCA",
    "section": "6. report mach",
    "text": "6. report mach\n\nreport(mach)\n\n(indim = 4,\n outdim = 3,\n tprincipalvar = 4.545608248041781,\n tresidualvar = 0.02368302712600201,\n tvar = 4.569291275167783,\n mean = [5.843333333333334, 3.0540000000000003, 3.758666666666666, 1.1986666666666668],\n principalvars = [4.224840768320111, 0.24224357162751553, 0.07852390809415473],)\n\n\n\n7.1 2 pcs\n\n  ax=(width=400, height=300)\n  table=(pc1=Ytr.x1,pc2=Ytr.x2,cat=ytrain)\n  datalayer=data(table)\n  maplayer=mapping(:pc1,:pc2,color=:cat)\n  vislayer=visual(Scatter,markersize=10,strokewidth=1,strokecolor=:black)\n  plt=datalayer*maplayer*vislayer*visual(alpha = 0.5)\n  draw(plt,axis=ax)\n\n\n\n\n\n\n\nFigure 1: iris-pca-2pcs\n\n\n\n\n\n\n\n7.2 3 pcs\n\nlet  \nax = (type = Axis3, width = 400, height = 300,azimuth =-0.1pi,elevation=0.1pi)\ntable=(pc1=Ytr.x1,pc2=Ytr.x2,pc3=Ytr.x3,cat=ytrain)\ndatalayer=data(table)\nmaplayer=mapping(:pc1,:pc2,:pc3,color=:cat)\nvislayer=visual(Scatter,markersize=10,strokewidth=1,strokecolor=:black)\nplt=datalayer*maplayer*vislayer*visual(alpha = 0.5)\ndraw(plt,axis=ax)\nend\n\n\n\n\n\n\n\nFigure 2: iris-pca-3pcs",
    "crumbs": [
      "3.IRIS  PCA "
    ]
  },
  {
    "objectID": "learn/Clustering/index.html",
    "href": "learn/Clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "Clustering",
    "crumbs": [
      "Clustering Intro"
    ]
  },
  {
    "objectID": "learn/Clustering/1-iris-pca-kmeans.html",
    "href": "learn/Clustering/1-iris-pca-kmeans.html",
    "title": "1 IRIS PCA-Kmeans clustering",
    "section": "",
    "text": "using CSV,DataFrames,Tidier,Pipe\n using CairoMakie,AlgebraOfGraphics,MakieThemes\n using MLJ,MLJModelInterface,Random\n import MLJ:transform,predict\n #Makie.set_theme!(ggthemr(:flat))",
    "crumbs": [
      "1-IRIS PCA-Kmeans clustering"
    ]
  },
  {
    "objectID": "learn/Clustering/1-iris-pca-kmeans.html#load-package",
    "href": "learn/Clustering/1-iris-pca-kmeans.html#load-package",
    "title": "1 IRIS PCA-Kmeans clustering",
    "section": "",
    "text": "using CSV,DataFrames,Tidier,Pipe\n using CairoMakie,AlgebraOfGraphics,MakieThemes\n using MLJ,MLJModelInterface,Random\n import MLJ:transform,predict\n #Makie.set_theme!(ggthemr(:flat))",
    "crumbs": [
      "1-IRIS PCA-Kmeans clustering"
    ]
  },
  {
    "objectID": "learn/Clustering/1-iris-pca-kmeans.html#load-csv",
    "href": "learn/Clustering/1-iris-pca-kmeans.html#load-csv",
    "title": "1 IRIS PCA-Kmeans clustering",
    "section": "2. load csv",
    "text": "2. load csv\n\ndf=CSV.File(\"../../data/iris.csv\")|&gt;DataFrame\ndf=@chain  df  begin\n    @clean_names\nend\nfirst(df,5)\n\n5×5 DataFrame\n\n\n\nRow\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\nFloat64\nFloat64\nFloat64\nFloat64\nString15\n\n\n\n\n1\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n2\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n3\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5\n5.0\n3.6\n1.4\n0.2\nsetosa",
    "crumbs": [
      "1-IRIS PCA-Kmeans clustering"
    ]
  },
  {
    "objectID": "learn/Clustering/1-iris-pca-kmeans.html#describe-df",
    "href": "learn/Clustering/1-iris-pca-kmeans.html#describe-df",
    "title": "1 IRIS PCA-Kmeans clustering",
    "section": "3. describe df",
    "text": "3. describe df\n\ndescribe(df)\n\n5×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nDataType\n\n\n\n\n1\nsepal_length\n5.84333\n4.3\n5.8\n7.9\n0\nFloat64\n\n\n2\nsepal_width\n3.054\n2.0\n3.0\n4.4\n0\nFloat64\n\n\n3\npetal_length\n3.75867\n1.0\n4.35\n6.9\n0\nFloat64\n\n\n4\npetal_width\n1.19867\n0.1\n1.3\n2.5\n0\nFloat64\n\n\n5\nspecies\n\nsetosa\n\nvirginica\n0\nString15",
    "crumbs": [
      "1-IRIS PCA-Kmeans clustering"
    ]
  },
  {
    "objectID": "learn/Clustering/1-iris-pca-kmeans.html#data-processing",
    "href": "learn/Clustering/1-iris-pca-kmeans.html#data-processing",
    "title": "1 IRIS PCA-Kmeans clustering",
    "section": "4. data processing",
    "text": "4. data processing\n\n    coerce!(df,:labels=&gt;Multiclass)\n    ytrain, Xtrain=  unpack(df, ==(:species), rng=123)\n    cat=levels(ytrain)\n    rows,cols=size(Xtrain)\n\n(150, 4)",
    "crumbs": [
      "1-IRIS PCA-Kmeans clustering"
    ]
  },
  {
    "objectID": "learn/Clustering/1-iris-pca-kmeans.html#mlj-workflow",
    "href": "learn/Clustering/1-iris-pca-kmeans.html#mlj-workflow",
    "title": "1 IRIS PCA-Kmeans clustering",
    "section": "5. MLJ workflow",
    "text": "5. MLJ workflow\n\n5.1 load model\n\nPCA doc \n\n\n\n\nPCA\n\n\n\n\n\n\n@doc(PCA)\n\nPCA\nA model type for constructing a pca, based on unknown.jl, and implementing the MLJ model interface.\nFrom MLJ, the type can be imported using\nPCA = @load PCA pkg=unknown\nDo model = PCA() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in PCA(maxoutdim=...).\nPrincipal component analysis learns a linear projection onto a lower dimensional space while preserving most of the initial variance seen in the training data.\n\nTraining data\nIn MLJ or MLJBase, bind an instance model to data with\nmach = machine(model, X)\nHere:\n\nX is any table of input features (eg, a DataFrame) whose columns are of scitype Continuous; check column scitypes with schema(X).\n\nTrain the machine using fit!(mach, rows=...).\n\n\nHyper-parameters\n\nmaxoutdim=0: Together with variance_ratio, controls the output dimension outdim chosen by the model. Specifically, suppose that k is the smallest integer such that retaining the k most significant principal components accounts for variance_ratio of the total variance in the training data. Then outdim = min(outdim, maxoutdim). If maxoutdim=0 (default) then the effective maxoutdim is min(n, indim - 1) where n is the number of observations and indim the number of features in the training data.\nvariance_ratio::Float64=0.99: The ratio of variance preserved after the transformation\nmethod=:auto: The method to use to solve the problem. Choices are\n\n:svd: Support Vector Decomposition of the matrix.\n:cov: Covariance matrix decomposition.\n:auto: Use :cov if the matrices first dimension is smaller than its second dimension and otherwise use :svd\n\nmean=nothing: if nothing, centering will be computed and applied, if set to 0 no centering (data is assumed pre-centered); if a vector is passed, the centering is done with that vector.\n\n\n\nOperations\n\ntransform(mach, Xnew): Return a lower dimensional projection of the input Xnew, which should have the same scitype as X above.\ninverse_transform(mach, Xsmall): For a dimension-reduced table Xsmall, such as returned by transform, reconstruct a table, having same the number of columns as the original training data X, that transforms to Xsmall. Mathematically, inverse_transform is a right-inverse for the PCA projection map, whose image is orthogonal to the kernel of that map. In particular, if Xsmall = transform(mach, Xnew), then inverse_transform(Xsmall) is only an approximation to Xnew.\n\n\n\nFitted parameters\nThe fields of fitted_params(mach) are:\n\nprojection: Returns the projection matrix, which has size (indim, outdim), where indim and outdim are the number of features of the input and output respectively.\n\n\n\nReport\nThe fields of report(mach) are:\n\nindim: Dimension (number of columns) of the training data and new data to be transformed.\noutdim = min(n, indim, maxoutdim) is the output dimension; here n is the number of observations.\ntprincipalvar: Total variance of the principal components.\ntresidualvar: Total residual variance.\ntvar: Total observation variance (principal + residual variance).\nmean: The mean of the untransformed training data, of length indim.\nprincipalvars: The variance of the principal components.\n\n\n\nExamples\nusing MLJ\n\nPCA = @load PCA pkg=MultivariateStats\n\nX, y = @load_iris # a table and a vector\n\nmodel = PCA(maxoutdim=2)\nmach = machine(model, X) |&gt; fit!\n\nXproj = transform(mach, X)\nSee also KernelPCA, ICA, FactorAnalysis, PPCA\n\n\n\n\n\n\nPCA = @load PCA pkg=MultivariateStats\nKMeans = @load KMeans pkg=Clustering\n\nimport MLJMultivariateStatsInterface ✔\nimport MLJClusteringInterface ✔\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n\n\nMLJClusteringInterface.KMeans\n\n\n\n\n5.2 fitting model\n\nmodel1 : pca model\nmodle2 : kmeans model\n\n\nmodel1 = PCA(maxoutdim=2)\nmach1 = machine(model1, Xtrain) |&gt; fit!\nmodel2 = KMeans(k=3)\nXproj = MLJ.transform(mach1, Xtrain)\nmach2 = machine(model2, Xproj) |&gt; fit!\nyhat = MLJ.predict(mach2, Xproj)\nshow(yhat)\n\nCategoricalArrays.CategoricalValue{Int64, UInt32}[3, 2, 3, 1, 2, 3, 1, 2, 1, 2, 2, 1, 1, 3, 3, 3, 3, 1, 2, 2, 3, 2, 2, 2, 1, 1, 2, 1, 1, 1, 2, 2, 1, 1, 1, 2, 3, 1, 1, 1, 2, 2, 3, 3, 2, 2, 2, 1, 1, 1, 3, 2, 3, 2, 3, 2, 1, 3, 2, 3, 3, 2, 1, 1, 3, 2, 1, 2, 2, 3, 3, 2, 2, 2, 1, 1, 3, 2, 1, 1, 3, 2, 1, 1, 3, 2, 2, 3, 2, 3, 2, 2, 3, 1, 1, 2, 2, 2, 2, 1, 2, 2, 1, 2, 1, 2, 3, 1, 3, 3, 1, 2, 2, 3, 2, 2, 2, 2, 2, 3, 1, 1, 1, 3, 2, 1, 2, 3, 2, 1, 1, 2, 1, 1, 2, 1, 3, 2, 2, 1, 3, 3, 3, 2, 3, 1, 3, 2, 1, 1]\n\n\n[ Info: Training machine(PCA(maxoutdim = 2, …), …).\n[ Info: Training machine(KMeans(k = 3, …), …).",
    "crumbs": [
      "1-IRIS PCA-Kmeans clustering"
    ]
  },
  {
    "objectID": "learn/Clustering/1-iris-pca-kmeans.html#plot-resutls",
    "href": "learn/Clustering/1-iris-pca-kmeans.html#plot-resutls",
    "title": "1 IRIS PCA-Kmeans clustering",
    "section": "6. plot resutls",
    "text": "6. plot resutls\n\n  ax=(width=400, height=300,title=\"IRIS PCA-Kmeans plot\",subtitle=\"(2pcs)\")\n  table=(pc1=Xproj.x1,pc2=Xproj.x2,cat=yhat)\n  datalayer=data(table)\n  maplayer=mapping(:pc1,:pc2,color=:cat)\n  vislayer=visual(Scatter,markersize=10,strokewidth=1,strokecolor=:black)\n  plt=datalayer*maplayer*vislayer\n  draw(plt,axis=ax)\n\n\n\n\n\n\n\nFigure 1: iris-pca-kmeans-2pcs",
    "crumbs": [
      "1-IRIS PCA-Kmeans clustering"
    ]
  },
  {
    "objectID": "learn/Classfication/2-mnist-classification.html",
    "href": "learn/Classfication/2-mnist-classification.html",
    "title": "2-Mnist Classification",
    "section": "",
    "text": "using CSV,DataFrames,Tidier,Pipe\n using CairoMakie,AlgebraOfGraphics,MakieThemes\n using MLJ,MLJModelInterface,Random\n import MLJ:transform,predict\n #Makie.set_theme!(ggthemr(:flat))",
    "crumbs": [
      "2-mnist-classification.qmd"
    ]
  },
  {
    "objectID": "learn/Classfication/2-mnist-classification.html#load-package",
    "href": "learn/Classfication/2-mnist-classification.html#load-package",
    "title": "2-Mnist Classification",
    "section": "",
    "text": "using CSV,DataFrames,Tidier,Pipe\n using CairoMakie,AlgebraOfGraphics,MakieThemes\n using MLJ,MLJModelInterface,Random\n import MLJ:transform,predict\n #Makie.set_theme!(ggthemr(:flat))",
    "crumbs": [
      "2-mnist-classification.qmd"
    ]
  },
  {
    "objectID": "learn/Classfication/2-mnist-classification.html#load-csv",
    "href": "learn/Classfication/2-mnist-classification.html#load-csv",
    "title": "2-Mnist Classification",
    "section": "2. load csv",
    "text": "2. load csv\n\ndf=CSV.File(\"../../data/mnist_train.csv\")|&gt;DataFrame\ndf=coerce(df,:label=&gt;Multiclass)\nfirst(df,5)\n\n5×785 DataFrame685 columns omitted\n\n\n\nRow\nlabel\n1x1\n1x2\n1x3\n1x4\n1x5\n1x6\n1x7\n1x8\n1x9\n1x10\n1x11\n1x12\n1x13\n1x14\n1x15\n1x16\n1x17\n1x18\n1x19\n1x20\n1x21\n1x22\n1x23\n1x24\n1x25\n1x26\n1x27\n1x28\n2x1\n2x2\n2x3\n2x4\n2x5\n2x6\n2x7\n2x8\n2x9\n2x10\n2x11\n2x12\n2x13\n2x14\n2x15\n2x16\n2x17\n2x18\n2x19\n2x20\n2x21\n2x22\n2x23\n2x24\n2x25\n2x26\n2x27\n2x28\n3x1\n3x2\n3x3\n3x4\n3x5\n3x6\n3x7\n3x8\n3x9\n3x10\n3x11\n3x12\n3x13\n3x14\n3x15\n3x16\n3x17\n3x18\n3x19\n3x20\n3x21\n3x22\n3x23\n3x24\n3x25\n3x26\n3x27\n3x28\n4x1\n4x2\n4x3\n4x4\n4x5\n4x6\n4x7\n4x8\n4x9\n4x10\n4x11\n4x12\n4x13\n4x14\n4x15\n⋯\n\n\n\nCat…\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\n⋯\n\n\n\n\n1\n5\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n⋯\n\n\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n⋯\n\n\n3\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n⋯\n\n\n4\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n⋯\n\n\n5\n9\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n⋯",
    "crumbs": [
      "2-mnist-classification.qmd"
    ]
  },
  {
    "objectID": "learn/Classfication/2-mnist-classification.html#describe-df",
    "href": "learn/Classfication/2-mnist-classification.html#describe-df",
    "title": "2-Mnist Classification",
    "section": "3. describe df",
    "text": "3. describe df\n\ndescribe(df)\n\n785×7 DataFrame760 rows omitted\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nDataType\n\n\n\n\n1\nlabel\n\n0\n\n9\n0\nCategoricalValue{Int64, UInt32}\n\n\n2\n1x1\n0.0\n0\n0.0\n0\n0\nInt64\n\n\n3\n1x2\n0.0\n0\n0.0\n0\n0\nInt64\n\n\n4\n1x3\n0.0\n0\n0.0\n0\n0\nInt64\n\n\n5\n1x4\n0.0\n0\n0.0\n0\n0\nInt64\n\n\n6\n1x5\n0.0\n0\n0.0\n0\n0\nInt64\n\n\n7\n1x6\n0.0\n0\n0.0\n0\n0\nInt64\n\n\n8\n1x7\n0.0\n0\n0.0\n0\n0\nInt64\n\n\n9\n1x8\n0.0\n0\n0.0\n0\n0\nInt64\n\n\n10\n1x9\n0.0\n0\n0.0\n0\n0\nInt64\n\n\n11\n1x10\n0.0\n0\n0.0\n0\n0\nInt64\n\n\n12\n1x11\n0.0\n0\n0.0\n0\n0\nInt64\n\n\n13\n1x12\n0.0\n0\n0.0\n0\n0\nInt64\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n774\n28x17\n0.482733\n0\n0.0\n255\n0\nInt64\n\n\n775\n28x18\n0.343517\n0\n0.0\n255\n0\nInt64\n\n\n776\n28x19\n0.200433\n0\n0.0\n254\n0\nInt64\n\n\n777\n28x20\n0.0888667\n0\n0.0\n254\n0\nInt64\n\n\n778\n28x21\n0.0456333\n0\n0.0\n253\n0\nInt64\n\n\n779\n28x22\n0.0192833\n0\n0.0\n253\n0\nInt64\n\n\n780\n28x23\n0.0151167\n0\n0.0\n254\n0\nInt64\n\n\n781\n28x24\n0.002\n0\n0.0\n62\n0\nInt64\n\n\n782\n28x25\n0.0\n0\n0.0\n0\n0\nInt64\n\n\n783\n28x26\n0.0\n0\n0.0\n0\n0\nInt64\n\n\n784\n28x27\n0.0\n0\n0.0\n0\n0\nInt64\n\n\n785\n28x28\n0.0\n0\n0.0\n0\n0\nInt64",
    "crumbs": [
      "2-mnist-classification.qmd"
    ]
  },
  {
    "objectID": "learn/Classfication/2-mnist-classification.html#split-data",
    "href": "learn/Classfication/2-mnist-classification.html#split-data",
    "title": "2-Mnist Classification",
    "section": "4. split data",
    "text": "4. split data\n\ny, X= unpack(df, ==(:label), rng=123);\ncat=y|&gt;levels\n(Xtrain, Xtest), (ytrain, ytest)  = partition((X, y), 0.2, multi=true,shuffle=true)\n\n\n((12000×784 DataFrame\n   Row │ 1x1    1x2    1x3    1x4    1x5    1x6    1x7    1x8    1x9    1x10   ⋯\n       │ Int64  Int64  Int64  Int64  Int64  Int64  Int64  Int64  Int64  Int64  ⋯\n───────┼────────────────────────────────────────────────────────────────────────\n     1 │     0      0      0      0      0      0      0      0      0      0  ⋯\n     2 │     0      0      0      0      0      0      0      0      0      0\n     3 │     0      0      0      0      0      0      0      0      0      0\n     4 │     0      0      0      0      0      0      0      0      0      0\n     5 │     0      0      0      0      0      0      0      0      0      0  ⋯\n     6 │     0      0      0      0      0      0      0      0      0      0\n     7 │     0      0      0      0      0      0      0      0      0      0\n     8 │     0      0      0      0      0      0      0      0      0      0\n     9 │     0      0      0      0      0      0      0      0      0      0  ⋯\n    10 │     0      0      0      0      0      0      0      0      0      0\n    11 │     0      0      0      0      0      0      0      0      0      0\n   ⋮   │   ⋮      ⋮      ⋮      ⋮      ⋮      ⋮      ⋮      ⋮      ⋮      ⋮    ⋱\n 11991 │     0      0      0      0      0      0      0      0      0      0\n 11992 │     0      0      0      0      0      0      0      0      0      0  ⋯\n 11993 │     0      0      0      0      0      0      0      0      0      0\n 11994 │     0      0      0      0      0      0      0      0      0      0\n 11995 │     0      0      0      0      0      0      0      0      0      0\n 11996 │     0      0      0      0      0      0      0      0      0      0  ⋯\n 11997 │     0      0      0      0      0      0      0      0      0      0\n 11998 │     0      0      0      0      0      0      0      0      0      0\n 11999 │     0      0      0      0      0      0      0      0      0      0\n 12000 │     0      0      0      0      0      0      0      0      0      0  ⋯\n                                              774 columns and 11979 rows omitted, 48000×784 DataFrame\n   Row │ 1x1    1x2    1x3    1x4    1x5    1x6    1x7    1x8    1x9    1x10   ⋯\n       │ Int64  Int64  Int64  Int64  Int64  Int64  Int64  Int64  Int64  Int64  ⋯\n───────┼────────────────────────────────────────────────────────────────────────\n     1 │     0      0      0      0      0      0      0      0      0      0  ⋯\n     2 │     0      0      0      0      0      0      0      0      0      0\n     3 │     0      0      0      0      0      0      0      0      0      0\n     4 │     0      0      0      0      0      0      0      0      0      0\n     5 │     0      0      0      0      0      0      0      0      0      0  ⋯\n     6 │     0      0      0      0      0      0      0      0      0      0\n     7 │     0      0      0      0      0      0      0      0      0      0\n     8 │     0      0      0      0      0      0      0      0      0      0\n     9 │     0      0      0      0      0      0      0      0      0      0  ⋯\n    10 │     0      0      0      0      0      0      0      0      0      0\n    11 │     0      0      0      0      0      0      0      0      0      0\n   ⋮   │   ⋮      ⋮      ⋮      ⋮      ⋮      ⋮      ⋮      ⋮      ⋮      ⋮    ⋱\n 47991 │     0      0      0      0      0      0      0      0      0      0\n 47992 │     0      0      0      0      0      0      0      0      0      0  ⋯\n 47993 │     0      0      0      0      0      0      0      0      0      0\n 47994 │     0      0      0      0      0      0      0      0      0      0\n 47995 │     0      0      0      0      0      0      0      0      0      0\n 47996 │     0      0      0      0      0      0      0      0      0      0  ⋯\n 47997 │     0      0      0      0      0      0      0      0      0      0\n 47998 │     0      0      0      0      0      0      0      0      0      0\n 47999 │     0      0      0      0      0      0      0      0      0      0\n 48000 │     0      0      0      0      0      0      0      0      0      0  ⋯\n                                              774 columns and 47979 rows omitted), (CategoricalArrays.CategoricalValue{Int64, UInt32}[1, 3, 9, 0, 8, 1, 6, 7, 1, 3  …  5, 2, 5, 4, 4, 2, 3, 3, 3, 1], CategoricalArrays.CategoricalValue{Int64, UInt32}[7, 4, 1, 0, 3, 8, 2, 2, 3, 8  …  4, 0, 3, 8, 8, 1, 5, 2, 7, 1]))",
    "crumbs": [
      "2-mnist-classification.qmd"
    ]
  },
  {
    "objectID": "learn/Classfication/2-mnist-classification.html#pca-workflow",
    "href": "learn/Classfication/2-mnist-classification.html#pca-workflow",
    "title": "2-Mnist Classification",
    "section": "5. pca workflow",
    "text": "5. pca workflow\n\n5.1 load model\n\nPCA = @load PCA pkg=MultivariateStats;\nmodel=PCA(maxoutdim=3);\n\nimport MLJMultivariateStatsInterface ✔\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n\n\n\n\n  \n     PCA doc \n     \n     \n \n \n doc\n \n \n \n PCA\nA model type for constructing a pca, based on unknown.jl, and implementing the MLJ model interface.\nFrom MLJ, the type can be imported using\nPCA = @load PCA pkg=unknown\nDo model = PCA() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in PCA(maxoutdim=...).\nPrincipal component analysis learns a linear projection onto a lower dimensional space while preserving most of the initial variance seen in the training data.\nTraining data\nIn MLJ or MLJBase, bind an instance model to data with\nmach = machine(model, X)\nHere:\n\nX is any table of input features (eg, a DataFrame) whose columns are of scitype Continuous; check column scitypes with schema(X).\n\n\nTrain the machine using fit!(mach, rows=...).\nHyper-parameters\n\nmaxoutdim=0: Together with variance_ratio, controls the output dimension outdim  chosen by the model. Specifically, suppose that k is the smallest integer such that  retaining the k most significant principal components accounts for variance_ratio of  the total variance in the training data. Then outdim = min(outdim, maxoutdim). If  maxoutdim=0 (default) then the effective maxoutdim is min(n, indim - 1) where n  is the number of observations and indim the number of features in the training data.\n\nvariance_ratio::Float64=0.99: The ratio of variance preserved after the transformation\n\nmethod=:auto: The method to use to solve the problem. Choices are\n\n:svd: Support Vector Decomposition of the matrix.\n\n:cov: Covariance matrix decomposition.\n\n:auto: Use :cov if the matrices first dimension is smaller than its second dimension and otherwise use :svd\n\n\n\nmean=nothing: if nothing, centering will be computed and applied, if set to 0 no centering (data is assumed pre-centered); if a vector is passed, the centering is done with that vector.\n\n\nOperations\n\ntransform(mach, Xnew): Return a lower dimensional projection of the input Xnew, which should have the same scitype as X above.\n\ninverse_transform(mach, Xsmall): For a dimension-reduced table Xsmall, such as returned by transform, reconstruct a table, having same the number of columns as the original training data X, that transforms to Xsmall. Mathematically, inverse_transform is a right-inverse for the PCA projection map, whose image is orthogonal to the kernel of that map. In particular, if Xsmall = transform(mach, Xnew), then inverse_transform(Xsmall) is only an approximation to Xnew.\n\n\nFitted parameters\nThe fields of fitted_params(mach) are:\n\nprojection: Returns the projection matrix, which has size (indim, outdim), where indim and outdim are the number of features of the input and output respectively.\n\n\nReport\nThe fields of report(mach) are:\n\nindim: Dimension (number of columns) of the training data and new data to be transformed.\n\noutdim = min(n, indim, maxoutdim) is the output dimension; here n is the number of observations.\n\ntprincipalvar: Total variance of the principal components.\n\ntresidualvar: Total residual variance.\n\ntvar: Total observation variance (principal + residual variance).\n\nmean: The mean of the untransformed training data, of length indim.\n\nprincipalvars: The variance of the principal components.\n\n\nExamples\nusing MLJ\n\nPCA = @load PCA pkg=MultivariateStats\n\nX, y = @load_iris # a table and a vector\n\nmodel = PCA(maxoutdim=2)\nmach = machine(model, X) |&gt; fit!\n\nXproj = transform(mach, X)\nSee also KernelPCA, ICA, FactorAnalysis, PPCA\n\n\n\n \n \n \n\n\n\n\n5.2 fitting model\n\nmach = machine(model, Xtrain) |&gt; fit!\nXtr =transform(mach, Xtrain)\n\n┌ Warning: The number and/or types of data arguments do not match what the specified model\n│ supports. Suppress this type check by specifying `scitype_check_level=0`.\n│ \n│ Run `@doc MultivariateStats.PCA` to learn more about your model's requirements.\n│ \n│ Commonly, but non exclusively, supervised models are constructed using the syntax\n│ `machine(model, X, y)` or `machine(model, X, y, w)` while most other models are\n│ constructed with `machine(model, X)`.  Here `X` are features, `y` a target, and `w`\n│ sample or class weights.\n│ \n│ In general, data in `machine(model, data...)` is expected to satisfy\n│ \n│     scitype(data) &lt;: MLJ.fit_data_scitype(model)\n│ \n│ In the present case:\n│ \n│ scitype(data) = Tuple{Table{AbstractVector{Count}}}\n│ \n│ fit_data_scitype(model) = Tuple{Table{&lt;:AbstractVector{&lt;:Continuous}}}\n└ @ MLJBase ~/.julia/packages/MLJBase/fEiP2/src/machines.jl:230\n[ Info: Training machine(PCA(maxoutdim = 3, …), …).\n\n\n12000×3 DataFrame11975 rows omitted\n\n\n\nRow\nx1\nx2\nx3\n\n\n\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n-856.852\n-332.575\n-34.3783\n\n\n2\n-245.793\n-112.833\n1046.94\n\n\n3\n-390.243\n401.559\n30.4265\n\n\n4\n966.51\n-359.698\n650.63\n\n\n5\n354.756\n-115.593\n527.175\n\n\n6\n-508.043\n-813.775\n-417.797\n\n\n7\n720.101\n-149.118\n-734.982\n\n\n8\n-461.916\n568.386\n447.195\n\n\n9\n-861.752\n-190.173\n75.7481\n\n\n10\n636.84\n428.198\n563.142\n\n\n11\n-475.992\n-179.361\n-396.116\n\n\n12\n545.461\n-188.282\n697.017\n\n\n13\n-269.984\n-381.653\n705.561\n\n\n⋮\n⋮\n⋮\n⋮\n\n\n11989\n-181.447\n-310.647\n-2.19181\n\n\n11990\n-956.569\n-595.445\n-138.727\n\n\n11991\n-44.6248\n225.234\n466.205\n\n\n11992\n386.423\n-17.1393\n-552.14\n\n\n11993\n566.271\n-80.3966\n-103.541\n\n\n11994\n-498.803\n-52.1383\n-26.2291\n\n\n11995\n3.47708\n500.304\n-260.501\n\n\n11996\n875.924\n44.0938\n-1050.98\n\n\n11997\n154.431\n-141.981\n-450.883\n\n\n11998\n-66.3014\n-687.415\n-39.575\n\n\n11999\n-420.884\n-151.585\n801.763\n\n\n12000\n-763.315\n-403.198\n-129.845",
    "crumbs": [
      "2-mnist-classification.qmd"
    ]
  },
  {
    "objectID": "learn/Classfication/2-mnist-classification.html#plot-pca-results",
    "href": "learn/Classfication/2-mnist-classification.html#plot-pca-results",
    "title": "2-Mnist Classification",
    "section": "6. plot pca results",
    "text": "6. plot pca results\n\ntable=(x1=Xtr.x1,x2=Xtr.x2,x3=Xtr.x3,labels=ytrain)\ndatalayer=data(table)\nmaplayer=mapping(:x1,:x2,color=:labels)\nvislayer=visual(Scatter,markersize=10,strokewidth=1,strokecolor=:black)\nplt=datalayer*maplayer*vislayer\ndraw(plt)",
    "crumbs": [
      "2-mnist-classification.qmd"
    ]
  },
  {
    "objectID": "dataset/penguins.html",
    "href": "dataset/penguins.html",
    "title": "🐧🐧 Penguins dataset",
    "section": "",
    "text": "info\n\n\n\nPenguins data set is another famous dataset for data science, compare with Iris, Penguins has more categorical feature.\nplease reference :\n\nPalmer penguins website\n[Aog Tutorial 🐧]",
    "crumbs": [
      "Dataset",
      "2.penguins"
    ]
  },
  {
    "objectID": "dataset/penguins.html#load-csv",
    "href": "dataset/penguins.html#load-csv",
    "title": "🐧🐧 Penguins dataset",
    "section": "1. load csv",
    "text": "1. load csv\n\nusing CSV,DataFrames,Tidier\nusing CairoMakie,AlgebraOfGraphics,MakieThemes\nMakie.set_theme!(ggthemr(:dust))\n\ndf=CSV.File(\"../data/penguine_data.csv\")|&gt;DataFrame\nfirst(df,5)\n\n5×7 DataFrame\n\n\n\nRow\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\nString15\nString15\nString7\nString7\nString3\nString7\nString7\n\n\n\n\n1\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmale\n\n\n2\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nfemale\n\n\n3\nAdelie\nTorgersen\n40.3\n18\n195\n3250\nfemale\n\n\n4\nAdelie\nTorgersen\nNA\nNA\nNA\nNA\nNA\n\n\n5\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale",
    "crumbs": [
      "Dataset",
      "2.penguins"
    ]
  },
  {
    "objectID": "dataset/penguins.html#describe-dataframe",
    "href": "dataset/penguins.html#describe-dataframe",
    "title": "🐧🐧 Penguins dataset",
    "section": "2. describe dataframe",
    "text": "2. describe dataframe\n\ndescribe(df)\n\n7×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nNothing\nInlineSt…\nNothing\nInlineSt…\nInt64\nDataType\n\n\n\n\n1\nspecies\n\nAdelie\n\nGentoo\n0\nString15\n\n\n2\nisland\n\nBiscoe\n\nTorgersen\n0\nString15\n\n\n3\nbill_length_mm\n\n32.1\n\nNA\n0\nString7\n\n\n4\nbill_depth_mm\n\n13.1\n\nNA\n0\nString7\n\n\n5\nflipper_length_mm\n\n172\n\nNA\n0\nString3\n\n\n6\nbody_mass_g\n\n2700\n\nNA\n0\nString7\n\n\n7\nsex\n\nNA\n\nmale\n0\nString7",
    "crumbs": [
      "Dataset",
      "2.penguins"
    ]
  },
  {
    "objectID": "dataset/penguins.html#eda",
    "href": "dataset/penguins.html#eda",
    "title": "🐧🐧 Penguins dataset",
    "section": "3. EDA",
    "text": "3. EDA\n\n3.1 frequency\n\ndatalayer=data(df)\nmappinglayer=frequency()*mapping(:species,color = :island,dodge = :island)\naxis = (width = 225, height = 225)\ndraw(datalayer*mappinglayer,axis=axis)\n\n\n\n\n\n\n\nFigure 1: fig-penguins-fequency\n\n\n\n\n\n\n\n3.2 correlation of two variables\n\n#show(names(df))\ncats=[\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\nlen=length(cats)\ncats=Symbol.(cats)\n\n4-element Vector{Symbol}:\n :bill_length_mm\n :bill_depth_mm\n :flipper_length_mm\n :body_mass_g\n\n\n\npalette = (color=tuple.([\"#FC7808\", \"#8C00EC\", \"#107A78\"], 0.65),\n           marker=[:circle, :utriangle, :rect])\ncycle = Cycle([:color, :marker], covary=true)\n\nfunction make_layer(sy1::Symbol,sy2::Symbol)\n   mapping_layer=sy1==sy2 ? mapping(sy1,color=:species) : mapping(sy1,sy2,color=:species)\n   vis_layer=sy1==sy2 ? visual(Density,color=:species) : visual(Scatter,color=:species,strokewidth=1,storkecolor=:black)\n   return  datalayer*mapping_layer*vis_layer\nend\n\nwith_theme(ggthemr(:dust),resolution = (800,800), palette=palette, Scatter=(cycle=cycle,)) do\n        cl=0.1\n        fig = Figure()\n        for i in 1:len\n            for  j in 1:len\n                ax=Axis(fig[i,j])\n                j==1 ? ax.ylabel= String(cats[i]) : nothing\n                i==len ? ax.xlabel= String(cats[j]) : nothing\n                layer=make_layer(cats[i],cats[j])\n              [i,j]==[len,len] ? cl=draw!(ax,layer) : draw!(ax,layer)\n            end\n        end\n        legend!(fig[2:3,5], cl)\n        fig\nend\n\n\n\n\n\n\n\nFigure 2: fig-penguins-pairplot\n\n\n\n\n\n\n\n3.3 varying weights of the penguins\n\nax=(width = 225, height = 225)\nmappinglayer2=mapping(:species, :bill_depth_mm;color=:species)\nvislayer2=visual(BoxPlot,show_notch=true)\ndraw(datalayer*mappinglayer2*vislayer2,axis=ax)\n\n\n\n\n\n\n\nFigure 3: fig-penguins-bodymass",
    "crumbs": [
      "Dataset",
      "2.penguins"
    ]
  },
  {
    "objectID": "dataset/mtcar.html",
    "href": "dataset/mtcar.html",
    "title": "🚗🚕🚙 auto-mpg dataset",
    "section": "",
    "text": "using CSV,DataFrames,Tidier\nusing CairoMakie,AlgebraOfGraphics,MakieThemes\nusing StatsBase\nusing ScientificTypes\nMakie.set_theme!(ggthemr(:flat));\n\n\ndf=CSV.File(\"../data/auto-mpg.csv\")|&gt;DataFrame\ndf=@chain df begin\n   @clean_names\nend\nfirst(df,5)\n\n\n\n\n5×7 DataFrame\n\n\n\nRow\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nmodel_year\n\n\n\nFloat64\nInt64\nFloat64\nInt64?\nInt64\nFloat64\nInt64\n\n\n\n\n1\n18.0\n8\n307.0\n130\n3504\n12.0\n70\n\n\n2\n15.0\n8\n350.0\n165\n3693\n11.5\n70\n\n\n3\n18.0\n8\n318.0\n150\n3436\n11.0\n70\n\n\n4\n16.0\n8\n304.0\n150\n3433\n12.0\n70\n\n\n5\n17.0\n8\n302.0\n140\n3449\n10.5\n70\n\n\n\n\n\n\n\nTable 1: auto-mpg dataset",
    "crumbs": [
      "Dataset",
      "4.auto mpg"
    ]
  },
  {
    "objectID": "dataset/mtcar.html#load-csv",
    "href": "dataset/mtcar.html#load-csv",
    "title": "🚗🚕🚙 auto-mpg dataset",
    "section": "",
    "text": "using CSV,DataFrames,Tidier\nusing CairoMakie,AlgebraOfGraphics,MakieThemes\nusing StatsBase\nusing ScientificTypes\nMakie.set_theme!(ggthemr(:flat));\n\n\ndf=CSV.File(\"../data/auto-mpg.csv\")|&gt;DataFrame\ndf=@chain df begin\n   @clean_names\nend\nfirst(df,5)\n\n\n\n\n5×7 DataFrame\n\n\n\nRow\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nmodel_year\n\n\n\nFloat64\nInt64\nFloat64\nInt64?\nInt64\nFloat64\nInt64\n\n\n\n\n1\n18.0\n8\n307.0\n130\n3504\n12.0\n70\n\n\n2\n15.0\n8\n350.0\n165\n3693\n11.5\n70\n\n\n3\n18.0\n8\n318.0\n150\n3436\n11.0\n70\n\n\n4\n16.0\n8\n304.0\n150\n3433\n12.0\n70\n\n\n5\n17.0\n8\n302.0\n140\n3449\n10.5\n70\n\n\n\n\n\n\n\nTable 1: auto-mpg dataset",
    "crumbs": [
      "Dataset",
      "4.auto mpg"
    ]
  },
  {
    "objectID": "dataset/mtcar.html#describe-data",
    "href": "dataset/mtcar.html#describe-data",
    "title": "🚗🚕🚙 auto-mpg dataset",
    "section": "2. describe data",
    "text": "2. describe data\n\ndropmissing!(df)\n@show describe(df);\n\ndescribe(df) = 7×7 DataFrame\n Row │ variable      mean        min     median   max     nmissing  eltype\n     │ Symbol        Float64     Real    Float64  Real    Int64     DataType\n─────┼───────────────────────────────────────────────────────────────────────\n   1 │ mpg             23.5172      9.0     23.0    46.6         0  Float64\n   2 │ cylinders        5.45707     3        4.0     8           0  Int64\n   3 │ displacement   193.65       68.0    148.5   455.0         0  Float64\n   4 │ horsepower     104.189      46       92.0   230           0  Int64\n   5 │ weight        2973.0      1613     2803.5  5140           0  Int64\n   6 │ acceleration    15.5558      8.0     15.5    24.8         0  Float64\n   7 │ model_year      76.0278     70       76.0    82           0  Int64",
    "crumbs": [
      "Dataset",
      "4.auto mpg"
    ]
  },
  {
    "objectID": "dataset/mtcar.html#eda",
    "href": "dataset/mtcar.html#eda",
    "title": "🚗🚕🚙 auto-mpg dataset",
    "section": "3. EDA",
    "text": "3. EDA\n\n3.1 heatmap of variables\n\ndf_cor = df|&gt;Matrix|&gt;cor.|&gt; d -&gt; round(d, digits=2)\nlabels=names(df)\nfunction plot_cov_cor()\n    fig = Figure(resolution=(800, 400)) \n    ax1 = Axis(fig[1, 1]; xticks=(1:7, labels), yticks=(1:7, labels), title=\"corr of mpg variables\",\n    xticklabelrotation = pi/8,\n    yreversed=true)\n    hm = heatmap!(ax1, df_cor)\n    Colorbar(fig[1, 2], hm)\n    [text!(ax1, x, y; text=string(df_cor[x, y]), color=:white, fontsize=18, align=(:center, :center)) for x in 1:7, y in 1:7]\n    fig\nend\nplot_cov_cor()\n\n\n\n\n\n\n\nFigure 1: fig-automag-coorlation\n\n\n\n\n\n\n\n3.2 Univariate Analysis\n\n3.2.1 cylinders data\nplot_univariate plot univariate count\n\n    function plot_univariate(df::AbstractDataFrame,feature1::Symbol,feature2::Symbol,cats::Symbol)\n    ax=(width = 225, height = 225)\n    data_layer=data(df)\n    mappinglayer=mapping(feature1,feature2,color=cats)\n    vislayer=visual(BarPlot,bar_labels=:y,flip_labels_at=130)\n    plt=data_layer*mappinglayer*vislayer\n        draw(plt,axis=ax)\n    end\n\nplot_univariate (generic function with 1 method)\n\n\n\ndf321=@chain df begin\n    @group_by(cylinders)\n    @summarize(count=n())\n    @ungroup\nend\nplot_univariate(df321,:cylinders,:count,:cylinders)\n\n\n\n\n\n\n\n\n\n\n3.2.2 model_year count\n\ndf322=@chain df begin\n    @group_by(model_year)\n    @summarize(count=n())\n    @ungroup\nend\n\nplot_univariate(df322,:model_year,:count,:model_year)\n\n\n\n\n\n\n\n\n\n\n3.2.3 density of horsepower\n\nax=(width = 400, height = 300)\ndatalayer323=data(df)\nmappinglayer323=mapping(:horsepower)\nvislayer3231=visual(AlgebraOfGraphics.Density,color=(:lightgreen,0.6),strokewidth=1,strokecolor=:black)\nvislayer3232=visual(AlgebraOfGraphics.Hist,strokewidth=1,strokecolor=:black,normalization = :pdf,color=(:red,0.5))\ndraw(datalayer323*mappinglayer323*(vislayer3231+vislayer3232),axis=ax)\n\n\n\n\n\n\n\nFigure 2: fig-automag-horsepower-density\n\n\n\n\n\n\n\n\n3.3 multivariate analysis\n\n3.3.1 mpg by cylinders\n\nlet\n    ax=(width =250, height = 250)\n    datalayer=data(df)\n    mappinglayer=mapping(:cylinders,:mpg,color=:cylinders)\n    vislayer=visual(BoxPlot)\n   data(df) * visual(BoxPlot) *\n    mapping(:cylinders, :mpg, color=:cylinders) |&gt;d-&gt;draw(d,axis=ax)\nend\n\n\n\n\n\n\n\n\n\n\n3.3.2 mpg by model_year\n\nlet\n    ax=(width =250, height = 250)\n    data(df) * visual(BoxPlot) *\n    mapping(:model_year, :mpg, color=:model_year) |&gt;d-&gt;draw(d,axis=ax)\nend\n\n\n\n\n\n\n\n\n\n\n3.3.3 horsepower-mpg relatiion\n\nax=(width =250, height = 250)\n plt1 = data(df)*mapping(:horsepower,:mpg) * linear()\n #plt2 = data(df)*mapping(:horsepower,:mpg) \n draw(plt1, axis=ax)",
    "crumbs": [
      "Dataset",
      "4.auto mpg"
    ]
  },
  {
    "objectID": "dataset/index.html",
    "href": "dataset/index.html",
    "title": "Make friends with data!",
    "section": "",
    "text": "first step to start data science is recognize your data only you get familiar with the data , then make flow work",
    "crumbs": [
      "Dataset",
      "intro"
    ]
  },
  {
    "objectID": "dataset/florida-lakes.html",
    "href": "dataset/florida-lakes.html",
    "title": "11 Florida Lakes",
    "section": "",
    "text": "info\n\n\n\nref : FloridaLakes Dataset",
    "crumbs": [
      "Dataset",
      "11.Florida Lakes Dataset"
    ]
  },
  {
    "objectID": "dataset/florida-lakes.html#load-package",
    "href": "dataset/florida-lakes.html#load-package",
    "title": "11 Florida Lakes",
    "section": "1. load package",
    "text": "1. load package\n\n using CSV,DataFrames,Tidier,Pipe\n using CairoMakie,AlgebraOfGraphics,MakieThemes\n using StatsBase\n #Makie.set_theme!(ggthemr(:flat))",
    "crumbs": [
      "Dataset",
      "11.Florida Lakes Dataset"
    ]
  },
  {
    "objectID": "dataset/florida-lakes.html#load-csv",
    "href": "dataset/florida-lakes.html#load-csv",
    "title": "11 Florida Lakes",
    "section": "2. load csv",
    "text": "2. load csv\n\ndf=CSV.File(\"../data/FloridaLakes.csv\")|&gt;DataFrame\ndf=@chain  df  begin\n    @clean_names\nend\nfirst(df,5)\n\n5×12 DataFrame\n\n\n\nRow\ni_d\nlake\nalkalinity\np_h\ncalcium\nchlorophyll\navg_mercury\nnum_samples\nmin_mercury\nmax_mercury\nthree_yr_std_mercury\nage_data\n\n\n\nInt64\nString31\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nInt64\nFloat64\nFloat64\nFloat64\nInt64\n\n\n\n\n1\n1\nAlligator\n5.9\n6.1\n3.0\n0.7\n1.23\n5\n0.85\n1.43\n1.53\n1\n\n\n2\n2\nAnnie\n3.5\n5.1\n1.9\n3.2\n1.33\n7\n0.92\n1.9\n1.33\n0\n\n\n3\n3\nApopka\n116.0\n9.1\n44.1\n128.3\n0.04\n6\n0.04\n0.06\n0.04\n0\n\n\n4\n4\nBlue Cypress\n39.4\n6.9\n16.4\n3.5\n0.44\n12\n0.13\n0.84\n0.44\n0\n\n\n5\n5\nBrick\n2.5\n4.6\n2.9\n1.8\n1.2\n12\n0.69\n1.5\n1.33\n1",
    "crumbs": [
      "Dataset",
      "11.Florida Lakes Dataset"
    ]
  },
  {
    "objectID": "dataset/florida-lakes.html#describe-df",
    "href": "dataset/florida-lakes.html#describe-df",
    "title": "11 Florida Lakes",
    "section": "3. describe df",
    "text": "3. describe df\n\ndescribe(df)\n\n12×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nDataType\n\n\n\n\n1\ni_d\n27.0\n1\n27.0\n53\n0\nInt64\n\n\n2\nlake\n\nAlligator\n\nYale\n0\nString31\n\n\n3\nalkalinity\n37.5302\n1.2\n19.6\n128.0\n0\nFloat64\n\n\n4\np_h\n6.59057\n3.6\n6.8\n9.1\n0\nFloat64\n\n\n5\ncalcium\n22.2019\n1.1\n12.6\n90.7\n0\nFloat64\n\n\n6\nchlorophyll\n23.117\n0.7\n12.8\n152.4\n0\nFloat64\n\n\n7\navg_mercury\n0.52717\n0.04\n0.48\n1.33\n0\nFloat64\n\n\n8\nnum_samples\n13.0566\n4\n12.0\n44\n0\nInt64\n\n\n9\nmin_mercury\n0.279811\n0.04\n0.25\n0.92\n0\nFloat64\n\n\n10\nmax_mercury\n0.874528\n0.06\n0.84\n2.04\n0\nFloat64\n\n\n11\nthree_yr_std_mercury\n0.513208\n0.04\n0.45\n1.53\n0\nFloat64\n\n\n12\nage_data\n0.811321\n0\n1.0\n1\n0\nInt64",
    "crumbs": [
      "Dataset",
      "11.Florida Lakes Dataset"
    ]
  },
  {
    "objectID": "dataset/florida-lakes.html#eda",
    "href": "dataset/florida-lakes.html#eda",
    "title": "11 Florida Lakes",
    "section": "4. EDA",
    "text": "4. EDA\n\n4.1 data processing\nselect numerical features and standarid\n\nnew_df=@chain df begin\n   @select(3:7,9:11)\n   mapcols(zscore,_)\nend\n\n53×8 DataFrame28 rows omitted\n\n\n\nRow\nalkalinity\np_h\ncalcium\nchlorophyll\navg_mercury\nmin_mercury\nmax_mercury\nthree_yr_std_mercury\n\n\n\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n-0.827939\n-0.380741\n-0.770153\n-0.727439\n2.06087\n2.51844\n1.06403\n3.00178\n\n\n2\n-0.89076\n-1.15687\n-0.814272\n-0.646313\n2.35409\n2.82762\n1.96433\n2.41134\n\n\n3\n2.05399\n1.94764\n0.878293\n3.41322\n-1.4285\n-1.05921\n-1.56026\n-1.39701\n\n\n4\n0.0489434\n0.24016\n-0.232703\n-0.636578\n-0.255603\n-0.661694\n-0.0661402\n-0.216124\n\n\n5\n-0.916936\n-1.54493\n-0.774163\n-0.691743\n1.9729\n1.81174\n1.19811\n2.41134\n\n\n6\n-0.469333\n0.550611\n-0.70999\n0.680906\n-0.754085\n-1.05921\n-0.755733\n-0.777044\n\n\n7\n-0.846262\n-0.92403\n-0.778174\n-0.639823\n-0.138313\n0.0891703\n-0.296005\n-0.186602\n\n\n8\n0.886562\n1.17151\n1.32349\n0.343423\n-0.988664\n-0.882536\n-0.947287\n-1.04274\n\n\n9\n-0.291339\n-0.613579\n-0.521482\n-0.698233\n0.887972\n-0.0875036\n1.00656\n0.610495\n\n\n10\n-0.856732\n-0.147903\n-0.70598\n-0.0200212\n0.829327\n0.575024\n1.14065\n0.876193\n\n\n11\n-0.809616\n-0.92403\n-0.782185\n-0.266644\n0.536103\n1.06088\n-0.0278295\n0.580972\n\n\n12\n-0.550478\n0.472998\n-0.336984\n-0.620352\n-0.0796685\n-0.794199\n-0.276849\n-0.00946935\n\n\n13\n-0.317515\n0.472998\n0.120249\n-0.37373\n-0.108991\n-0.0875036\n0.259501\n0.0790969\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n42\n1.56975\n0.162547\n0.934445\n-0.548962\n-1.25257\n-1.01504\n-1.17715\n-0.954176\n\n\n43\n-0.673503\n-0.535967\n0.0801407\n-0.698233\n-0.138313\n-0.0433351\n0.336122\n-0.216124\n\n\n44\n0.758302\n1.32674\n0.152335\n1.46296\n-0.93002\n-1.01504\n-0.755733\n-1.04274\n\n\n45\n-0.563565\n0.0849346\n0.76198\n0.0318993\n0.97594\n0.354181\n1.00656\n0.462884\n\n\n46\n-0.851497\n-0.303129\n0.0560758\n-0.438631\n-0.0210236\n0.133339\n0.144569\n0.108619\n\n\n47\n-0.31228\n-0.303129\n-0.385114\n0.148721\n0.360168\n0.0891703\n0.431899\n0.197185\n\n\n48\n1.15094\n1.79241\n-0.0682596\n-0.438631\n-0.754085\n-1.05921\n-0.908976\n-0.718\n\n\n49\n-0.950964\n-1.77777\n-0.80625\n-0.542472\n1.21052\n1.37006\n0.700074\n1.37807\n\n\n50\n-0.0924048\n0.317773\n-0.36506\n-0.600882\n-0.372893\n-0.882536\n0.048792\n-0.599911\n\n\n51\n-0.576653\n0.24016\n-0.681915\n-0.214723\n-0.284926\n-0.220009\n-0.353471\n-0.245646\n\n\n52\n-0.529537\n-1.07926\n-0.770153\n-0.665783\n-0.81273\n-0.573357\n-0.908976\n-0.688477\n\n\n53\n0.897033\n1.01629\n-0.0682596\n-0.464591\n-0.754085\n-0.573357\n-0.698267\n-0.777044\n\n\n\n\n\n\n\n\n4.2 heatmap\n\ncats=names(new_df)\nrow,col=size(new_df)\ncor_matrix=Matrix(new_df)|&gt;cor\nrow,col=size(cor_matrix)\n\n(8, 8)\n\n\n\nfig = Figure(size = (800, 800))\nax = Axis(fig[1, 1], xticks = (1:row, cats), yticks = (1:row, cats),xticklabelrotation = pi/8,ylabelsize=8,xlabelsize=8)\nhmap = heatmap!(ax, cor_matrix, colormap = :plasma)\nColorbar(fig[1, 2], hmap; label = \"values\", width = 10, ticksize = 10)\nfig\n\n\n\n\n\n\n\n\n\n\n4.3 pair plot\n\ndata_layer=data(new_df)\nfunction make_layer(sy1::Symbol,sy2::Symbol)\n   mapping_layer=sy1==sy2 ? mapping(sy1,) : mapping(sy1,sy2)\n   vis_layer=sy1==sy2 ? visual(Density,color=(:green,0.6)) : visual(Scatter,strokewidth=1,storkecolor=:black,color=:red,markersize=4)\n   return  data_layer*mapping_layer*vis_layer\nend\n\ncats=@pipe names(new_df)|&gt;Symbol.(_)\nlen=length(cats)\n\n8\n\n\n\nwith_theme(theme_minimal(),resolution = (1200,1200)) do\n        cl=0.1\n        fig = Figure()\n        for i in 1:len\n            for  j in 1:len\n                ax=Axis(fig[i,j])\n                j==1 ? ax.ylabel= String(cats[i]) : nothing\n                i==len ? ax.xlabel= String(cats[j]) : nothing\n                layer=make_layer(cats[i],cats[j])\n              [i,j]==[len,len] ? cl=draw!(ax,layer) : draw!(ax,layer)\n            end\n        end\n        \n        fig\nend",
    "crumbs": [
      "Dataset",
      "11.Florida Lakes Dataset"
    ]
  },
  {
    "objectID": "dataset/diabetes.html",
    "href": "dataset/diabetes.html",
    "title": "Diabetes Diagnosis Dataset",
    "section": "",
    "text": "info\n\n\n\nref : Diabetes Classification|EDA|SVM|LR|DT|RF|XGBOOST\nref : EDA, Feature and ComparisonML 0.87(Diabetes)",
    "crumbs": [
      "Dataset",
      "8.Diabetes diagnosis dataset"
    ]
  },
  {
    "objectID": "dataset/diabetes.html#load-package",
    "href": "dataset/diabetes.html#load-package",
    "title": "Diabetes Diagnosis Dataset",
    "section": "1. load package",
    "text": "1. load package\n\n using CSV,DataFrames,Tidier,Pipe\n using CairoMakie,AlgebraOfGraphics,MakieThemes\n using ScientificTypes,StatsBase,RCall\n #Makie.set_theme!(ggthemr(:flat))",
    "crumbs": [
      "Dataset",
      "8.Diabetes diagnosis dataset"
    ]
  },
  {
    "objectID": "dataset/diabetes.html#load-csv",
    "href": "dataset/diabetes.html#load-csv",
    "title": "Diabetes Diagnosis Dataset",
    "section": "2. load csv",
    "text": "2. load csv\n\ndf=CSV.File(\"../data/diabetes.csv\")|&gt;DataFrame\ndf=@chain  df  begin\n    @clean_names\nend\nfirst(df,5)\n\n5×9 DataFrame\n\n\n\nRow\npregnancies\nglucose\nblood_pressure\nskin_thickness\ninsulin\nb_m_i\ndiabetes_pedigree_function\nage\noutcome\n\n\n\nInt64\nInt64\nInt64\nInt64\nInt64\nFloat64\nFloat64\nInt64\nInt64\n\n\n\n\n1\n6\n148\n72\n35\n0\n33.6\n0.627\n50\n1\n\n\n2\n1\n85\n66\n29\n0\n26.6\n0.351\n31\n0\n\n\n3\n8\n183\n64\n0\n0\n23.3\n0.672\n32\n1\n\n\n4\n1\n89\n66\n23\n94\n28.1\n0.167\n21\n0\n\n\n5\n0\n137\n40\n35\n168\n43.1\n2.288\n33\n1",
    "crumbs": [
      "Dataset",
      "8.Diabetes diagnosis dataset"
    ]
  },
  {
    "objectID": "dataset/diabetes.html#describe-df",
    "href": "dataset/diabetes.html#describe-df",
    "title": "Diabetes Diagnosis Dataset",
    "section": "3. describe df",
    "text": "3. describe df\n\ndescribe(df)\n\n9×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nFloat64\nReal\nFloat64\nReal\nInt64\nDataType\n\n\n\n\n1\npregnancies\n3.84505\n0\n3.0\n17\n0\nInt64\n\n\n2\nglucose\n120.895\n0\n117.0\n199\n0\nInt64\n\n\n3\nblood_pressure\n69.1055\n0\n72.0\n122\n0\nInt64\n\n\n4\nskin_thickness\n20.5365\n0\n23.0\n99\n0\nInt64\n\n\n5\ninsulin\n79.7995\n0\n30.5\n846\n0\nInt64\n\n\n6\nb_m_i\n31.9926\n0.0\n32.0\n67.1\n0\nFloat64\n\n\n7\ndiabetes_pedigree_function\n0.471876\n0.078\n0.3725\n2.42\n0\nFloat64\n\n\n8\nage\n33.2409\n21\n29.0\n81\n0\nInt64\n\n\n9\noutcome\n0.348958\n0\n0.0\n1\n0\nInt64",
    "crumbs": [
      "Dataset",
      "8.Diabetes diagnosis dataset"
    ]
  },
  {
    "objectID": "dataset/diabetes.html#eda",
    "href": "dataset/diabetes.html#eda",
    "title": "Diabetes Diagnosis Dataset",
    "section": "4. EDA",
    "text": "4. EDA\n\n4.1 cor of eachcol\n\ndf41=@chain df begin\n    @select(1:8)\nend\noutcome=df.outcome\n\ncors=[cor(col, outcome) for col in eachcol(df41)]\nfeatures=names(df41)\n\n8-element Vector{String}:\n \"pregnancies\"\n \"glucose\"\n \"blood_pressure\"\n \"skin_thickness\"\n \"insulin\"\n \"b_m_i\"\n \"diabetes_pedigree_function\"\n \"age\"\n\n\n\nax=(width=250,height=250,xticklabelrotation = pi/8)\ndatalayer=data(DataFrame(cors=cors,features=features))\nmaplayer=mapping(:features,:cors,color=:features)\nvislayer=visual(BarPlot,strokewidth=1,strokecolor=:black)\nplt=datalayer*maplayer*vislayer\ndraw(plt;axis=ax)\n\n\n\n\n\n\n\n\n\n\n4.2 Feature Engineering\n\nnew_df=@chain df begin\n  @mutate(age_cat = case_when(age&lt;21 =&gt; \"young\",\n                                age&gt;50 =&gt; \"elder\",\n                                true =&gt; \"mature\"\n                                ))\n  @mutate(glucose_cat=case_when(\n                                glucose&gt;=140=&gt;\"at_rist\",\n                                true=&gt;\"normal\"\n  )) \n\n  @mutate(age_gul_cat=case_when((glucose&lt;140) & (age&lt;=50) =&gt; \"normal_mature\", \n                                 (glucose&lt;140) & (age&gt;50) =&gt; \"normal_elder\",\n                                 (glucose&gt;140) & (age&lt;50) =&gt; \"at_risk_mature\",\n                                 true=&gt;\"at_risk_elder\"\n  ))\n\n  @mutate(age_insul_level=case_when(\n\n                                   (insulin&lt;126) & (age&lt;=50) =&gt; \"is_normal_mature\",\n                                   (insulin&lt;126) & (age&gt;50) =&gt; \"is_normal_elder\",\n                                   (insulin&gt;=126) & (age&lt;=50) =&gt; \"not_normal_mature\",\n                                   (insulin&gt;=126) & (age&gt;50) =&gt; \"not_normal_elder\",\n\n\n  ))\n  @mutate(insulin_level=case_when(insulin&lt;126 =&gt;\"is_normal\",\n                                  insulin&gt;=126 =&gt;\"not_normal\"\n                            \n                                 \n  ))\n\n  @mutate(gulcose_insulin=glucose*insulin)\nend\n\n768×15 DataFrame743 rows omitted\n\n\n\nRow\npregnancies\nglucose\nblood_pressure\nskin_thickness\ninsulin\nb_m_i\ndiabetes_pedigree_function\nage\noutcome\nage_cat\nglucose_cat\nage_gul_cat\nage_insul_level\ninsulin_level\ngulcose_insulin\n\n\n\nInt64\nInt64\nInt64\nInt64\nInt64\nFloat64\nFloat64\nInt64\nInt64\nString\nString\nString\nString\nString\nInt64\n\n\n\n\n1\n6\n148\n72\n35\n0\n33.6\n0.627\n50\n1\nmature\nat_rist\nat_risk_elder\nis_normal_mature\nis_normal\n0\n\n\n2\n1\n85\n66\n29\n0\n26.6\n0.351\n31\n0\nmature\nnormal\nnormal_mature\nis_normal_mature\nis_normal\n0\n\n\n3\n8\n183\n64\n0\n0\n23.3\n0.672\n32\n1\nmature\nat_rist\nat_risk_mature\nis_normal_mature\nis_normal\n0\n\n\n4\n1\n89\n66\n23\n94\n28.1\n0.167\n21\n0\nmature\nnormal\nnormal_mature\nis_normal_mature\nis_normal\n8366\n\n\n5\n0\n137\n40\n35\n168\n43.1\n2.288\n33\n1\nmature\nnormal\nnormal_mature\nnot_normal_mature\nnot_normal\n23016\n\n\n6\n5\n116\n74\n0\n0\n25.6\n0.201\n30\n0\nmature\nnormal\nnormal_mature\nis_normal_mature\nis_normal\n0\n\n\n7\n3\n78\n50\n32\n88\n31.0\n0.248\n26\n1\nmature\nnormal\nnormal_mature\nis_normal_mature\nis_normal\n6864\n\n\n8\n10\n115\n0\n0\n0\n35.3\n0.134\n29\n0\nmature\nnormal\nnormal_mature\nis_normal_mature\nis_normal\n0\n\n\n9\n2\n197\n70\n45\n543\n30.5\n0.158\n53\n1\nelder\nat_rist\nat_risk_elder\nnot_normal_elder\nnot_normal\n106971\n\n\n10\n8\n125\n96\n0\n0\n0.0\n0.232\n54\n1\nelder\nnormal\nnormal_elder\nis_normal_elder\nis_normal\n0\n\n\n11\n4\n110\n92\n0\n0\n37.6\n0.191\n30\n0\nmature\nnormal\nnormal_mature\nis_normal_mature\nis_normal\n0\n\n\n12\n10\n168\n74\n0\n0\n38.0\n0.537\n34\n1\nmature\nat_rist\nat_risk_mature\nis_normal_mature\nis_normal\n0\n\n\n13\n10\n139\n80\n0\n0\n27.1\n1.441\n57\n0\nelder\nnormal\nnormal_elder\nis_normal_elder\nis_normal\n0\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n757\n7\n137\n90\n41\n0\n32.0\n0.391\n39\n0\nmature\nnormal\nnormal_mature\nis_normal_mature\nis_normal\n0\n\n\n758\n0\n123\n72\n0\n0\n36.3\n0.258\n52\n1\nelder\nnormal\nnormal_elder\nis_normal_elder\nis_normal\n0\n\n\n759\n1\n106\n76\n0\n0\n37.5\n0.197\n26\n0\nmature\nnormal\nnormal_mature\nis_normal_mature\nis_normal\n0\n\n\n760\n6\n190\n92\n0\n0\n35.5\n0.278\n66\n1\nelder\nat_rist\nat_risk_elder\nis_normal_elder\nis_normal\n0\n\n\n761\n2\n88\n58\n26\n16\n28.4\n0.766\n22\n0\nmature\nnormal\nnormal_mature\nis_normal_mature\nis_normal\n1408\n\n\n762\n9\n170\n74\n31\n0\n44.0\n0.403\n43\n1\nmature\nat_rist\nat_risk_mature\nis_normal_mature\nis_normal\n0\n\n\n763\n9\n89\n62\n0\n0\n22.5\n0.142\n33\n0\nmature\nnormal\nnormal_mature\nis_normal_mature\nis_normal\n0\n\n\n764\n10\n101\n76\n48\n180\n32.9\n0.171\n63\n0\nelder\nnormal\nnormal_elder\nnot_normal_elder\nnot_normal\n18180\n\n\n765\n2\n122\n70\n27\n0\n36.8\n0.34\n27\n0\nmature\nnormal\nnormal_mature\nis_normal_mature\nis_normal\n0\n\n\n766\n5\n121\n72\n23\n112\n26.2\n0.245\n30\n0\nmature\nnormal\nnormal_mature\nis_normal_mature\nis_normal\n13552\n\n\n767\n1\n126\n60\n0\n0\n30.1\n0.349\n47\n1\nmature\nnormal\nnormal_mature\nis_normal_mature\nis_normal\n0\n\n\n768\n1\n93\n70\n31\n0\n30.4\n0.315\n23\n0\nmature\nnormal\nnormal_mature\nis_normal_mature\nis_normal\n0\n\n\n\n\n\n\n\nax=(width=150,height=250)\ndatalayer=data(new_df)\nmaplayer=mapping(:glucose_cat)*frequency()*mapping(color=:glucose_cat)\ndraw(datalayer*maplayer,axis=ax)",
    "crumbs": [
      "Dataset",
      "8.Diabetes diagnosis dataset"
    ]
  },
  {
    "objectID": "dataset/clustering-dataset.html",
    "href": "dataset/clustering-dataset.html",
    "title": "Clustering Data Set",
    "section": "",
    "text": "info\n\n\n\nref: Clustering Exercises include lots of csv files",
    "crumbs": [
      "Dataset",
      "14 clustering datset(kaggle)"
    ]
  },
  {
    "objectID": "dataset/clustering-dataset.html#load-package",
    "href": "dataset/clustering-dataset.html#load-package",
    "title": "Clustering Data Set",
    "section": "1. load package",
    "text": "1. load package\n\n #| eval: false\n using CSV,DataFrames,Tidier,FreqTables,PrettyTables\n using CairoMakie,AlgebraOfGraphics\n using MLJ",
    "crumbs": [
      "Dataset",
      "14 clustering datset(kaggle)"
    ]
  },
  {
    "objectID": "dataset/clustering-dataset.html#load-csv",
    "href": "dataset/clustering-dataset.html#load-csv",
    "title": "Clustering Data Set",
    "section": "2. load csv",
    "text": "2. load csv\n\nconst path = \"../data/clustering-datasets/\"\ndfArr = DataFrame[] # store dataframe by files\nnameArr = String[]  # csv file names\n\n\nfor (root, dirs, files) in walkdir(path)\n    for file in files\n        name,suffix = split(file, \".\")\n        if suffix == \"csv\"\n            res = CSV.File(joinpath(path, file)) |&gt; DataFrame\n            res.name=fill(name,nrow(res))\n            push!(dfArr, res)\n            #push!(nameArr, name)\n        else\n            continue\n        end\n    end\nend\n\ntotal_df=reduce(vcat,[dfArr...],cols=:union)\nsize(total_df)",
    "crumbs": [
      "Dataset",
      "14 clustering datset(kaggle)"
    ]
  },
  {
    "objectID": "dataset/clustering-dataset.html#plot",
    "href": "dataset/clustering-dataset.html#plot",
    "title": "Clustering Data Set",
    "section": "3. plot",
    "text": "3. plot\n\n\n\n    function group_data(df)\n    gdf = groupby(df, :color)\n    label_size = length(keys(gdf))\n    return gdf, label_size\n    end\n    function plot_examples()\n    fig = Figure(resolution=(1200, 1100))\n    for idx in eachindex(dfArr)\n        row,col=fldmod1(idx,6)\n        gdf, label_size = group_data(dfArr[idx])\n            ax = Axis(fig[row, col], title=nameArr[idx])\n            for k in 1:label_size\n                scatter!(ax, gdf[k][:, :x], gdf[k][:, :y])\n                #hidespines!(ax)\n                hidedecorations!(ax)\n            end\n    end\n    fig\nend\nplot_examples()\n\n\n\n\nclustering-datasets plots",
    "crumbs": [
      "Dataset",
      "14 clustering datset(kaggle)"
    ]
  },
  {
    "objectID": "dataset/boston-house.html",
    "href": "dataset/boston-house.html",
    "title": "🏠🏡🏣 Boston Housing dataset",
    "section": "",
    "text": "info\n\n\n\nBoston Housing is another famous dataset",
    "crumbs": [
      "Dataset",
      "3.boston housing"
    ]
  },
  {
    "objectID": "dataset/boston-house.html#load-csv",
    "href": "dataset/boston-house.html#load-csv",
    "title": "🏠🏡🏣 Boston Housing dataset",
    "section": "1. load csv",
    "text": "1. load csv\n\n using CSV,DataFrames,Tidier\n using CairoMakie,AlgebraOfGraphics\n\ndf=CSV.File(\"../data/housing.csv\")|&gt;DataFrame\n first(df,5)\n\n5×14 DataFrame\n\n\n\nRow\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nb\nlstat\nmedv\n\n\n\nFloat64\nFloat64\nFloat64\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\nInt64\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n0.00632\n18.0\n2.31\n0\n0.538\n6.575\n65.2\n4.09\n1\n296\n15.3\n396.9\n4.98\n24.0\n\n\n2\n0.02731\n0.0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242\n17.8\n396.9\n9.14\n21.6\n\n\n3\n0.02729\n0.0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242\n17.8\n392.83\n4.03\n34.7\n\n\n4\n0.03237\n0.0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222\n18.7\n394.63\n2.94\n33.4\n\n\n5\n0.06905\n0.0\n2.18\n0\n0.458\n7.147\n54.2\n6.0622\n3\n222\n18.7\n396.9\n5.33\n36.2",
    "crumbs": [
      "Dataset",
      "3.boston housing"
    ]
  },
  {
    "objectID": "dataset/boston-house.html#describe-dataframe",
    "href": "dataset/boston-house.html#describe-dataframe",
    "title": "🏠🏡🏣 Boston Housing dataset",
    "section": "2. describe dataframe",
    "text": "2. describe dataframe\n\ndescribe(df)\n\n14×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nFloat64\nReal\nFloat64\nReal\nInt64\nDataType\n\n\n\n\n1\ncrim\n3.61352\n0.00632\n0.25651\n88.9762\n0\nFloat64\n\n\n2\nzn\n11.3636\n0.0\n0.0\n100.0\n0\nFloat64\n\n\n3\nindus\n11.1368\n0.46\n9.69\n27.74\n0\nFloat64\n\n\n4\nchas\n0.06917\n0\n0.0\n1\n0\nInt64\n\n\n5\nnox\n0.554695\n0.385\n0.538\n0.871\n0\nFloat64\n\n\n6\nrm\n6.28463\n3.561\n6.2085\n8.78\n0\nFloat64\n\n\n7\nage\n68.5749\n2.9\n77.5\n100.0\n0\nFloat64\n\n\n8\ndis\n3.79504\n1.1296\n3.20745\n12.1265\n0\nFloat64\n\n\n9\nrad\n9.54941\n1\n5.0\n24\n0\nInt64\n\n\n10\ntax\n408.237\n187\n330.0\n711\n0\nInt64\n\n\n11\nptratio\n18.4555\n12.6\n19.05\n22.0\n0\nFloat64\n\n\n12\nb\n356.674\n0.32\n391.44\n396.9\n0\nFloat64\n\n\n13\nlstat\n12.6531\n1.73\n11.36\n37.97\n0\nFloat64\n\n\n14\nmedv\n22.5328\n5.0\n21.2\n50.0\n0\nFloat64",
    "crumbs": [
      "Dataset",
      "3.boston housing"
    ]
  },
  {
    "objectID": "dataset/boston-house.html#eda",
    "href": "dataset/boston-house.html#eda",
    "title": "🏠🏡🏣 Boston Housing dataset",
    "section": "3 EDA",
    "text": "3 EDA\n\n3.1 CHA\ncharles river is a famous river around harvard univ and mit so , we want to know by ther river side affect house price\n\ndf1=@chain df begin\n   @group_by(chas)\n   @summarize(mean=mean(medv),median=median(medv))\n\nend\n\nresolution = (500, 250)\nfig = Figure(; resolution)\nax1 = Axis(fig[1, 1],xlabel=\"chas\",ylabel=\"mean\")\nax2 = Axis(fig[1, 2],xlabel=\"chas\",ylabel=\"median\")\n\ndatalayer1=data(df1)\nmappinglayer1=mapping(:chas,:mean, color=:chas) \nmappinglayer2=mapping(:chas,:median, color=:chas)\nvislayer=visual(BarPlot,bar_labels=:y,flip_labels_at=23)\n\nplt1 = datalayer1* mappinglayer1*vislayer\nplt2 = datalayer1* mappinglayer2*vislayer\ndraw!(ax1,plt1)\ngrid=draw!(ax2,plt2)\nlegend!(fig[1, 3], grid)\nfig\n\n\n\n\n\n\n\n\n\n\n3.2 Rad count\n\ndf2=@chain df begin\n    @group_by(rad)\n    @summarize(count=n())\n    @ungroup\nend\n\n9×2 DataFrame\n\n\n\nRow\nrad\ncount\n\n\n\nInt64\nInt64\n\n\n\n\n1\n1\n20\n\n\n2\n2\n24\n\n\n3\n3\n38\n\n\n4\n4\n110\n\n\n5\n5\n115\n\n\n6\n6\n26\n\n\n7\n7\n17\n\n\n8\n8\n24\n\n\n9\n24\n132\n\n\n\n\n\n\n\nlet \n    resolution = (400, 300)\n    fig = Figure(; resolution)\n    ax = Axis(fig[1, 1],xlabel=\"rad\",ylabel=\"count\")\n    \n    datalayer32=data(df2)\n    mappinglayer32=mapping(:rad,:count,color=:rad)\n    vislayer32=visual(BarPlot,bar_labels=:y,flip_labels_at=130)\n    plt=datalayer32*mappinglayer32*vislayer32\n    draw!(ax,plt)\n    fig\nend\n\n\n\n\n\n\n\n\n\n\n3.3 Rad price mean\n\ndf33=@chain df begin\n    @group_by(rad)\n    @summarize(mean=mean(medv))\n    @ungroup\nend\n\n9×2 DataFrame\n\n\n\nRow\nrad\nmean\n\n\n\nInt64\nFloat64\n\n\n\n\n1\n1\n24.365\n\n\n2\n2\n26.8333\n\n\n3\n3\n27.9289\n\n\n4\n4\n21.3873\n\n\n5\n5\n25.707\n\n\n6\n6\n20.9769\n\n\n7\n7\n27.1059\n\n\n8\n8\n30.3583\n\n\n9\n24\n16.4038\n\n\n\n\n\n\n\n\n3.4 medv density\n\ndatalayer34=data(df)\nmappinglayer34=mapping(:medv)\nvislayer341=visual(Density,color=(:lightgreen,0.6),strokewidth=1,strokecolor=:black)\nvislayer342=visual(Hist,strokewidth=1,strokecolor=:black,normalization = :pdf,color=(:red,0.5))\ndraw(datalayer34*mappinglayer34*(vislayer341+vislayer342))",
    "crumbs": [
      "Dataset",
      "3.boston housing"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "dataset/breast-cancer.html",
    "href": "dataset/breast-cancer.html",
    "title": "Breast Cancer Wisconsin (Diagnostic) Data Set",
    "section": "",
    "text": "info\n\n\n\nref : Breast Cancer Wisconsin (Diagnostic) Data Set",
    "crumbs": [
      "Dataset",
      "7.breast cancer"
    ]
  },
  {
    "objectID": "dataset/breast-cancer.html#load-package",
    "href": "dataset/breast-cancer.html#load-package",
    "title": "Breast Cancer Wisconsin (Diagnostic) Data Set",
    "section": "1. load package",
    "text": "1. load package\n\n using CSV,DataFrames,Tidier,Pipe\n using CairoMakie,AlgebraOfGraphics,MakieThemes\n using ScientificTypes,StatsBase,RCall\n #Makie.set_theme!(ggthemr(:flat))",
    "crumbs": [
      "Dataset",
      "7.breast cancer"
    ]
  },
  {
    "objectID": "dataset/breast-cancer.html#load-csv",
    "href": "dataset/breast-cancer.html#load-csv",
    "title": "Breast Cancer Wisconsin (Diagnostic) Data Set",
    "section": "2. load csv",
    "text": "2. load csv\n\ndf=CSV.File(\"../data/Breast-Cancer-Wisconsin.csv\")|&gt;DataFrame\ndf=@chain  df  begin  \n    @clean_names \n    @select(Not(33))\n    @mutate(diagnosis = case_when(diagnosis==\"M\"  =&gt; \"malignant\",\n                        true    =&gt; \"benign\"))\nend\nfirst(df,5)\n\n5×32 DataFrame\n\n\n\nRow\nid\ndiagnosis\nradius_mean\ntexture_mean\nperimeter_mean\narea_mean\nsmoothness_mean\ncompactness_mean\nconcavity_mean\nconcave_points_mean\nsymmetry_mean\nfractal_dimension_mean\nradius_se\ntexture_se\nperimeter_se\narea_se\nsmoothness_se\ncompactness_se\nconcavity_se\nconcave_points_se\nsymmetry_se\nfractal_dimension_se\nradius_worst\ntexture_worst\nperimeter_worst\narea_worst\nsmoothness_worst\ncompactness_worst\nconcavity_worst\nconcave_points_worst\nsymmetry_worst\nfractal_dimension_worst\n\n\n\nInt64\nString\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n842302\nmalignant\n17.99\n10.38\n122.8\n1001.0\n0.1184\n0.2776\n0.3001\n0.1471\n0.2419\n0.07871\n1.095\n0.9053\n8.589\n153.4\n0.006399\n0.04904\n0.05373\n0.01587\n0.03003\n0.006193\n25.38\n17.33\n184.6\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.1189\n\n\n2\n842517\nmalignant\n20.57\n17.77\n132.9\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n0.1812\n0.05667\n0.5435\n0.7339\n3.398\n74.08\n0.005225\n0.01308\n0.0186\n0.0134\n0.01389\n0.003532\n24.99\n23.41\n158.8\n1956.0\n0.1238\n0.1866\n0.2416\n0.186\n0.275\n0.08902\n\n\n3\n84300903\nmalignant\n19.69\n21.25\n130.0\n1203.0\n0.1096\n0.1599\n0.1974\n0.1279\n0.2069\n0.05999\n0.7456\n0.7869\n4.585\n94.03\n0.00615\n0.04006\n0.03832\n0.02058\n0.0225\n0.004571\n23.57\n25.53\n152.5\n1709.0\n0.1444\n0.4245\n0.4504\n0.243\n0.3613\n0.08758\n\n\n4\n84348301\nmalignant\n11.42\n20.38\n77.58\n386.1\n0.1425\n0.2839\n0.2414\n0.1052\n0.2597\n0.09744\n0.4956\n1.156\n3.445\n27.23\n0.00911\n0.07458\n0.05661\n0.01867\n0.05963\n0.009208\n14.91\n26.5\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.173\n\n\n5\n84358402\nmalignant\n20.29\n14.34\n135.1\n1297.0\n0.1003\n0.1328\n0.198\n0.1043\n0.1809\n0.05883\n0.7572\n0.7813\n5.438\n94.44\n0.01149\n0.02461\n0.05688\n0.01885\n0.01756\n0.005115\n22.54\n16.67\n152.2\n1575.0\n0.1374\n0.205\n0.4\n0.1625\n0.2364\n0.07678",
    "crumbs": [
      "Dataset",
      "7.breast cancer"
    ]
  },
  {
    "objectID": "dataset/breast-cancer.html#decscrbe-of-dataset",
    "href": "dataset/breast-cancer.html#decscrbe-of-dataset",
    "title": "Breast Cancer Wisconsin (Diagnostic) Data Set",
    "section": "3. decscrbe of dataset",
    "text": "3. decscrbe of dataset\n\ndescribe(df)\n\n32×7 DataFrame7 rows omitted\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nDataType\n\n\n\n\n1\nid\n3.03718e7\n8670\n906024.0\n911320502\n0\nInt64\n\n\n2\ndiagnosis\n\nbenign\n\nmalignant\n0\nString\n\n\n3\nradius_mean\n14.1273\n6.981\n13.37\n28.11\n0\nFloat64\n\n\n4\ntexture_mean\n19.2896\n9.71\n18.84\n39.28\n0\nFloat64\n\n\n5\nperimeter_mean\n91.969\n43.79\n86.24\n188.5\n0\nFloat64\n\n\n6\narea_mean\n654.889\n143.5\n551.1\n2501.0\n0\nFloat64\n\n\n7\nsmoothness_mean\n0.0963603\n0.05263\n0.09587\n0.1634\n0\nFloat64\n\n\n8\ncompactness_mean\n0.104341\n0.01938\n0.09263\n0.3454\n0\nFloat64\n\n\n9\nconcavity_mean\n0.0887993\n0.0\n0.06154\n0.4268\n0\nFloat64\n\n\n10\nconcave_points_mean\n0.0489191\n0.0\n0.0335\n0.2012\n0\nFloat64\n\n\n11\nsymmetry_mean\n0.181162\n0.106\n0.1792\n0.304\n0\nFloat64\n\n\n12\nfractal_dimension_mean\n0.0627976\n0.04996\n0.06154\n0.09744\n0\nFloat64\n\n\n13\nradius_se\n0.405172\n0.1115\n0.3242\n2.873\n0\nFloat64\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n21\nsymmetry_se\n0.0205423\n0.007882\n0.01873\n0.07895\n0\nFloat64\n\n\n22\nfractal_dimension_se\n0.0037949\n0.0008948\n0.003187\n0.02984\n0\nFloat64\n\n\n23\nradius_worst\n16.2692\n7.93\n14.97\n36.04\n0\nFloat64\n\n\n24\ntexture_worst\n25.6772\n12.02\n25.41\n49.54\n0\nFloat64\n\n\n25\nperimeter_worst\n107.261\n50.41\n97.66\n251.2\n0\nFloat64\n\n\n26\narea_worst\n880.583\n185.2\n686.5\n4254.0\n0\nFloat64\n\n\n27\nsmoothness_worst\n0.132369\n0.07117\n0.1313\n0.2226\n0\nFloat64\n\n\n28\ncompactness_worst\n0.254265\n0.02729\n0.2119\n1.058\n0\nFloat64\n\n\n29\nconcavity_worst\n0.272188\n0.0\n0.2267\n1.252\n0\nFloat64\n\n\n30\nconcave_points_worst\n0.114606\n0.0\n0.09993\n0.291\n0\nFloat64\n\n\n31\nsymmetry_worst\n0.290076\n0.1565\n0.2822\n0.6638\n0\nFloat64\n\n\n32\nfractal_dimension_worst\n0.0839458\n0.05504\n0.08004\n0.2075\n0\nFloat64",
    "crumbs": [
      "Dataset",
      "7.breast cancer"
    ]
  },
  {
    "objectID": "dataset/breast-cancer.html#eda",
    "href": "dataset/breast-cancer.html#eda",
    "title": "Breast Cancer Wisconsin (Diagnostic) Data Set",
    "section": "4. EDA",
    "text": "4. EDA\n\n4.1 diagnostic of cancer\n\nlet\n    ax=(width=250,height=250)\n    plt=data(df)*mapping(:diagnosis)*frequency()*mapping(color=:diagnosis)\n    draw(plt;axis=ax)\nend\n\n\n\n\n\n\n\n\n\n\n4.2 diagonstic mean\n\n#df42= @pivot_longer(df, 3:12, names_to = \"features\", values_to = \"mean_value\");\n\ndf421=@chain df begin\n   @select(1:2)\nend\ndf422=@chain df begin\n   @select(3:12)\n   mapcols(zscore, _)\nend\ndf42=hcat(df421,df422)\ndf42= @pivot_longer(df42, 3:12, names_to = \"features\", values_to = \"mean_value\")\n\n5690×4 DataFrame5665 rows omitted\n\n\n\nRow\nid\ndiagnosis\nfeatures\nmean_value\n\n\n\nInt64\nString\nString\nFloat64\n\n\n\n\n1\n842302\nmalignant\nradius_mean\n1.0961\n\n\n2\n842517\nmalignant\nradius_mean\n1.82821\n\n\n3\n84300903\nmalignant\nradius_mean\n1.5785\n\n\n4\n84348301\nmalignant\nradius_mean\n-0.768233\n\n\n5\n84358402\nmalignant\nradius_mean\n1.74876\n\n\n6\n843786\nmalignant\nradius_mean\n-0.475956\n\n\n7\n844359\nmalignant\nradius_mean\n1.16988\n\n\n8\n84458202\nmalignant\nradius_mean\n-0.118413\n\n\n9\n844981\nmalignant\nradius_mean\n-0.319885\n\n\n10\n84501001\nmalignant\nradius_mean\n-0.473118\n\n\n11\n845636\nmalignant\nradius_mean\n0.537083\n\n\n12\n84610002\nmalignant\nradius_mean\n0.46898\n\n\n13\n846226\nmalignant\nradius_mean\n1.43094\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n5679\n925236\nbenign\nfractal_dimension_mean\n-0.312677\n\n\n5680\n925277\nbenign\nfractal_dimension_mean\n-0.188037\n\n\n5681\n925291\nbenign\nfractal_dimension_mean\n0.411082\n\n\n5682\n925292\nbenign\nfractal_dimension_mean\n-0.154044\n\n\n5683\n925311\nbenign\nfractal_dimension_mean\n-1.10159\n\n\n5684\n925622\nmalignant\nfractal_dimension_mean\n1.2354\n\n\n5685\n926125\nmalignant\nfractal_dimension_mean\n0.848737\n\n\n5686\n926424\nmalignant\nfractal_dimension_mean\n-0.930209\n\n\n5687\n926682\nmalignant\nfractal_dimension_mean\n-1.05768\n\n\n5688\n926954\nmalignant\nfractal_dimension_mean\n-0.8948\n\n\n5689\n927241\nmalignant\nfractal_dimension_mean\n1.04278\n\n\n5690\n92751\nbenign\nfractal_dimension_mean\n-0.560539\n\n\n\n\n\n\n\nlet\nax=(width=600,height=250,xticklabelrotation = pi/8)\ndatalayer=data(df42)\nmappinglayer=mapping(:features,:mean_value)\nvislayer=visual(Violin)\nplt=datalayer*mappinglayer*vislayer\nplt=data(df42) * visual(Violin) *mapping(:features, :mean_value,color=:diagnosis,dodge=:diagnosis,side=:diagnosis) \ndraw(plt;axis=ax)\n  \nend\n\n\n\n\n\n\n\n\n\n\n4.3 second 10 features\n\ndf431=@chain df begin\n   @select(1:2)\nend\ndf432=@chain df begin\n   @select(13:22)\n   mapcols(zscore, _)\nend\ndf43=hcat(df431,df432)\ndf43= @pivot_longer(df43, 3:12, names_to = \"features\", values_to = \"mean_value\")\n\n5690×4 DataFrame5665 rows omitted\n\n\n\nRow\nid\ndiagnosis\nfeatures\nmean_value\n\n\n\nInt64\nString\nString\nFloat64\n\n\n\n\n1\n842302\nmalignant\nradius_se\n2.48755\n\n\n2\n842517\nmalignant\nradius_se\n0.498816\n\n\n3\n84300903\nmalignant\nradius_se\n1.2276\n\n\n4\n84348301\nmalignant\nradius_se\n0.326087\n\n\n5\n84358402\nmalignant\nradius_se\n1.26943\n\n\n6\n843786\nmalignant\nradius_se\n-0.254846\n\n\n7\n844359\nmalignant\nradius_se\n0.149751\n\n\n8\n84458202\nmalignant\nradius_se\n0.643057\n\n\n9\n844981\nmalignant\nradius_se\n-0.356536\n\n\n10\n84501001\nmalignant\nradius_se\n-0.387909\n\n\n11\n845636\nmalignant\nradius_se\n-0.0925744\n\n\n12\n84610002\nmalignant\nradius_se\n0.362868\n\n\n13\n846226\nmalignant\nradius_se\n1.9845\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n5679\n925236\nbenign\nfractal_dimension_se\n-0.177963\n\n\n5680\n925277\nbenign\nfractal_dimension_se\n0.230945\n\n\n5681\n925291\nbenign\nfractal_dimension_se\n0.356414\n\n\n5682\n925292\nbenign\nfractal_dimension_se\n0.570316\n\n\n5683\n925311\nbenign\nfractal_dimension_se\n-0.764116\n\n\n5684\n925622\nmalignant\nfractal_dimension_se\n0.887012\n\n\n5685\n926125\nmalignant\nfractal_dimension_se\n0.913844\n\n\n5686\n926424\nmalignant\nfractal_dimension_se\n0.167832\n\n\n5687\n926682\nmalignant\nfractal_dimension_se\n-0.490124\n\n\n5688\n926954\nmalignant\nfractal_dimension_se\n0.0366945\n\n\n5689\n927241\nmalignant\nfractal_dimension_se\n0.903262\n\n\n5690\n92751\nbenign\nfractal_dimension_se\n-0.382418\n\n\n\n\n\n\n\nlet\nax=(width=600,height=400,xticklabelrotation = pi/8)\nplt=data(df43) * visual(Violin) *mapping(:features, :mean_value,color=:diagnosis,dodge=:diagnosis) \ndraw(plt;axis=ax)\nend\n\n\n\n\n\n\n\n\n\n\n4.4 bee-swarm plot\n\ndf44=dropmissing(df43)\nrdata=df43.mean_value\ncats=df43.features\ndiagnosis=df.diagnosis\n@rput rdata\n@rput cats\n@rput diagnosis\n\nR\"\"\"\nlibrary(beeswarm)\nbeeswarm(rdata ~ cats,\n         pch = 19, pwcol = as.numeric(diagnosis))\nlegend(\"topright\", legend = c(\"malignant\", \"benign\"),\n       col = 1:2, pch = 19)\n\"\"\"\n\n\n\n4.5 variables corelation\n\ndf45=select(df,3:32)\ncats=names(df45)\ncor_matrix=Matrix(df45)|&gt;cor\n\n30×30 Matrix{Float64}:\n  1.0          0.323782    …   0.744214    0.163953    0.00706589\n  0.323782     1.0             0.295316    0.105008    0.119205\n  0.997855     0.329533        0.771241    0.189115    0.0510185\n  0.987357     0.321086        0.722017    0.14357     0.0037376\n  0.170581    -0.0233885       0.503053    0.394309    0.499316\n  0.506124     0.236702    …   0.815573    0.510223    0.687382\n  0.676764     0.302418        0.861323    0.409464    0.51493\n  0.822529     0.293464        0.910155    0.375744    0.368661\n  0.147741     0.071401        0.430297    0.699826    0.438413\n -0.311631    -0.0764372       0.175325    0.334019    0.767297\n  0.67909      0.275869    …   0.531062    0.0945428   0.0495594\n -0.0973174    0.386358       -0.119638   -0.128215   -0.0456546\n  0.674172     0.281673        0.554897    0.10993     0.0854326\n  ⋮                        ⋱                          \n -0.104321     0.00912717     -0.0304134   0.389402    0.0780795\n -0.0426413    0.0544575       0.215204    0.111094    0.591328\n  0.969539     0.352573    …   0.787424    0.243529    0.093492\n  0.297008     0.912045        0.359755    0.233027    0.219122\n  0.965137     0.35804         0.816322    0.269493    0.138957\n  0.941082     0.343546        0.747419    0.209146    0.079647\n  0.119616     0.0775034       0.547691    0.493838    0.617624\n  0.413463     0.27783     …   0.80108     0.614441    0.810455\n  0.526911     0.301025        0.855434    0.53252     0.686511\n  0.744214     0.295316        1.0         0.502528    0.511114\n  0.163953     0.105008        0.502528    1.0         0.537848\n  0.00706589   0.119205        0.511114    0.537848    1.0\n\n\n\nfig = Figure(size = (1800, 2400))\nax = Axis(fig[1, 1], xticks = (1:30, cats), yticks = (1:30, cats),xticklabelrotation = pi/2,ylabelsize=8,xlabelsize=8)\nhmap = heatmap!(ax, cor_matrix, colormap = :plasma)\nColorbar(fig[1, 2], hmap; label = \"values\", width = 10, ticksize = 10)\nfig",
    "crumbs": [
      "Dataset",
      "7.breast cancer"
    ]
  },
  {
    "objectID": "dataset/cricket-chirps.html",
    "href": "dataset/cricket-chirps.html",
    "title": "10 🦗🦗🦗cricket chirps data",
    "section": "",
    "text": "info\n\n\n\nref : snowy tree cricket"
  },
  {
    "objectID": "dataset/cricket-chirps.html#load-package",
    "href": "dataset/cricket-chirps.html#load-package",
    "title": "10 🦗🦗🦗cricket chirps data",
    "section": "1. load package",
    "text": "1. load package\n\n #| eval: false\n using CSV,DataFrames,Tidier,Pipe\n using CairoMakie,AlgebraOfGraphics,MakieThemes\n using StatsBase,RCall\n #Makie.set_theme!(ggthemr(:flat))"
  },
  {
    "objectID": "dataset/cricket-chirps.html#load-csv",
    "href": "dataset/cricket-chirps.html#load-csv",
    "title": "10 🦗🦗🦗cricket chirps data",
    "section": "2. load csv",
    "text": "2. load csv\n\ndf=CSV.File(\"../data/CricketChirps.csv\")|&gt;DataFrame\ndf=@chain  df  begin\n    @clean_names\n\nend\nfirst(df,5)\n\n5×2 DataFrame\n\n\n\nRow\ntemperature\nchirps\n\n\n\nFloat64\nInt64\n\n\n\n\n1\n54.5\n81\n\n\n2\n59.5\n97\n\n\n3\n63.5\n103\n\n\n4\n67.5\n123\n\n\n5\n72.0\n150"
  },
  {
    "objectID": "dataset/cricket-chirps.html#describe-df",
    "href": "dataset/cricket-chirps.html#describe-df",
    "title": "10 🦗🦗🦗cricket chirps data",
    "section": "3. describe df",
    "text": "3. describe df\n\ndescribe(df)\n\n2×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nFloat64\nReal\nFloat64\nReal\nInt64\nDataType\n\n\n\n\n1\ntemperature\n68.3571\n54.5\n67.5\n83.0\n0\nFloat64\n\n\n2\nchirps\n133.0\n81\n123.0\n195\n0\nInt64"
  },
  {
    "objectID": "dataset/cricket-chirps.html#eda",
    "href": "dataset/cricket-chirps.html#eda",
    "title": "10 🦗🦗🦗cricket chirps data",
    "section": "4. EDA",
    "text": "4. EDA\n\ndatalayer=data(df)\nmaplayer=mapping(:temperature,:chirps)\nax=(width=250,height=250)\ndraw(datalayer*maplayer,axis=ax)"
  },
  {
    "objectID": "dataset/ecommerce-customer-dataset.html",
    "href": "dataset/ecommerce-customer-dataset.html",
    "title": "👧👱👳Ecommerce Customer dataset EDA",
    "section": "",
    "text": "info\n\n\n\ndataset: Linear Regression E-commerce Dataset\nmodel : E-commerce Dataset - Linear Regression Model\ndataset standard methods ref: Standardize all columns of DataFrame",
    "crumbs": [
      "Dataset",
      "12.Ecommerce Customer dataset"
    ]
  },
  {
    "objectID": "dataset/ecommerce-customer-dataset.html#load-package",
    "href": "dataset/ecommerce-customer-dataset.html#load-package",
    "title": "👧👱👳Ecommerce Customer dataset EDA",
    "section": "1. load package",
    "text": "1. load package\n\n using CSV,DataFrames,Tidier,Pipe\n using CairoMakie,AlgebraOfGraphics,MakieThemes\n using StatsBase",
    "crumbs": [
      "Dataset",
      "12.Ecommerce Customer dataset"
    ]
  },
  {
    "objectID": "dataset/ecommerce-customer-dataset.html#load-data",
    "href": "dataset/ecommerce-customer-dataset.html#load-data",
    "title": "👧👱👳Ecommerce Customer dataset EDA",
    "section": "2. load data",
    "text": "2. load data\n\ndf=CSV.File(\"../data/Ecommerce-Customers.csv\")|&gt;DataFrame\ndf=@chain df begin\n  @select(4:8)\n  @clean_names\n  mapcols(zscore, _)   #standard methods    \nend\n\nfirst(df,5)\n\n5×5 DataFrame\n\n\n\nRow\navg_session_length\ntime_on_app\ntime_on_website\nlength_of_membership\nyearly_amount_spent\n\n\n\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n1.45489\n0.606672\n2.49109\n0.549556\n1.11753\n\n\n2\n-1.13537\n-0.948514\n0.206349\n-0.870056\n-1.35043\n\n\n3\n-0.0526705\n-0.726412\n0.0496314\n0.571495\n-0.148352\n\n\n4\n1.26175\n1.67471\n-0.335642\n-0.413582\n1.04064\n\n\n5\n0.279558\n0.747022\n0.471265\n0.913507\n1.26196",
    "crumbs": [
      "Dataset",
      "12.Ecommerce Customer dataset"
    ]
  },
  {
    "objectID": "dataset/ecommerce-customer-dataset.html#describe-df",
    "href": "dataset/ecommerce-customer-dataset.html#describe-df",
    "title": "👧👱👳Ecommerce Customer dataset EDA",
    "section": "3. describe df",
    "text": "3. describe df\n\n describe(df)\n\n5×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nFloat64\nFloat64\nFloat64\nFloat64\nInt64\nDataType\n\n\n\n\n1\navg_session_length\n-4.32565e-15\n-3.54714\n0.02903\n3.10959\n0\nFloat64\n\n\n2\ntime_on_app\n6.92668e-16\n-3.56496\n-0.0696596\n3.09239\n0\nFloat64\n\n\n3\ntime_on_website\n-1.05427e-15\n-3.11394\n0.00882856\n2.91417\n0\nFloat64\n\n\n4\nlength_of_membership\n-1.803e-16\n-3.26592\n0.000513819\n3.39168\n0\nFloat64\n\n\n5\nyearly_amount_spent\n-1.57718e-15\n-3.05925\n-0.00537306\n3.3563\n0\nFloat64",
    "crumbs": [
      "Dataset",
      "12.Ecommerce Customer dataset"
    ]
  },
  {
    "objectID": "dataset/ecommerce-customer-dataset.html#wide-to-long-table",
    "href": "dataset/ecommerce-customer-dataset.html#wide-to-long-table",
    "title": "👧👱👳Ecommerce Customer dataset EDA",
    "section": "4. wide to long table",
    "text": "4. wide to long table\n\nlong_table=@chain df begin\n@pivot_longer(_,1:5)\nend\n\n2500×2 DataFrame2475 rows omitted\n\n\n\nRow\nvariable\nvalue\n\n\n\nString\nFloat64\n\n\n\n\n1\navg_session_length\n1.45489\n\n\n2\navg_session_length\n-1.13537\n\n\n3\navg_session_length\n-0.0526705\n\n\n4\navg_session_length\n1.26175\n\n\n5\navg_session_length\n0.279558\n\n\n6\navg_session_length\n0.823972\n\n\n7\navg_session_length\n-1.03933\n\n\n8\navg_session_length\n-0.316404\n\n\n9\navg_session_length\n0.941582\n\n\n10\navg_session_length\n-1.12501\n\n\n11\navg_session_length\n0.946418\n\n\n12\navg_session_length\n0.832357\n\n\n13\navg_session_length\n-3.54714\n\n\n⋮\n⋮\n⋮\n\n\n2489\nyearly_amount_spent\n1.24095\n\n\n2490\nyearly_amount_spent\n-2.16777\n\n\n2491\nyearly_amount_spent\n0.139789\n\n\n2492\nyearly_amount_spent\n0.141051\n\n\n2493\nyearly_amount_spent\n-1.20399\n\n\n2494\nyearly_amount_spent\n1.61747\n\n\n2495\nyearly_amount_spent\n0.143072\n\n\n2496\nyearly_amount_spent\n0.939716\n\n\n2497\nyearly_amount_spent\n0.374898\n\n\n2498\nyearly_amount_spent\n0.659475\n\n\n2499\nyearly_amount_spent\n-0.540183\n\n\n2500\nyearly_amount_spent\n-0.0193583",
    "crumbs": [
      "Dataset",
      "12.Ecommerce Customer dataset"
    ]
  },
  {
    "objectID": "dataset/ecommerce-customer-dataset.html#eda",
    "href": "dataset/ecommerce-customer-dataset.html#eda",
    "title": "👧👱👳Ecommerce Customer dataset EDA",
    "section": "5 EDA",
    "text": "5 EDA\n\n5.1 boxplot\n\nlet\nax=(width=600, height=300,xticklabelrotation = pi/8)\ndatalayer=data(long_table)\nmaplayer=mapping(:variable,:value, color=:variable)\nvislayer=visual(BoxPlot, storkewidth=1,storkecolor=:black)\nplt51=datalayer*maplayer*vislayer\ndraw(plt51,axis=ax)\nend\n\n\n\n\n\n\n\n\n\n\n5.2 pair plot\n\ncats=names(df)\ncats=Symbol.(cats)\nlen=length(cats)\n\ndata_layer=data(df)\n\nfunction make_layer(sy1::Symbol,sy2::Symbol)\n   mapping_layer=sy1==sy2 ? mapping(sy1) : mapping(sy1,sy2)\n   vis_layer=sy1==sy2 ? visual(Density,color=(:green, 0.5)) : visual(Scatter,color=(:purple,0.5),strokewidth=1,strokecolor=:black,markersize=6)\n   return  data_layer*mapping_layer*vis_layer\nend\n\nwith_theme(theme_minimal(),resolution = (800,800)) do\n        cl=0.1\n        fig = Figure()\n        for i in 1:len\n            for  j in 1:len\n                ax=Axis(fig[i,j],yticklabelrotation = pi/8)\n                j==1 ? ax.ylabel= String(cats[i]) : nothing\n                i==len ? ax.xlabel= String(cats[j]) : nothing\n                layer=make_layer(cats[i],cats[j])\n              [i,j]==[len,len] ? cl=draw!(ax,layer) : draw!(ax,layer)\n            end\n        end\n    fig\nend\n\n\n\n\n\n\n\n\n\n\n5.3 heatmap\n\ncats=names(df)\nrow,col=size(df)\ncor_matrix=Matrix(df)|&gt;cor\nrow,col=size(cor_matrix)\n\n(5, 5)\n\n\n\nlet\nfig = Figure(size = (800, 800))\nax = Axis(fig[1, 1], xticks = (1:row, cats), yticks = (1:row, cats),xticklabelrotation = pi/8,ylabelsize=8,xlabelsize=8)\nhmap = heatmap!(ax, cor_matrix, colormap = :plasma)\nColorbar(fig[1, 2], hmap; label = \"values\", width = 10, ticksize = 10)\nfig\nend",
    "crumbs": [
      "Dataset",
      "12.Ecommerce Customer dataset"
    ]
  },
  {
    "objectID": "dataset/german-credit-card.html",
    "href": "dataset/german-credit-card.html",
    "title": "German Credit Card Dataset",
    "section": "",
    "text": "info\n\n\n\n\ndataset1 : Analysis of German Credit Data\ndataset2 : Predicting Credit Risk - Model Pipeline\n\nthere are two dataset name german credit .dataset1 is original data,but now we use dataset2, here is why change to dataset2 : Content",
    "crumbs": [
      "Dataset",
      "13. German Credit Card dataset"
    ]
  },
  {
    "objectID": "dataset/german-credit-card.html#load-package",
    "href": "dataset/german-credit-card.html#load-package",
    "title": "German Credit Card Dataset",
    "section": "1. load package",
    "text": "1. load package\n\n using CSV,DataFrames,Tidier,FreqTables,PrettyTables\n using CairoMakie,AlgebraOfGraphics\n using MLJ",
    "crumbs": [
      "Dataset",
      "13. German Credit Card dataset"
    ]
  },
  {
    "objectID": "dataset/german-credit-card.html#load-csv",
    "href": "dataset/german-credit-card.html#load-csv",
    "title": "German Credit Card Dataset",
    "section": "2. load csv",
    "text": "2. load csv\n\ndf=CSV.File(\"../data/german_credit_data.csv\")|&gt;DataFrame\ndf=@chain df begin\n   #coerce(_, :creditability =&gt;Multiclass)\n   @clean_names()\n   \nend\nfirst(df,5)\n\n5×11 DataFrame\n\n\n\nRow\ncolumn1\nage\nsex\njob\nhousing\nsaving_accounts\nchecking_account\ncredit_amount\nduration\npurpose\nrisk\n\n\n\nInt64\nInt64\nString7\nInt64\nString7\nString15\nString15\nInt64\nInt64\nString31\nString7\n\n\n\n\n1\n0\n67\nmale\n2\nown\nNA\nlittle\n1169\n6\nradio/TV\ngood\n\n\n2\n1\n22\nfemale\n2\nown\nlittle\nmoderate\n5951\n48\nradio/TV\nbad\n\n\n3\n2\n49\nmale\n1\nown\nlittle\nNA\n2096\n12\neducation\ngood\n\n\n4\n3\n45\nmale\n2\nfree\nlittle\nlittle\n7882\n42\nfurniture/equipment\ngood\n\n\n5\n4\n53\nmale\n2\nfree\nlittle\nlittle\n4870\n24\ncar\nbad\n\n\n\n\n\n\n\n\n  \n     CatBoostClassifier \n     \n     \n \n \n doc\n \n \n \n No documentation found.\nBinding catboost does not exist.\n\n \n \n \n\n\n\ndescribe(df)\n\n11×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nDataType\n\n\n\n\n1\ncolumn1\n499.5\n0\n499.5\n999\n0\nInt64\n\n\n2\nage\n35.546\n19\n33.0\n75\n0\nInt64\n\n\n3\nsex\n\nfemale\n\nmale\n0\nString7\n\n\n4\njob\n1.904\n0\n2.0\n3\n0\nInt64\n\n\n5\nhousing\n\nfree\n\nrent\n0\nString7\n\n\n6\nsaving_accounts\n\nNA\n\nrich\n0\nString15\n\n\n7\nchecking_account\n\nNA\n\nrich\n0\nString15\n\n\n8\ncredit_amount\n3271.26\n250\n2319.5\n18424\n0\nInt64\n\n\n9\nduration\n20.903\n4\n18.0\n72\n0\nInt64\n\n\n10\npurpose\n\nbusiness\n\nvacation/others\n0\nString31\n\n\n11\nrisk\n\nbad\n\ngood\n0\nString7\n\n\n\n\n\n\n\nschema(df)\n\n\n┌──────────────────┬──────────┬──────────┐\n│ names            │ scitypes │ types    │\n├──────────────────┼──────────┼──────────┤\n│ column1          │ Count    │ Int64    │\n│ age              │ Count    │ Int64    │\n│ sex              │ Textual  │ String7  │\n│ job              │ Count    │ Int64    │\n│ housing          │ Textual  │ String7  │\n│ saving_accounts  │ Textual  │ String15 │\n│ checking_account │ Textual  │ String15 │\n│ credit_amount    │ Count    │ Int64    │\n│ duration         │ Count    │ Int64    │\n│ purpose          │ Textual  │ String31 │\n│ risk             │ Textual  │ String7  │\n└──────────────────┴──────────┴──────────┘",
    "crumbs": [
      "Dataset",
      "13. German Credit Card dataset"
    ]
  },
  {
    "objectID": "dataset/german-credit-card.html#eda",
    "href": "dataset/german-credit-card.html#eda",
    "title": "German Credit Card Dataset",
    "section": "3. EDA",
    "text": "3. EDA\n\n3.1 saving_account_type\n\ndf31=@chain df @filter(saving_accounts !=\"NA\")\nax=(width=400, height=300)\nlet\n  datalayer31=data(df31)\n  maplayer31=mapping(:saving_accounts,color=:risk,dodge=:risk)\n  vislayer31=visual(BarPlot, strokewidth=1,strokecolor=:black)\n  plt31=datalayer31*frequency()*maplayer31*vislayer31\n  draw(plt31,axis=ax)\nend\n\n\n\n\n\n\n\n\n\n\n3.2 saving_account_type and job relation\n\nlet\n  datalayer32=data(df31)\n  maplayer32=mapping(:saving_accounts,:job,color=:risk,dodge=:saving_accounts,\n  side=:risk)\n  vislayer32=visual(Violin)\n  plt32=datalayer32*maplayer32*vislayer32\n  draw(plt32,axis=ax)\nend\n\n\n\n\n\n\n\n\n\n\n3.3 saving_account_type and credit amount\n\nlet\n  datalayer33=data(df31)\n  maplayer33=mapping(:saving_accounts,:credit_amount,color=:risk,dodge=:risk)\n  vislayer33=visual(BoxPlot,show_notch=true)\n  plt33=datalayer33*maplayer33*vislayer33\n  draw(plt33,axis=ax)\nend\n\n\n\n\n\n\n\n\n\n\n3.4 explore purpose of creditcard use\n\n3.4.1 purpose count\n\nlet\n  ax=(width=400, height=300,xticklabelrotation = pi/8)\n  datalayer34=data(df)\n  maplayer341=mapping(:purpose,color=:risk,dodge=:risk)\n  plt341=datalayer34*frequency()*maplayer341\n  draw(plt341,axis=ax)\n\nend\n\n\n\n\n\n\n\n\n\n\n3.4.2 purpose and age relation\n\nlet\n  ax=(width=400, height=200,xticklabelrotation = pi/8)\n  datalayer342=data(df)\n  maplayer342=mapping(:purpose,:age,color=:risk,dodge=:purpose,\n  side=:risk)\n  vislayer342=visual(Violin,datalimits=extrema)\n  plt342=datalayer342*maplayer342*vislayer342\n  draw(plt342,axis=ax)\n\nend\n\n\n\n\n\n\n\n\n\n\n3.4.3 purpose and credit amount\n\nlet\n  ax=(width=500, height=300,xticklabelrotation = pi/8)\n  datalayer343=data(df)\n  maplayer343=mapping(:purpose,:credit_amount,color=:risk,dodge=:risk)\n  vislayer343=visual(BoxPlot,show_notch=true)\n  plt343=datalayer343*maplayer343*vislayer343\n  draw(plt343,axis=ax)\nend\n\n\n\n\n\n\n\n\n\n\n\n3.5 duration count\n\nlet\n ax=(width=800, height=500,xticklabelrotation = pi/8)\n datalayer=data(df)\n maplayer=mapping(:duration,color=:risk,dodge=:risk)\n vislayer=visual(BarPlot)\n plt35=datalayer*maplayer*frequency()*vislayer\n draw(plt35,axis=ax)\nend\n#show(df.duration)\n\n\n\n\n\n\n\n\n\n\n3.6 duration density\n\nlet\n ax=(width=800, height=300,xticklabelrotation = pi/8)\n datalayer=data(df)\n maplayer=mapping(:duration,color=:risk)\n vislayer=(visual(Hist,normalization=:pdf)+\n visual(Density,strokewidth=1,strokecolor=:red))*visual(alpha=0.5)\n plt=datalayer*maplayer*vislayer\n draw(plt,axis=ax)\nend\n\n\n\n\n\n\n\n\n\n\n3.7 purpose and sex freqtable\n\n  tbl = freqtable(df,:purpose,:sex)\n\n8×2 Named Matrix{Int64}\n      purpose ╲ sex │ female    male\n────────────────────┼───────────────\nbusiness            │     19      78\ncar                 │     94     243\ndomestic appliances │      6       6\neducation           │     24      35\nfurniture/equipment │     74     107\nradio/TV            │     85     195\nrepairs             │      5      17\nvacation/others     │      3       9",
    "crumbs": [
      "Dataset",
      "13. German Credit Card dataset"
    ]
  },
  {
    "objectID": "dataset/iris.html",
    "href": "dataset/iris.html",
    "title": "🌺🌸Iris dataset",
    "section": "",
    "text": "info\n\n\n\nIRIS is a an famous dataset for statistics and data science. t has long history. we will use it angain and agian.rery time wo use iris dataset we can get new knowledget is like exercies for running",
    "crumbs": [
      "Dataset",
      "1.iris"
    ]
  },
  {
    "objectID": "dataset/iris.html#load-csv",
    "href": "dataset/iris.html#load-csv",
    "title": "🌺🌸Iris dataset",
    "section": "1. load csv",
    "text": "1. load csv\n\n using CSV,DataFrames,Tidier\n using CairoMakie,AlgebraOfGraphics\n\n #df=download(\"http://localhost:5220/data/iris.csv\")|&gt;CSV.File|&gt;DataFrame;\n df=CSV.File(\"../data/iris.csv\")|&gt;DataFrame\n first(df,5)\n\n5×5 DataFrame\n\n\n\nRow\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\nFloat64\nFloat64\nFloat64\nFloat64\nString15\n\n\n\n\n1\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n2\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n3\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\nCSV,DataFrames,Tidier 用于数据框处理\nCairoMakie,AlgebraOfGraphics用于绘图",
    "crumbs": [
      "Dataset",
      "1.iris"
    ]
  },
  {
    "objectID": "dataset/iris.html#describe-dataframe",
    "href": "dataset/iris.html#describe-dataframe",
    "title": "🌺🌸Iris dataset",
    "section": "2. describe dataframe",
    "text": "2. describe dataframe\n\ndescribe(df)\n\n5×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nDataType\n\n\n\n\n1\nsepal_length\n5.84333\n4.3\n5.8\n7.9\n0\nFloat64\n\n\n2\nsepal_width\n3.054\n2.0\n3.0\n4.4\n0\nFloat64\n\n\n3\npetal_length\n3.75867\n1.0\n4.35\n6.9\n0\nFloat64\n\n\n4\npetal_width\n1.19867\n0.1\n1.3\n2.5\n0\nFloat64\n\n\n5\nspecies\n\nsetosa\n\nvirginica\n0\nString15",
    "crumbs": [
      "Dataset",
      "1.iris"
    ]
  },
  {
    "objectID": "dataset/iris.html#eda",
    "href": "dataset/iris.html#eda",
    "title": "🌺🌸Iris dataset",
    "section": "3. EDA",
    "text": "3. EDA\n\n3.1 density plot\n\npalette = (color=tuple.([\"#FC7808\", \"#8C00EC\", \"#107A78\"], 0.65),\n           marker=[:circle, :utriangle, :rect])\ncycle = Cycle([:color, :marker], covary=true)\n\n\ndatalayer=data(df)\nmappinglayer1=mapping(:sepal_length,:sepal_width,color=:species)\nmappinglayer2=mapping(:sepal_length,color=:species)\nmappinglayer3=mapping(:sepal_width,color=:species)\n\nvisuallayer1=visual(Scatter,strokewidth=1,storkecolor=:black)\nvisuallayer2=visual(Density)\nvisuallayer3=visual(Density,direction=:y)\nplc=datalayer*mappinglayer1*visuallayer1  # top pic\nplt=datalayer*mappinglayer2*visuallayer2  # main pic\nplr=datalayer*mappinglayer3*visuallayer3  # right pic\n\nwith_theme(theme_light(),resolution = (600,400), palette=palette, Scatter=(cycle=cycle,)) do\n        fig = Figure()\n        axs = [Axis(fig[2,1], xlabel = \"sepal_length\", ylabel = \"sepal_width\"),\n            Axis(fig[1,1]), Axis(fig[2,2])]\n        dots = draw!(axs[1], plc)  # bject for lengend extract species info\n        draw!(axs[2], plt)\n        draw!(axs[3], plr)\n        # getting the right layout aspect\n        colsize!(fig.layout, 1, Auto(4.0))\n        rowsize!(fig.layout, 1, Auto(1/3))\n        colgap!(fig.layout,3)\n        rowgap!(fig.layout, 3)\n        linkxaxes!(axs[1], axs[2])\n        linkyaxes!(axs[1], axs[3])\n        hidedecorations!.(axs[2:3], grid=false)\n        \n        legend!(fig[1,2], dots)\n        fig\n    end\n\n\n\n\n\n\n\n\n\n\n3.2 pair plot\n\ncats=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\nlen=length(cats)\ncats=Symbol.(cats)\n\n\ndata_layer=data(df)\n\nfunction make_layer(sy1::Symbol,sy2::Symbol)\n   mapping_layer=sy1==sy2 ? mapping(sy1,color=:species) : mapping(sy1,sy2,color=:species)\n   vis_layer=sy1==sy2 ? visual(Density,color=:species) : visual(Scatter,color=:species,strokewidth=1,storkecolor=:black)\n   return  data_layer*mapping_layer*vis_layer\nend\n\nwith_theme(theme_minimal(),resolution = (800,800), palette=palette, Scatter=(cycle=cycle,)) do\n        cl=0.1\n        fig = Figure()\n        for i in 1:len\n            for  j in 1:len\n                ax=Axis(fig[i,j])\n                j==1 ? ax.ylabel= String(cats[i]) : nothing\n                i==len ? ax.xlabel= String(cats[j]) : nothing\n                layer=make_layer(cats[i],cats[j])\n              [i,j]==[len,len] ? cl=draw!(ax,layer) : draw!(ax,layer)\n            end\n        end\n        legend!(fig[2:3,5], cl)\n        fig\nend\n\n\n\n\n\n\n\n\n\n\n3.3 box plot\ntransform four feature to attribute and value\n\nlong_data=@pivot_longer(df, 1:5, names_to = attribute, values_to = value,-species)\nfirst(long_data,5)\n\n5×3 DataFrame\n\n\n\nRow\nspecies\nattribute\nvalue\n\n\n\nString15\nString\nFloat64\n\n\n\n\n1\nsetosa\nsepal_length\n5.1\n\n\n2\nsetosa\nsepal_length\n4.9\n\n\n3\nsetosa\nsepal_length\n4.7\n\n\n4\nsetosa\nsepal_length\n4.6\n\n\n5\nsetosa\nsepal_length\n5.0\n\n\n\n\n\n\n\ndata(long_data) * visual(BoxPlot, show_notch=true) *\n    mapping(:attribute, :value, color=:species, dodge=:species) |&gt; draw\n\n\n\n\n\n\n\n\n\n\n3.4 correlation of numerical variables",
    "crumbs": [
      "Dataset",
      "1.iris"
    ]
  },
  {
    "objectID": "dataset/telecom-churn.html",
    "href": "dataset/telecom-churn.html",
    "title": "📱📱📱telcom customer churn",
    "section": "",
    "text": "info\n\n\n\nref : Telco Customer Churn\nchurn means loss of customers",
    "crumbs": [
      "Dataset",
      "6.telecom customer churn"
    ]
  },
  {
    "objectID": "dataset/telecom-churn.html#load-csv",
    "href": "dataset/telecom-churn.html#load-csv",
    "title": "📱📱📱telcom customer churn",
    "section": "1. load csv",
    "text": "1. load csv\n\n using CSV,DataFrames,Tidier\n using CairoMakie,AlgebraOfGraphics,MakieThemes\n using ScientificTypes\n #Makie.set_theme!(ggthemr(:flat))\ndf=CSV.File(\"../data/Telco-Customer-Churn.csv\")|&gt;DataFrame\ndf=@chain  df  begin  @clean_names end\n#df=coerce(df,:total_charges=&gt;Continuous)\n#df.total_charges=[length(idx)== 0 ? 0 : parse.(Float64,idx) for idx in  df.total_charges]\nfirst(df,5)\n\n5×21 DataFrame\n\n\n\nRow\ncustomer_i_d\ngender\nsenior_citizen\npartner\ndependents\ntenure\nphone_service\nmultiple_lines\ninternet_service\nonline_security\nonline_backup\ndevice_protection\ntech_support\nstreaming_t_v\nstreaming_movies\ncontract\npaperless_billing\npayment_method\nmonthly_charges\ntotal_charges\nchurn\n\n\n\nString15\nString7\nInt64\nString3\nString3\nInt64\nString3\nString31\nString15\nString31\nString31\nString31\nString31\nString31\nString31\nString15\nString3\nString31\nFloat64\nString7\nString3\n\n\n\n\n1\n7590-VHVEG\nFemale\n0\nYes\nNo\n1\nNo\nNo phone service\nDSL\nNo\nYes\nNo\nNo\nNo\nNo\nMonth-to-month\nYes\nElectronic check\n29.85\n29.85\nNo\n\n\n2\n5575-GNVDE\nMale\n0\nNo\nNo\n34\nYes\nNo\nDSL\nYes\nNo\nYes\nNo\nNo\nNo\nOne year\nNo\nMailed check\n56.95\n1889.5\nNo\n\n\n3\n3668-QPYBK\nMale\n0\nNo\nNo\n2\nYes\nNo\nDSL\nYes\nYes\nNo\nNo\nNo\nNo\nMonth-to-month\nYes\nMailed check\n53.85\n108.15\nYes\n\n\n4\n7795-CFOCW\nMale\n0\nNo\nNo\n45\nNo\nNo phone service\nDSL\nYes\nNo\nYes\nYes\nNo\nNo\nOne year\nNo\nBank transfer (automatic)\n42.3\n1840.75\nNo\n\n\n5\n9237-HQITU\nFemale\n0\nNo\nNo\n2\nYes\nNo\nFiber optic\nNo\nNo\nNo\nNo\nNo\nNo\nMonth-to-month\nYes\nElectronic check\n70.7\n151.65\nYes",
    "crumbs": [
      "Dataset",
      "6.telecom customer churn"
    ]
  },
  {
    "objectID": "dataset/telecom-churn.html#describe-dataframe",
    "href": "dataset/telecom-churn.html#describe-dataframe",
    "title": "📱📱📱telcom customer churn",
    "section": "2. describe dataframe",
    "text": "2. describe dataframe\n\ndescribe(df)\n\n21×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nDataType\n\n\n\n\n1\ncustomer_i_d\n\n0002-ORFBO\n\n9995-HOTOH\n0\nString15\n\n\n2\ngender\n\nFemale\n\nMale\n0\nString7\n\n\n3\nsenior_citizen\n0.162147\n0\n0.0\n1\n0\nInt64\n\n\n4\npartner\n\nNo\n\nYes\n0\nString3\n\n\n5\ndependents\n\nNo\n\nYes\n0\nString3\n\n\n6\ntenure\n32.3711\n0\n29.0\n72\n0\nInt64\n\n\n7\nphone_service\n\nNo\n\nYes\n0\nString3\n\n\n8\nmultiple_lines\n\nNo\n\nYes\n0\nString31\n\n\n9\ninternet_service\n\nDSL\n\nNo\n0\nString15\n\n\n10\nonline_security\n\nNo\n\nYes\n0\nString31\n\n\n11\nonline_backup\n\nNo\n\nYes\n0\nString31\n\n\n12\ndevice_protection\n\nNo\n\nYes\n0\nString31\n\n\n13\ntech_support\n\nNo\n\nYes\n0\nString31\n\n\n14\nstreaming_t_v\n\nNo\n\nYes\n0\nString31\n\n\n15\nstreaming_movies\n\nNo\n\nYes\n0\nString31\n\n\n16\ncontract\n\nMonth-to-month\n\nTwo year\n0\nString15\n\n\n17\npaperless_billing\n\nNo\n\nYes\n0\nString3\n\n\n18\npayment_method\n\nBank transfer (automatic)\n\nMailed check\n0\nString31\n\n\n19\nmonthly_charges\n64.7617\n18.25\n70.35\n118.75\n0\nFloat64\n\n\n20\ntotal_charges\n\n\n\n999.9\n0\nString7\n\n\n21\nchurn\n\nNo\n\nYes\n0\nString3",
    "crumbs": [
      "Dataset",
      "6.telecom customer churn"
    ]
  },
  {
    "objectID": "dataset/telecom-churn.html#eda",
    "href": "dataset/telecom-churn.html#eda",
    "title": "📱📱📱telcom customer churn",
    "section": "3. EDA",
    "text": "3. EDA\n\n3.1 gender of customer\n\ndf31=@chain df begin\n  @group_by(gender)\n  @summarize(count=n())\n  @ungroup\nend\n\n2×2 DataFrame\n\n\n\nRow\ngender\ncount\n\n\n\nString7\nInt64\n\n\n\n\n1\nFemale\n3488\n\n\n2\nMale\n3555\n\n\n\n\n\n\n\nlet\nax=(width=250,height=250)\ndatalayer=data(df31)\nmplayer=mapping(:gender,:count,color=:gender)\nvislayer=visual(BarPlot,strokewidth=1,strokcolor=:black,bar_labels=:y,flip_labels_at=2000)\nplt=datalayer*mplayer*vislayer\ndraw(plt,axis=ax)\nend\n\n\n\n\n\n\n\n\n\n\n3.2 churn of customers\n\ndf32=@chain df begin\n   @group_by(churn)\n   @summarize(count=n())\n   @ungroup\nend\n\n2×2 DataFrame\n\n\n\nRow\nchurn\ncount\n\n\n\nString3\nInt64\n\n\n\n\n1\nNo\n5174\n\n\n2\nYes\n1869\n\n\n\n\n\n\n\nlet\nax=(width=250,height=250)\ndatalayer=data(df32)\nmplayer=mapping(:churn,:count,color=:churn)\nvislayer=visual(BarPlot,strokewidth=1,strokcolor=:black,bar_labels=:y,flip_labels_at=2000)\nplt=datalayer*mplayer*vislayer\ndraw(plt,axis=ax)\nend\n\n\n\n\n\n\n\n\n\n\n3.3 relation of churn and tenure\n\nlet\nax=(width=250,height=250)\nplt=data(df)*mapping(:churn,:tenure,color=:churn)*visual(BoxPlot;)\ndraw(plt; axis=ax)\nend\n\n\n\n\n\n\n\n\n\n\n3.4 contract type and churn\n\nlet\nax=(width=400,height=250)\nplt=data(df)*mapping(:contract)*frequency()*mapping(color=:churn,dodge=:churn)\ndraw(plt; axis=ax)\nend\n\n\n\n\n\n\n\n\n\n\n3.5 Churn by Seniority\n\nlet\nax=(width=400,height=250)\nplt=data(df)*mapping(:senior_citizen)*frequency()*mapping(color=:churn,dodge=:churn)\ndraw(plt; axis=ax)\nend\n\n\n\n\n\n\n\n\n\n\n3.6 monthly_charges and churn\n\nlet \n ax=(width=400,height=250)\n plt=data(df)*mapping(:monthly_charges)*AlgebraOfGraphics.density(datalimits=((0, 140),))*mapping(color=:churn)\n draw(plt; axis=ax)\n\nend\n\n\n\n\n\n\n\n\n\n\n3.7 totalcharges and churn\n\n\ndf36=@chain df begin\n  @filter(total_charges != \" \")\n  coerce(_,:total_charges=&gt;Continuous)\n end;\n\n\nlet \n ax=(width=400,height=250)\n plt=data(df36)*mapping(:total_charges)*AlgebraOfGraphics.density()*mapping(color=:churn)\n draw(plt; axis=ax)\nend",
    "crumbs": [
      "Dataset",
      "6.telecom customer churn"
    ]
  },
  {
    "objectID": "learn/Classfication/1-diabetes-catboost-classification.html",
    "href": "learn/Classfication/1-diabetes-catboost-classification.html",
    "title": "1 Diabetes CatBoost Classification",
    "section": "",
    "text": "info\n\n\n\nref gradient-boosting-decision-trees/",
    "crumbs": [
      "1-Diabetes Catboost Classification"
    ]
  },
  {
    "objectID": "learn/Classfication/1-diabetes-catboost-classification.html#load-package",
    "href": "learn/Classfication/1-diabetes-catboost-classification.html#load-package",
    "title": "1 Diabetes CatBoost Classification",
    "section": "1. load package",
    "text": "1. load package\n\n import MLJ:transform,predict\n using CSV,DataFrames,Tidier,Pipe\n using CairoMakie,AlgebraOfGraphics,MakieThemes\n using MLJ,MLJModelInterface,Random\n #Makie.set_theme!(ggthemr(:flat))",
    "crumbs": [
      "1-Diabetes Catboost Classification"
    ]
  },
  {
    "objectID": "learn/Classfication/1-diabetes-catboost-classification.html#load-csv",
    "href": "learn/Classfication/1-diabetes-catboost-classification.html#load-csv",
    "title": "1 Diabetes CatBoost Classification",
    "section": "2. load csv",
    "text": "2. load csv\n\ndf=CSV.File(\"../../data/diabetes.csv\")|&gt;DataFrame\ndf=@chain  df  begin\n    @clean_names\n    coerce(_,:outcome=&gt; Multiclass,\n             :glucose =&gt;Continuous,\n             :blood_pressure=&gt;Continuous,\n             :skin_thickness=&gt;Continuous,\n             :insulin=&gt;Continuous,  \n             :b_m_i =&gt;Continuous                       \n    )\nend\nfirst(df,5)\n\n5×9 DataFrame\n\n\n\nRow\npregnancies\nglucose\nblood_pressure\nskin_thickness\ninsulin\nb_m_i\ndiabetes_pedigree_function\nage\noutcome\n\n\n\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nInt64\nCat…\n\n\n\n\n1\n6\n148.0\n72.0\n35.0\n0.0\n33.6\n0.627\n50\n1\n\n\n2\n1\n85.0\n66.0\n29.0\n0.0\n26.6\n0.351\n31\n0\n\n\n3\n8\n183.0\n64.0\n0.0\n0.0\n23.3\n0.672\n32\n1\n\n\n4\n1\n89.0\n66.0\n23.0\n94.0\n28.1\n0.167\n21\n0\n\n\n5\n0\n137.0\n40.0\n35.0\n168.0\n43.1\n2.288\n33\n1",
    "crumbs": [
      "1-Diabetes Catboost Classification"
    ]
  },
  {
    "objectID": "learn/Classfication/1-diabetes-catboost-classification.html#schema-of-df",
    "href": "learn/Classfication/1-diabetes-catboost-classification.html#schema-of-df",
    "title": "1 Diabetes CatBoost Classification",
    "section": "3. schema of df",
    "text": "3. schema of df\n\nschema(df)\n\n\n┌────────────────────────────┬───────────────┬──────────────────────────────────\n│ names                      │ scitypes      │ types                           ⋯\n├────────────────────────────┼───────────────┼──────────────────────────────────\n│ pregnancies                │ Count         │ Int64                           ⋯\n│ glucose                    │ Continuous    │ Float64                         ⋯\n│ blood_pressure             │ Continuous    │ Float64                         ⋯\n│ skin_thickness             │ Continuous    │ Float64                         ⋯\n│ insulin                    │ Continuous    │ Float64                         ⋯\n│ b_m_i                      │ Continuous    │ Float64                         ⋯\n│ diabetes_pedigree_function │ Continuous    │ Float64                         ⋯\n│ age                        │ Count         │ Int64                           ⋯\n│ outcome                    │ Multiclass{2} │ CategoricalValue{Int64, UInt32} ⋯\n└────────────────────────────┴───────────────┴──────────────────────────────────",
    "crumbs": [
      "1-Diabetes Catboost Classification"
    ]
  },
  {
    "objectID": "learn/Classfication/1-diabetes-catboost-classification.html#split-data",
    "href": "learn/Classfication/1-diabetes-catboost-classification.html#split-data",
    "title": "1 Diabetes CatBoost Classification",
    "section": "4. split data",
    "text": "4. split data\n\ny, X =  unpack(df, ==(:outcome), rng=123);\n#schema(X)\n(Xtrain, Xtest), (ytrain, ytest)  = partition((X, y), 0.7, multi=true,  rng=123)\n\n\n((538×8 DataFrame\n Row │ pregnancies  glucose  blood_pressure  skin_thickness  insulin  b_m_i    ⋯\n     │ Int64        Float64  Float64         Float64         Float64  Float64  ⋯\n─────┼──────────────────────────────────────────────────────────────────────────\n   1 │           0    126.0            86.0            27.0    120.0     27.4  ⋯\n   2 │           0    120.0            74.0            18.0     63.0     30.5\n   3 │           4     99.0            76.0            15.0     51.0     23.2\n   4 │           2    118.0            80.0             0.0      0.0     42.9\n   5 │           1     86.0            66.0            52.0     65.0     41.3  ⋯\n   6 │           3    191.0            68.0            15.0    130.0     30.9\n   7 │           2    108.0            62.0            10.0    278.0     25.3\n   8 │           6     99.0            60.0            19.0     54.0     26.9\n   9 │          11    143.0            94.0            33.0    146.0     36.6  ⋯\n  10 │           5    112.0            66.0             0.0      0.0     37.8\n  11 │           3    125.0            58.0             0.0      0.0     31.6\n  ⋮  │      ⋮          ⋮           ⋮               ⋮            ⋮        ⋮     ⋱\n 529 │           3     88.0            58.0            11.0     54.0     24.8\n 530 │           0    188.0            82.0            14.0    185.0     32.0  ⋯\n 531 │           4    110.0            66.0             0.0      0.0     31.9\n 532 │           0     67.0            76.0             0.0      0.0     45.3\n 533 │           0    125.0            68.0             0.0      0.0     24.7\n 534 │           7    196.0            90.0             0.0      0.0     39.8  ⋯\n 535 │           3    150.0            76.0             0.0      0.0     21.0\n 536 │           6     87.0            80.0             0.0      0.0     23.2\n 537 │           0    146.0            70.0             0.0      0.0     37.9\n 538 │           0     78.0            88.0            29.0     40.0     36.9  ⋯\n                                                  2 columns and 517 rows omitted, 230×8 DataFrame\n Row │ pregnancies  glucose  blood_pressure  skin_thickness  insulin  b_m_i    ⋯\n     │ Int64        Float64  Float64         Float64         Float64  Float64  ⋯\n─────┼──────────────────────────────────────────────────────────────────────────\n   1 │           0    139.0            62.0            17.0    210.0     22.1  ⋯\n   2 │           2     94.0            68.0            18.0     76.0     26.0\n   3 │          12    100.0            84.0            33.0    105.0     30.0\n   4 │           8    133.0            72.0             0.0      0.0     32.9\n   5 │           0    137.0            84.0            27.0      0.0     27.3  ⋯\n   6 │           6    108.0            44.0            20.0    130.0     24.0\n   7 │          13    152.0            90.0            33.0     29.0     26.8\n   8 │           5    122.0            86.0             0.0      0.0     34.7\n   9 │           4    131.0            68.0            21.0    166.0     33.1  ⋯\n  10 │           9    122.0            56.0             0.0      0.0     33.3\n  11 │           8    167.0           106.0            46.0    231.0     37.6\n  ⋮  │      ⋮          ⋮           ⋮               ⋮            ⋮        ⋮     ⋱\n 221 │           7    152.0            88.0            44.0      0.0     50.0\n 222 │           0     94.0             0.0             0.0      0.0      0.0  ⋯\n 223 │           3     87.0            60.0            18.0      0.0     21.8\n 224 │           6    194.0            78.0             0.0      0.0     23.5\n 225 │           3    111.0            62.0             0.0      0.0     22.6\n 226 │           7    142.0            90.0            24.0    480.0     30.4  ⋯\n 227 │           0     93.0           100.0            39.0     72.0     43.4\n 228 │           4    146.0            78.0             0.0      0.0     38.5\n 229 │          11    136.0            84.0            35.0    130.0     28.3\n 230 │           0     74.0            52.0            10.0     36.0     27.8  ⋯\n                                                  2 columns and 209 rows omitted), (CategoricalArrays.CategoricalValue{Int64, UInt32}[0, 0, 0, 1, 0, 0, 0, 0, 1, 1  …  0, 1, 0, 0, 0, 1, 0, 0, 1, 0], CategoricalArrays.CategoricalValue{Int64, UInt32}[0, 0, 0, 1, 0, 0, 1, 0, 0, 1  …  1, 0, 0, 1, 0, 1, 0, 1, 1, 0]))",
    "crumbs": [
      "1-Diabetes Catboost Classification"
    ]
  },
  {
    "objectID": "learn/Classfication/1-diabetes-catboost-classification.html#catboost-workflow",
    "href": "learn/Classfication/1-diabetes-catboost-classification.html#catboost-workflow",
    "title": "1 Diabetes CatBoost Classification",
    "section": "5. catboost workflow",
    "text": "5. catboost workflow\n\n5.1 load model\n\nusing CatBoost.MLJCatBoostInterface\ncatboost = CatBoostClassifier(iterations=5,learning_rate=0.20,depth=6,loss_function=\"Logloss\",);\n\n\n\n  \n     CatBoostClassifier \n     \n     \n \n \n doc\n \n \n \n No documentation found.\ncatboost is of type CatBoostClassifier.\nSummary\nmutable struct CatBoostClassifier\nFields\niterations                     :: Int64\nlearning_rate                  :: Float64\ndepth                          :: Int64\nl2_leaf_reg                    :: Float64\nmodel_size_reg                 :: Float64\nrsm                            :: Float64\nloss_function                  :: Union{Nothing, String}\nborder_count                   :: Union{Nothing, Int64}\nfeature_border_type            :: Union{Nothing, String}\nper_float_feature_quantization :: Union{Nothing, PythonCall.Py}\ninput_borders                  :: Union{Nothing, String}\noutput_borders                 :: Union{Nothing, String}\nfold_permutation_block         :: Int64\nnan_mode                       :: String\ncounter_calc_method            :: String\nleaf_estimation_iterations     :: Union{Nothing, Int64}\nleaf_estimation_method         :: Union{Nothing, String}\nthread_count                   :: Int64\nrandom_seed                    :: Union{Nothing, Int64}\nmetric_period                  :: Int64\nctr_leaf_count_limit           :: Union{Nothing, Int64}\nstore_all_simple_ctr           :: Bool\nmax_ctr_complexity             :: Union{Nothing, Bool}\nhas_time                       :: Bool\nallow_const_label              :: Bool\ntarget_border                  :: Union{Nothing, Float64}\nclass_weights                  :: Union{Nothing, PythonCall.Py}\nauto_class_weights             :: Union{Nothing, Bool}\none_hot_max_size               :: Union{Nothing, Int64}\nrandom_strength                :: Float64\nbagging_temperature            :: Float64\nfold_len_multiplier            :: Float64\nused_ram_limit                 :: Union{Nothing, Int64}\ngpu_ram_part                   :: Float64\npinned_memory_size             :: Int64\nallow_writing_files            :: Union{Nothing, Bool}\napprox_on_full_history         :: Bool\nboosting_type                  :: Union{Nothing, String}\nsimple_ctr                     :: Union{Nothing, PythonCall.Py}\ncombinations_ctr               :: Union{Nothing, PythonCall.Py}\nper_feature_ctr                :: Union{Nothing, PythonCall.Py}\ntask_type                      :: Union{Nothing, String}\ndevices                        :: Union{Nothing, String}\nbootstrap_type                 :: Union{Nothing, String}\nsubsample                      :: Union{Nothing, Int64}\nsampling_frequency             :: String\nsampling_unit                  :: String\ngpu_cat_features_storage       :: String\ndata_partition                 :: Union{Nothing, String}\nearly_stopping_rounds          :: Union{Nothing, Int64}\ngrow_policy                    :: String\nmin_data_in_leaf               :: Int64\nmax_leaves                     :: Int64\nleaf_estimation_backtracking   :: String\nfeature_weights                :: Union{Nothing, PythonCall.Py}\npenalties_coefficient          :: Float64\nmodel_shrink_rate              :: Union{Nothing, Float64}\nmodel_shrink_mode              :: String\nlangevin                       :: Bool\ndiffusion_temperature          :: Float64\nposterior_sampling             :: Bool\nboost_from_average             :: Union{Nothing, Bool}\ntext_processing                :: Union{Nothing, PythonCall.Py}\nSupertype Hierarchy\nCatBoostClassifier &lt;: Probabilistic &lt;: Supervised &lt;: Model &lt;: MLJType &lt;: Any\n\n \n \n \n\n\n\n\n5.2 fitting model ,predict test data\n\nmach = machine(catboost, Xtrain, ytrain)|&gt;fit!\n\n# #probs = predict(mach, Xtrain)\nyhat = predict_mode(mach, Xtest) \nshow(yhat)\n\n[ Info: Training machine(CatBoostClassifier(iterations = 5, …), …).\n\n\n[0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n\n\n\n\n5.3 performance\n\naccuracy(yhat,ytest)\n\n0.782608695652174",
    "crumbs": [
      "1-Diabetes Catboost Classification"
    ]
  },
  {
    "objectID": "learn/Classfication/index.html",
    "href": "learn/Classfication/index.html",
    "title": "Classfication Introduction",
    "section": "",
    "text": "Classfication Introduction",
    "crumbs": [
      "Classfication Intro"
    ]
  },
  {
    "objectID": "learn/Clustering/2-iris-kernelpca-kmeans.html",
    "href": "learn/Clustering/2-iris-kernelpca-kmeans.html",
    "title": "2 IRIS KernnelPCA-Kmeans clustering",
    "section": "",
    "text": "info\n\n\n\nusing kernel methods mapping dataset to feature space",
    "crumbs": [
      "2-IRIS KernelPCA-Kmeans clustering"
    ]
  },
  {
    "objectID": "learn/Clustering/2-iris-kernelpca-kmeans.html#load-package",
    "href": "learn/Clustering/2-iris-kernelpca-kmeans.html#load-package",
    "title": "2 IRIS KernnelPCA-Kmeans clustering",
    "section": "1. load package",
    "text": "1. load package\n\n using CSV,DataFrames,Tidier,Pipe\n using CairoMakie,AlgebraOfGraphics,MakieThemes\n using MLJ,MLJModelInterface,Random\n import MLJ:transform,predict\n using LinearAlgebra\n #Makie.set_theme!(ggthemr(:flat))",
    "crumbs": [
      "2-IRIS KernelPCA-Kmeans clustering"
    ]
  },
  {
    "objectID": "learn/Clustering/2-iris-kernelpca-kmeans.html#load-csv",
    "href": "learn/Clustering/2-iris-kernelpca-kmeans.html#load-csv",
    "title": "2 IRIS KernnelPCA-Kmeans clustering",
    "section": "2. load csv",
    "text": "2. load csv\n\ndf=CSV.File(\"../../data/iris.csv\")|&gt;DataFrame\ndf=@chain  df  begin\n    @clean_names\nend;",
    "crumbs": [
      "2-IRIS KernelPCA-Kmeans clustering"
    ]
  },
  {
    "objectID": "learn/Clustering/2-iris-kernelpca-kmeans.html#data-processing",
    "href": "learn/Clustering/2-iris-kernelpca-kmeans.html#data-processing",
    "title": "2 IRIS KernnelPCA-Kmeans clustering",
    "section": "3. data processing",
    "text": "3. data processing\n\n    coerce!(df,:labels=&gt;Multiclass)\n    ytrain, Xtrain=  unpack(df, ==(:species), rng=123)\n    cat=levels(ytrain)\n    rows,cols=size(Xtrain)\n\n(150, 4)",
    "crumbs": [
      "2-IRIS KernelPCA-Kmeans clustering"
    ]
  },
  {
    "objectID": "learn/Clustering/2-iris-kernelpca-kmeans.html#mlj-workflow",
    "href": "learn/Clustering/2-iris-kernelpca-kmeans.html#mlj-workflow",
    "title": "2 IRIS KernnelPCA-Kmeans clustering",
    "section": "4. MLJ workflow",
    "text": "4. MLJ workflow\n\n4.1 load model\nusing KernelPCA from MultivariateStats.jl package\n\nKernelPCA doc \n\n\n\n\nKernelPCA\n\n\n\n\n\n\n@doc(KernelPCA)\n\nKernelPCA\nA model type for constructing a kernel prinicipal component analysis model, based on unknown.jl, and implementing the MLJ model interface.\nFrom MLJ, the type can be imported using\nKernelPCA = @load KernelPCA pkg=unknown\nDo model = KernelPCA() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in KernelPCA(maxoutdim=...).\nIn kernel PCA the linear operations of ordinary principal component analysis are performed in a reproducing Hilbert space.\n\nTraining data\nIn MLJ or MLJBase, bind an instance model to data with\nmach = machine(model, X)\nHere:\n\nX is any table of input features (eg, a DataFrame) whose columns are of scitype Continuous; check column scitypes with schema(X).\n\nTrain the machine using fit!(mach, rows=...).\n\n\nHyper-parameters\n\nmaxoutdim=0: Controls the the dimension (number of columns) of the output, outdim. Specifically, outdim = min(n, indim, maxoutdim), where n is the number of observations and indim the input dimension.\nkernel::Function=(x,y)-&gt;x'y: The kernel function, takes in 2 vector arguments x and y, returns a scalar value. Defaults to the dot product of x and y.\nsolver::Symbol=:eig: solver to use for the eigenvalues, one of :eig(default, uses LinearAlgebra.eigen), :eigs(uses Arpack.eigs).\ninverse::Bool=true: perform calculations needed for inverse transform\nbeta::Real=1.0: strength of the ridge regression that learns the inverse transform when inverse is true.\ntol::Real=0.0: Convergence tolerance for eigenvalue solver.\nmaxiter::Int=300: maximum number of iterations for eigenvalue solver.\n\n\n\nOperations\n\ntransform(mach, Xnew): Return a lower dimensional projection of the input Xnew, which should have the same scitype as X above.\ninverse_transform(mach, Xsmall): For a dimension-reduced table Xsmall, such as returned by transform, reconstruct a table, having same the number of columns as the original training data X, that transforms to Xsmall. Mathematically, inverse_transform is a right-inverse for the PCA projection map, whose image is orthogonal to the kernel of that map. In particular, if Xsmall = transform(mach, Xnew), then inverse_transform(Xsmall) is only an approximation to Xnew.\n\n\n\nFitted parameters\nThe fields of fitted_params(mach) are:\n\nprojection: Returns the projection matrix, which has size (indim, outdim), where indim and outdim are the number of features of the input and ouput respectively.\n\n\n\nReport\nThe fields of report(mach) are:\n\nindim: Dimension (number of columns) of the training data and new data to be transformed.\noutdim: Dimension of transformed data.\nprincipalvars: The variance of the principal components.\n\n\n\nExamples\nusing MLJ\nusing LinearAlgebra\n\nKernelPCA = @load KernelPCA pkg=MultivariateStats\n\nX, y = @load_iris # a table and a vector\n\nfunction rbf_kernel(length_scale)\n    return (x,y) -&gt; norm(x-y)^2 / ((2 * length_scale)^2)\nend\n\nmodel = KernelPCA(maxoutdim=2, kernel=rbf_kernel(1))\nmach = machine(model, X) |&gt; fit!\n\nXproj = transform(mach, X)\nSee also PCA, ICA, FactorAnalysis, PPCA\n\n\n\n\n\n\nKernelPCA = @load KernelPCA pkg=MultivariateStats\nKMeans = @load KMeans pkg=Clustering\n\nimport MLJMultivariateStatsInterface ✔\nimport MLJClusteringInterface ✔\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: For silent loading, specify `verbosity=0`. \n\n\nMLJClusteringInterface.KMeans\n\n\n\n\n4.2 defin rbs function\nchange params length_scale\n\nfunction rbf_kernel(length_scale)\n    return (x,y) -&gt; norm(x-y)^2 / ((2 * length_scale)^2)\nend\n\nrbf_kernel (generic function with 1 method)\n\n\n\n\n4.3 define tunning function\n\nfunction tune(i)\n    model1 = KernelPCA(maxoutdim=3, kernel=rbf_kernel(i))\n    mach1 = machine(model1, Xtrain) |&gt; fit!\n    model2 = KMeans(k=3)\n    Xproj = MLJ.transform(mach1, Xtrain)\n    mach2 = machine(model2, Xproj) |&gt; fit!\n    yhat = MLJ.predict(mach2, Xproj)\n    return i,Xproj,yhat\nend\n\ntune (generic function with 1 method)\n\n\n\nlength_scales=1:9\nyhat_arr=[tune(i) for i in length_scales];\n\n[ Info: Training machine(KernelPCA(maxoutdim = 3, …), …).\n[ Info: Training machine(KMeans(k = 3, …), …).\n[ Info: Training machine(KernelPCA(maxoutdim = 3, …), …).\n[ Info: Training machine(KMeans(k = 3, …), …).\n[ Info: Training machine(KernelPCA(maxoutdim = 3, …), …).\n[ Info: Training machine(KMeans(k = 3, …), …).\n[ Info: Training machine(KernelPCA(maxoutdim = 3, …), …).\n[ Info: Training machine(KMeans(k = 3, …), …).\n[ Info: Training machine(KernelPCA(maxoutdim = 3, …), …).\n[ Info: Training machine(KMeans(k = 3, …), …).\n[ Info: Training machine(KernelPCA(maxoutdim = 3, …), …).\n[ Info: Training machine(KMeans(k = 3, …), …).\n[ Info: Training machine(KernelPCA(maxoutdim = 3, …), …).\n[ Info: Training machine(KMeans(k = 3, …), …).\n[ Info: Training machine(KernelPCA(maxoutdim = 3, …), …).\n[ Info: Training machine(KMeans(k = 3, …), …).\n[ Info: Training machine(KernelPCA(maxoutdim = 3, …), …).\n[ Info: Training machine(KMeans(k = 3, …), …).",
    "crumbs": [
      "2-IRIS KernelPCA-Kmeans clustering"
    ]
  },
  {
    "objectID": "learn/Clustering/2-iris-kernelpca-kmeans.html#plot-results",
    "href": "learn/Clustering/2-iris-kernelpca-kmeans.html#plot-results",
    "title": "2 IRIS KernnelPCA-Kmeans clustering",
    "section": "5 plot results",
    "text": "5 plot results\n\n5.1 two pcs\n\nfig=Figure()\nfor i in length_scales\n   ls,df,label=yhat_arr[i]\n   row,col=fldmod1(i,3)\n   ax=Axis(fig[row,col],subtitle=\"len_scale=$(ls)\")\n   table=(pc1=df.x1,pc2=df.x2,label=label)\n   datalayer=data(table)\n   maplayer=mapping(:pc1,:pc2,color=:label)\n   vislayer=visual(Scatter,markersize=8,strokewidth=1,strokecolor=:black)\n   plt=datalayer*maplayer*vislayer\n   hidedecorations!(ax)\n   draw!(ax, plt)\nend\nfig\n\n\n\n\n\n\n\n\n\nfig=Figure()\nfor i in length_scales\n   ls,df,label=yhat_arr[i]\n   row,col=fldmod1(i,3)\n   ax=Axis3(fig[row,col],title=\"len_scale=$(ls)\")\n   table=(pc1=df.x1,pc2=df.x2,pc3=df.x3,label=label)\n   datalayer=data(table)\n   maplayer=mapping(:pc1,:pc2,:pc3,color=:label)\n   vislayer=visual(Scatter,markersize=8,strokewidth=1,strokecolor=:black)\n   plt=datalayer*maplayer*vislayer\n   hidedecorations!(ax)\n   draw!(ax, plt)\n   \nend\nfig",
    "crumbs": [
      "2-IRIS KernelPCA-Kmeans clustering"
    ]
  },
  {
    "objectID": "learn/Dimension/1-nir-spectra-milk.html",
    "href": "learn/Dimension/1-nir-spectra-milk.html",
    "title": "1 NIR Spectra of Milk",
    "section": "",
    "text": "info\n\n\n\nref : Classification of NIR spectra using Principal Component Analysis in Python\n在全频谱范围内的检测数据很有可能存在冗余,因为相邻谱线检测的度量值可能反映的同一个属性. 因此可以使用 PCA 分析方法降低检测数据的维度",
    "crumbs": [
      "1.NIR Spectra of Milk"
    ]
  },
  {
    "objectID": "learn/Dimension/1-nir-spectra-milk.html#load-package",
    "href": "learn/Dimension/1-nir-spectra-milk.html#load-package",
    "title": "1 NIR Spectra of Milk",
    "section": "1. load package",
    "text": "1. load package\n\n import MLJ:transform,predict\n using CSV,DataFrames,Tidier,Pipe\n using CairoMakie,AlgebraOfGraphics,MakieThemes\n using MLJ,MLJModelInterface,Random\n #Makie.set_theme!(ggthemr(:flat))",
    "crumbs": [
      "1.NIR Spectra of Milk"
    ]
  },
  {
    "objectID": "learn/Dimension/1-nir-spectra-milk.html#load-csv",
    "href": "learn/Dimension/1-nir-spectra-milk.html#load-csv",
    "title": "1 NIR Spectra of Milk",
    "section": "2. load csv",
    "text": "2. load csv\n\ndf=CSV.File(\"../../data/nir-spectra-milk.csv\")|&gt;DataFrame\ndf=@chain  df  begin\n    @clean_names\nend\nfirst(df,5)\n\n5×603 DataFrame503 columns omitted\n\n\n\nRow\ncolumn1\nlabels\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n⋯\n\n\n\nString15\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\n⋯\n\n\n\n\n1\n1/02/2018\n1\n2.39753\n2.3942\n2.38895\n2.38128\n2.37191\n2.36094\n2.34909\n2.33675\n2.32383\n2.30971\n2.29375\n2.27532\n2.25373\n2.22825\n2.19869\n2.16501\n2.12755\n2.08735\n2.0453\n2.00258\n1.95983\n1.91818\n1.87829\n1.84078\n1.80598\n1.77438\n1.74615\n1.72117\n1.69931\n1.68041\n1.66405\n1.65001\n1.6378\n1.6275\n1.61876\n1.61132\n1.60516\n1.59983\n1.5951\n1.59059\n1.58612\n1.5818\n1.57757\n1.57357\n1.56972\n1.56607\n1.56265\n1.55942\n1.55642\n1.55372\n1.55133\n1.54911\n1.54703\n1.54519\n1.54358\n1.54237\n1.54159\n1.54116\n1.54116\n1.54145\n1.54202\n1.54291\n1.54406\n1.5453\n1.54653\n1.54758\n1.54866\n1.54986\n1.55122\n1.55296\n1.55495\n1.55694\n1.55884\n1.56038\n1.56144\n1.56212\n1.56237\n1.56225\n1.5615\n1.56006\n1.55796\n1.5552\n1.552\n1.54864\n1.54536\n1.54231\n1.53927\n1.53626\n1.53293\n1.52915\n1.52472\n1.51945\n1.51336\n1.50644\n1.49855\n1.48957\n1.47928\n1.46769\n⋯\n\n\n2\n1/02/2018.1\n1\n2.39953\n2.39672\n2.39168\n2.38328\n2.37282\n2.36116\n2.34854\n2.33635\n2.32436\n2.31153\n2.29693\n2.27919\n2.25795\n2.23234\n2.20204\n2.16763\n2.12913\n2.08767\n2.04446\n2.00054\n1.957\n1.91494\n1.87484\n1.83739\n1.80299\n1.77194\n1.74441\n1.72002\n1.69886\n1.68052\n1.66447\n1.65057\n1.63856\n1.62809\n1.61911\n1.61122\n1.6043\n1.59809\n1.59243\n1.58731\n1.58272\n1.57838\n1.57426\n1.57032\n1.56641\n1.56272\n1.5593\n1.5564\n1.55416\n1.5522\n1.55054\n1.54904\n1.54754\n1.54596\n1.54428\n1.54276\n1.54154\n1.5405\n1.54005\n1.54011\n1.54063\n1.54154\n1.54263\n1.54386\n1.54516\n1.54637\n1.54776\n1.54926\n1.55094\n1.55289\n1.55495\n1.55709\n1.55911\n1.56082\n1.56214\n1.56297\n1.56332\n1.56318\n1.56242\n1.56104\n1.55903\n1.55636\n1.55316\n1.54962\n1.54597\n1.54234\n1.53891\n1.53554\n1.53214\n1.52839\n1.52415\n1.51917\n1.51333\n1.50651\n1.49868\n1.48974\n1.47952\n1.46797\n⋯\n\n\n3\n1/02/2018.2\n1\n2.39647\n2.3936\n2.38845\n2.38099\n2.37132\n2.35993\n2.34811\n2.33635\n2.32446\n2.3115\n2.29619\n2.27764\n2.25522\n2.22877\n2.19835\n2.16399\n2.12632\n2.08603\n2.04386\n2.00081\n1.95794\n1.91628\n1.87625\n1.83868\n1.80421\n1.77314\n1.74554\n1.72122\n1.69974\n1.68103\n1.66475\n1.65072\n1.63884\n1.62889\n1.62055\n1.61334\n1.60698\n1.60136\n1.5961\n1.5911\n1.5863\n1.58153\n1.57689\n1.57237\n1.5682\n1.56448\n1.56114\n1.55825\n1.55571\n1.55338\n1.55137\n1.5495\n1.54781\n1.54615\n1.54467\n1.54339\n1.5424\n1.54185\n1.54178\n1.54207\n1.54268\n1.54334\n1.54407\n1.54484\n1.54569\n1.54667\n1.54784\n1.54923\n1.55093\n1.55288\n1.555\n1.55721\n1.55931\n1.56103\n1.56229\n1.56291\n1.56305\n1.56267\n1.56183\n1.5605\n1.55858\n1.55614\n1.55327\n1.5501\n1.54687\n1.54363\n1.54043\n1.53708\n1.53339\n1.52924\n1.52439\n1.51887\n1.51259\n1.50551\n1.49762\n1.4887\n1.4786\n1.46721\n⋯\n\n\n4\n1/02/2018.3\n1\n2.40688\n2.40424\n2.3992\n2.39114\n2.38054\n2.36857\n2.35634\n2.34405\n2.33153\n2.31797\n2.30259\n2.28396\n2.26211\n2.23634\n2.20659\n2.17255\n2.1348\n2.09384\n2.05097\n2.00741\n1.96439\n1.9222\n1.88192\n1.8439\n1.80882\n1.77683\n1.74832\n1.72348\n1.70202\n1.68354\n1.66778\n1.65442\n1.64315\n1.63344\n1.62511\n1.61791\n1.61168\n1.6062\n1.60137\n1.59691\n1.59266\n1.58823\n1.58372\n1.57906\n1.57448\n1.57023\n1.56651\n1.5632\n1.56034\n1.5578\n1.55561\n1.55371\n1.55202\n1.55071\n1.54959\n1.54857\n1.54776\n1.54717\n1.54693\n1.54715\n1.54776\n1.54868\n1.54974\n1.55077\n1.55177\n1.55282\n1.5539\n1.55508\n1.55643\n1.55789\n1.55934\n1.56085\n1.56242\n1.5641\n1.56577\n1.56717\n1.56826\n1.56873\n1.56832\n1.56701\n1.56485\n1.56201\n1.55865\n1.55508\n1.55154\n1.54815\n1.54492\n1.54169\n1.53825\n1.53445\n1.53002\n1.52496\n1.51904\n1.51225\n1.50436\n1.49529\n1.48484\n1.4731\n⋯\n\n\n5\n1/02/2018.4\n1\n2.40988\n2.40702\n2.40131\n2.39267\n2.38137\n2.3686\n2.35552\n2.34279\n2.33123\n2.31932\n2.30561\n2.2889\n2.26787\n2.2421\n2.21174\n2.17669\n2.13787\n2.09599\n2.05236\n2.00855\n1.96539\n1.92386\n1.88469\n1.8481\n1.8141\n1.7832\n1.75517\n1.73034\n1.70849\n1.68946\n1.67313\n1.65908\n1.64709\n1.63696\n1.62814\n1.62052\n1.61401\n1.60831\n1.60316\n1.59844\n1.59383\n1.58932\n1.58481\n1.58043\n1.57648\n1.5729\n1.56975\n1.56697\n1.56438\n1.56207\n1.55983\n1.55779\n1.55587\n1.55403\n1.55245\n1.55101\n1.55001\n1.54934\n1.54917\n1.54943\n1.55003\n1.55074\n1.55161\n1.55259\n1.55369\n1.55489\n1.55635\n1.55799\n1.55985\n1.56166\n1.56343\n1.56506\n1.56649\n1.56767\n1.56875\n1.56963\n1.57023\n1.57043\n1.57005\n1.56886\n1.5668\n1.56399\n1.56073\n1.55728\n1.55384\n1.5506\n1.5475\n1.54435\n1.54093\n1.53714\n1.53273\n1.52766\n1.52176\n1.51488\n1.50685\n1.49765\n1.4871\n1.4754\n⋯",
    "crumbs": [
      "1.NIR Spectra of Milk"
    ]
  },
  {
    "objectID": "learn/Dimension/1-nir-spectra-milk.html#describe-df",
    "href": "learn/Dimension/1-nir-spectra-milk.html#describe-df",
    "title": "1 NIR Spectra of Milk",
    "section": "3. describe df",
    "text": "3. describe df\n\ndescribe(df)\n\n603×7 DataFrame578 rows omitted\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nDataType\n\n\n\n\n1\ncolumn1\n\n1/02/2018\n\n1/02/2018.9\n0\nString15\n\n\n2\nlabels\n5.0\n1\n5.0\n9\n0\nInt64\n\n\n3\n2\n2.32107\n2.26879\n2.31618\n2.4278\n0\nFloat64\n\n\n4\n3\n2.31841\n2.2665\n2.31326\n2.42489\n0\nFloat64\n\n\n5\n4\n2.31322\n2.2613\n2.30787\n2.41844\n0\nFloat64\n\n\n6\n5\n2.30525\n2.25338\n2.30009\n2.40855\n0\nFloat64\n\n\n7\n6\n2.295\n2.24273\n2.29005\n2.39609\n0\nFloat64\n\n\n8\n7\n2.28338\n2.23087\n2.27877\n2.38277\n0\nFloat64\n\n\n9\n8\n2.27142\n2.21857\n2.2673\n2.36939\n0\nFloat64\n\n\n10\n9\n2.25971\n2.20678\n2.25572\n2.35651\n0\nFloat64\n\n\n11\n10\n2.24804\n2.19569\n2.24402\n2.34441\n0\nFloat64\n\n\n12\n11\n2.23571\n2.18358\n2.23136\n2.332\n0\nFloat64\n\n\n13\n12\n2.2216\n2.16986\n2.21734\n2.31786\n0\nFloat64\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n592\n591\n0.111289\n0.102479\n0.110969\n0.126853\n0\nFloat64\n\n\n593\n592\n0.112013\n0.103186\n0.11167\n0.127408\n0\nFloat64\n\n\n594\n593\n0.113108\n0.104336\n0.112797\n0.128351\n0\nFloat64\n\n\n595\n594\n0.114627\n0.105957\n0.11418\n0.129728\n0\nFloat64\n\n\n596\n595\n0.11661\n0.107904\n0.116109\n0.131623\n0\nFloat64\n\n\n597\n596\n0.119091\n0.110365\n0.11858\n0.134025\n0\nFloat64\n\n\n598\n597\n0.1221\n0.113292\n0.121513\n0.136916\n0\nFloat64\n\n\n599\n598\n0.12565\n0.116627\n0.12504\n0.1403\n0\nFloat64\n\n\n600\n599\n0.129757\n0.120672\n0.129096\n0.14466\n0\nFloat64\n\n\n601\n600\n0.134425\n0.125333\n0.133807\n0.149401\n0\nFloat64\n\n\n602\n601\n0.139656\n0.13064\n0.139068\n0.154445\n0\nFloat64\n\n\n603\n602\n0.145435\n0.136488\n0.144987\n0.160379\n0\nFloat64",
    "crumbs": [
      "1.NIR Spectra of Milk"
    ]
  },
  {
    "objectID": "learn/Dimension/1-nir-spectra-milk.html#data-processing",
    "href": "learn/Dimension/1-nir-spectra-milk.html#data-processing",
    "title": "1 NIR Spectra of Milk",
    "section": "4. data processing",
    "text": "4. data processing\n\n    coerce!(df,:labels=&gt;Multiclass)\n    ytrain, Xtrain=  unpack(df, ==(:labels),!=(:column1), rng=123)\n    cat=levels(ytrain)\n    rows,cols=size(Xtrain)\n\n(450, 601)",
    "crumbs": [
      "1.NIR Spectra of Milk"
    ]
  },
  {
    "objectID": "learn/Dimension/1-nir-spectra-milk.html#pca-workflow",
    "href": "learn/Dimension/1-nir-spectra-milk.html#pca-workflow",
    "title": "1 NIR Spectra of Milk",
    "section": "5. pca workflow",
    "text": "5. pca workflow\n\nPCA = @load PCA pkg=MultivariateStats\nmaxdim=6\nmodel=PCA(maxoutdim=maxdim)\nmach = machine(model, Xtrain) |&gt; fit!\nYtr =transform(mach, Xtrain)\n\nimport MLJMultivariateStatsInterface ✔\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: Training machine(PCA(maxoutdim = 6, …), …).\n\n\n450×3 DataFrame425 rows omitted\n\n\n\nRow\nx1\nx2\nx3\n\n\n\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n-0.11799\n0.124592\n-0.0436853\n\n\n2\n-0.202372\n0.121133\n-0.0536663\n\n\n3\n0.0229564\n-0.023132\n-0.00146871\n\n\n4\n0.15374\n0.0578446\n-0.0032877\n\n\n5\n-0.146434\n-0.0390585\n-0.015652\n\n\n6\n-0.182492\n0.221615\n0.0234858\n\n\n7\n0.0357099\n-0.0664287\n0.00560643\n\n\n8\n-0.449471\n-0.227503\n0.0231776\n\n\n9\n0.18417\n0.0521712\n0.00049485\n\n\n10\n0.43951\n-0.111014\n0.0372552\n\n\n11\n0.179981\n-0.0794656\n-0.0206554\n\n\n12\n0.1185\n-0.0962695\n-0.00643676\n\n\n13\n-0.0548056\n0.00447843\n0.00556863\n\n\n⋮\n⋮\n⋮\n⋮\n\n\n439\n-0.173193\n0.219183\n0.0277421\n\n\n440\n-0.194956\n0.152946\n-0.0407209\n\n\n441\n-0.0691982\n0.0651594\n-0.0220876\n\n\n442\n0.0574283\n-0.0991685\n-0.0182021\n\n\n443\n0.040947\n0.01118\n0.00350249\n\n\n444\n0.128935\n0.0565545\n-0.00341345\n\n\n445\n-0.0507774\n-0.0914194\n-0.0112601\n\n\n446\n0.287571\n-0.0619512\n-0.0149125\n\n\n447\n0.0562878\n-0.0871696\n-0.00901997\n\n\n448\n0.100222\n0.0684576\n-0.0227443\n\n\n449\n-0.36232\n0.260408\n0.0649335\n\n\n450\n0.233495\n0.0890649\n0.0104651",
    "crumbs": [
      "1.NIR Spectra of Milk"
    ]
  },
  {
    "objectID": "learn/Dimension/1-nir-spectra-milk.html#report-mach",
    "href": "learn/Dimension/1-nir-spectra-milk.html#report-mach",
    "title": "1 NIR Spectra of Milk",
    "section": "6. report mach",
    "text": "6. report mach\n\nreport(mach)\n\n(indim = 601,\n outdim = 3,\n tprincipalvar = 0.10106321733774248,\n tresidualvar = 0.0004884449234925647,\n tvar = 0.10155166226123505,\n mean = [2.32107355, 2.3184108062666664, 2.3132191630222225, 2.305245434466667, 2.2949972109777774, 2.283383981711111, 2.2714193757777776, 2.259710103377777, 2.2480409782222215, 2.2357105499333327  …  0.11310844424222223, 0.11462666989555556, 0.11660962977333336, 0.11909146695777777, 0.1220999842266667, 0.12564987484, 0.12975719079999998, 0.13442543589777778, 0.13965588253333336, 0.14543503400888888],\n principalvars = [0.08224441321648794, 0.01789077556081856, 0.0009280285604359924],)",
    "crumbs": [
      "1.NIR Spectra of Milk"
    ]
  },
  {
    "objectID": "learn/Dimension/1-nir-spectra-milk.html#plot-results",
    "href": "learn/Dimension/1-nir-spectra-milk.html#plot-results",
    "title": "1 NIR Spectra of Milk",
    "section": "7. plot results",
    "text": "7. plot results\n\n7.1 2 pcs\n\n  ax=(width=400, height=300)\n  table=(pc1=Ytr.x1,pc2=Ytr.x2,cat=ytrain)\n  datalayer=data(table)\n  maplayer=mapping(:pc1,:pc2,color=:cat)\n  vislayer=visual(Scatter,markersize=10,strokewidth=1,strokecolor=:black)\n  plt=datalayer*maplayer*vislayer\n  draw(plt,axis=ax)\n\n\n\n\n\n\n\nFigure 1: nir-spec-milk-2pcs\n\n\n\n\n\n\n\n7.2 3 pcs\n\nlet  \nax = (type = Axis3, width = 400, height = 300,azimuth =-0.1pi,elevation=0.1pi)\ntable=(pc1=Ytr.x1,pc2=Ytr.x2,pc3=Ytr.x3,cat=ytrain)\ndatalayer=data(table)\nmaplayer=mapping(:pc1,:pc2,:pc3,color=:cat)\nvislayer=visual(Scatter,markersize=10,strokewidth=1,strokecolor=:black)\nplt=datalayer*maplayer*vislayer\ndraw(plt,axis=ax)\nend\n\n\n\n\n\n\n\nFigure 2: nir-spec-milk-3pcs",
    "crumbs": [
      "1.NIR Spectra of Milk"
    ]
  },
  {
    "objectID": "learn/Dimension/index.html",
    "href": "learn/Dimension/index.html",
    "title": "Clustering",
    "section": "",
    "text": "Clustering",
    "crumbs": [
      "Dimensionreduction Intro"
    ]
  },
  {
    "objectID": "learn/Julia-MachineLearning.html",
    "href": "learn/Julia-MachineLearning.html",
    "title": "Julia-MachineLearning",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "learn/Julia-MachineLearning.html#quarto",
    "href": "learn/Julia-MachineLearning.html#quarto",
    "title": "Julia-MachineLearning",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "learn/Regression/1-cricket-chirps.html",
    "href": "learn/Regression/1-cricket-chirps.html",
    "title": "Cricket Chirp Linear Regression",
    "section": "",
    "text": "info\n\n\n\nref :snowy tree cricket\n雪树蟋蟀鸣叫的频率和温度的线性关系非常精确. 线性模型在两个不同的物理量之间寻求映射关系",
    "crumbs": [
      "1.Cricket Chirps Rate"
    ]
  },
  {
    "objectID": "learn/Regression/1-cricket-chirps.html#load-package",
    "href": "learn/Regression/1-cricket-chirps.html#load-package",
    "title": "Cricket Chirp Linear Regression",
    "section": "1. load package",
    "text": "1. load package\n\n import FileIO:load\n import MLJ:fit!,match,predict,table,fitted_params\n using CSV,DataFrames,Tidier,Pipe\n using CairoMakie,AlgebraOfGraphics,MakieThemes\n using MLJ,StatsBase",
    "crumbs": [
      "1.Cricket Chirps Rate"
    ]
  },
  {
    "objectID": "learn/Regression/1-cricket-chirps.html#load-csvimg",
    "href": "learn/Regression/1-cricket-chirps.html#load-csvimg",
    "title": "Cricket Chirp Linear Regression",
    "section": "2. load csv,img",
    "text": "2. load csv,img\n\nimg=load(\"../../imgs/snowy-cricket.png\");\ndf=CSV.File(\"../../data/CricketChirps.csv\")|&gt;DataFrame\nfirst(df,5)\n\n5×2 DataFrame\n\n\n\nRow\nTemperature\nChirps\n\n\n\nFloat64\nInt64\n\n\n\n\n1\n54.5\n81\n\n\n2\n59.5\n97\n\n\n3\n63.5\n103\n\n\n4\n67.5\n123\n\n\n5\n72.0\n150",
    "crumbs": [
      "1.Cricket Chirps Rate"
    ]
  },
  {
    "objectID": "learn/Regression/1-cricket-chirps.html#make-data-and-test-data",
    "href": "learn/Regression/1-cricket-chirps.html#make-data-and-test-data",
    "title": "Cricket Chirp Linear Regression",
    "section": "3. make data and test data",
    "text": "3. make data and test data\n测试数据使用利用温度区间的极值区间, 插值 n=50获得\ntest_X=range(extrema(df[:,1])...,50)\n\nX=MLJ.table(reshape(df[:,1],7,1))\ny=Vector(df[:,2])\ntest_X=range(extrema(df[:,1])...,50)\ntest_X=MLJ.table(reshape(test_X,50,1))\ncols=names(df)\n\n2-element Vector{String}:\n \"Temperature\"\n \"Chirps\"",
    "crumbs": [
      "1.Cricket Chirps Rate"
    ]
  },
  {
    "objectID": "learn/Regression/1-cricket-chirps.html#wrap-linearreg-model",
    "href": "learn/Regression/1-cricket-chirps.html#wrap-linearreg-model",
    "title": "Cricket Chirp Linear Regression",
    "section": "4. wrap LinearReg model",
    "text": "4. wrap LinearReg model\nreturn fitting Model\n\nfunction LineReg(X,y)\n    LinearRegressor = @load LinearRegressor pkg=MLJLinearModels\n    mach = fit!(machine(LinearRegressor(), X, y))\n    return mach\nend\n\nLineReg (generic function with 1 method)\n\n\n\nmach=LineReg(X,y)\nyhat=predict(mach,test_X).|&gt;(d-&gt;round(d,digits=2))\n\nimport MLJLinearModels ✔\n\n\n50-element Vector{Float64}:\n  74.05\n  76.52\n  79.0\n  81.47\n  83.94\n  86.42\n  88.89\n  91.37\n  93.84\n  96.32\n  98.79\n 101.27\n 103.74\n   ⋮\n 168.08\n 170.55\n 173.03\n 175.5\n 177.97\n 180.45\n 182.92\n 185.4\n 187.87\n 190.35\n 192.82\n 195.3",
    "crumbs": [
      "1.Cricket Chirps Rate"
    ]
  },
  {
    "objectID": "learn/Regression/1-cricket-chirps.html#plot-fitting-curve",
    "href": "learn/Regression/1-cricket-chirps.html#plot-fitting-curve",
    "title": "Cricket Chirp Linear Regression",
    "section": "5. plot fitting curve",
    "text": "5. plot fitting curve\n\nfunction plot_fitting_curve(df,yhat)\n    X=df[:,1]\n    test_X=range(extrema(df[:,1])...,50)\n    cols=names(df)\n    fig=Figure()\n    ax=Axis(fig[1:3,1:3];xlabel=\"$(cols[1])\",ylabel=\"$(cols[2])\",title=\"cricket-chirp\")\n    ax2 = Axis(fig[2,4],title=\"snowy-tree-cricket\")\n    scatter!(ax, X,y,markersize=12,color=(:red,0.8),strokewidth=1,strokecolor=:black,label=\"origin data\")\n    lines!(ax, test_X,yhat,color=:blue,label=\"fitting lines\")\n    image!(ax2,img)\n    hidespines!(ax2)\n    hidedecorations!(ax2)\n    axislegend(ax,position=:rb)\n    fig\nend\n\nplot_fitting_curve(df,yhat)",
    "crumbs": [
      "1.Cricket Chirps Rate"
    ]
  },
  {
    "objectID": "learn/Regression/3-gaussian-mixturemodels-regression.html",
    "href": "learn/Regression/3-gaussian-mixturemodels-regression.html",
    "title": "3 Gaussian Mixture Model Regression in Boston Housing",
    "section": "",
    "text": "import MLJ:predict\nusing MLJ",
    "crumbs": [
      "3-gaussian-mixturemodels-regression(boston housing)"
    ]
  },
  {
    "objectID": "learn/Regression/3-gaussian-mixturemodels-regression.html#load-package",
    "href": "learn/Regression/3-gaussian-mixturemodels-regression.html#load-package",
    "title": "3 Gaussian Mixture Model Regression in Boston Housing",
    "section": "",
    "text": "import MLJ:predict\nusing MLJ",
    "crumbs": [
      "3-gaussian-mixturemodels-regression(boston housing)"
    ]
  },
  {
    "objectID": "learn/Regression/3-gaussian-mixturemodels-regression.html#load-data",
    "href": "learn/Regression/3-gaussian-mixturemodels-regression.html#load-data",
    "title": "3 Gaussian Mixture Model Regression in Boston Housing",
    "section": "2. load data",
    "text": "2. load data\n\nX, y= @load_boston;",
    "crumbs": [
      "3-gaussian-mixturemodels-regression(boston housing)"
    ]
  },
  {
    "objectID": "learn/Regression/3-gaussian-mixturemodels-regression.html#load-model",
    "href": "learn/Regression/3-gaussian-mixturemodels-regression.html#load-model",
    "title": "3 Gaussian Mixture Model Regression in Boston Housing",
    "section": "3. load model",
    "text": "3. load model\n\nGaussian Mixture Model Regression doc \n\n\n\n\nGMR\n\n\n\n\n\n\n @doc(GaussianMixtureRegressor)\n\nNo documentation found.\nBinding GaussianMixtureRegressor does not exist.\n\n\n\n\n\nmodelType= @load GaussianMixtureRegressor pkg = \"BetaML\"\ngmr= modelType()\n\nimport BetaML ✔\n\n\nGaussianMixtureRegressor(\n  n_classes = 3, \n  initial_probmixtures = Float64[], \n  mixtures = BetaML.GMM.DiagonalGaussian{Float64}[BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing), BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing), BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing)], \n  tol = 1.0e-6, \n  minimum_variance = 0.05, \n  minimum_covariance = 0.0, \n  initialisation_strategy = \"kmeans\", \n  maximum_iterations = 9223372036854775807, \n  rng = Random._GLOBAL_RNG())",
    "crumbs": [
      "3-gaussian-mixturemodels-regression(boston housing)"
    ]
  },
  {
    "objectID": "learn/Regression/3-gaussian-mixturemodels-regression.html#fit-model",
    "href": "learn/Regression/3-gaussian-mixturemodels-regression.html#fit-model",
    "title": "3 Gaussian Mixture Model Regression in Boston Housing",
    "section": "4. fit model",
    "text": "4. fit model\n\n  (fitResults, cache, report) = MLJ.fit(gmr, 1, X, y);\n  y_res= predict(gmr, fitResults, X)\n  show(y_res)\n\nIter. 1:    Var. of the post  21.74887448784977       Log-likelihood -21687.09917379566\n[24.703442835305566, 24.70344283512715, 24.70344283528248, 24.70343670603417, 24.70343160864073, 24.703419428431936, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 17.172486989760344, 17.172486989759705, 17.172486989760284, 17.172486989763467, 17.17248698975961, 17.172486989760017, 17.172486989759697, 17.172486989759594, 17.172486989759623, 17.172486989759545, 17.172486989759616, 17.17248698975958, 17.172486989759566, 17.17248698975965, 17.172486989759577, 17.172486989759623, 17.17248698975965, 17.17248698975961, 17.17248698975957, 17.172486989759594, 17.172486989759616, 17.172486989759598, 19.935012259225488, 20.00187226969026, 23.088066583272646, 23.57897883887432, 28.25, 28.25, 24.703442835300468, 24.703442835294837, 24.70344283529346, 24.70344283520512, 24.70344283515141, 24.70344283508591, 24.70344283199117, 24.703442805344434, 24.703442834581693, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 28.25, 28.25, 28.25, 28.25, 28.25, 28.25, 28.25, 28.25, 28.25, 28.25, 24.703442502044158, 28.25, 28.25, 24.702446866451194, 24.6985898691971, 24.701467076406036, 24.524797107279493, 24.096147807623883, 24.496947009118436, 24.481596487075514, 24.70308048914557, 24.700317753746603, 24.67399885689107, 24.6979242685182, 24.698189265902187, 24.700824797735443, 28.25, 28.25, 28.25, 28.25, 24.703439077303198, 24.703438457694865, 24.703435040168955, 24.703429918198168, 24.703442835094833, 24.70344283524452, 24.703442835099494, 24.703442835015117, 28.25, 28.25, 28.25, 24.703442833366402, 24.70344282768721, 24.703442834098148, 24.70344283513251, 24.70344283412747, 17.172486989759786, 17.172486989760102, 17.17248698975958, 17.172486989759665, 17.172486989759626, 17.17248698975962, 17.172486989759623, 17.172486989759594, 17.172486989759605, 17.172486989759577, 17.17248698976003, 24.703442817517537, 24.703442692841023, 24.703442696942748, 24.703442795292872, 24.703442720024928, 24.70344281582881, 24.703442805523782, 24.70344278294844, 24.703442813844035, 17.454738508149536, 17.311003690852925, 17.22460487036884, 17.187096358744846, 17.214312468495734, 17.263898064954027, 17.181446045201845, 17.172486989759566, 17.172486989759566, 17.172486989759566, 17.172486989759566, 17.172486989759566, 17.172486989759566, 17.172486989759566, 17.172486989759566, 17.172486989759566, 17.172486989759566, 17.172486989759566, 17.172486989759566, 17.172486989759566, 17.172486989759566, 17.172486989759566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.70344283527209, 24.70344283520386, 24.703442835162008, 24.703442835117126, 24.703442834967593, 24.703442834130552, 24.703442835106276, 24.70344283529511, 28.25, 28.25, 28.25, 28.25, 28.25, 28.25, 28.25, 28.25, 28.25, 28.25, 28.25, 28.25, 28.25, 28.25, 28.25, 28.25, 28.25, 28.25, 24.703358347522453, 24.703208347530257, 24.701155891825163, 24.702755037952084, 24.671621907548783, 24.696464463176728, 24.681898743667606, 24.702378640463284, 24.70335390049757, 24.70264440411786, 24.7032719326835, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835301445, 24.703442835277393, 24.703442835303612, 24.70344283530352, 24.703442835305136, 24.703442835305136, 24.703442835304934, 24.70344283530431, 24.703442835305616, 24.703442835305527, 24.70344283530346, 24.703442835304983, 24.70344283530541, 24.70344283530532, 24.703442835304717, 24.703442835304294, 24.703442835304397, 24.70344283530529, 28.25, 28.25, 28.25, 28.25, 28.25, 28.25, 28.25, 28.25, 28.25, 28.25, 28.25, 28.25, 28.25, 28.25, 28.25, 28.25, 28.25, 28.25, 28.25, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703213700160653, 24.703385262654866, 24.70343760030826, 24.703378711983294, 24.7034298399071, 28.25, 28.25, 28.25, 28.25, 28.25, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 28.25, 28.25, 28.25, 28.25, 28.25, 28.25, 28.25, 28.25, 28.25, 28.25, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 24.703442835305566, 28.25, 28.25, 28.25, 28.25, 28.25, 28.25, 28.25, 28.25, 28.25, 28.25, 24.7034048283545, 24.703346601070763, 24.703375075393758, 24.70342090302755, 24.703163624990665, 24.70336635301459, 24.703370356150774, 24.703361730629254, 24.703240819037475, 24.703355070489028, 24.703408592515213, 24.703417543809774, 17.208656008369964, 17.205128445480774, 17.20563411477498, 17.177006395284604, 17.258463352939142, 17.73353486263012, 17.40036626018715, 17.21524526064883, 24.703442835305566, 24.703442835305566, 24.703442835305566, 28.25, 28.25, 17.172521733540417, 17.17251693819644, 17.172506509841966, 17.172491504360483, 17.172489702001034, 17.172494163130544, 17.17249085826865, 17.172489011910766, 28.25, 24.703442835305566, 28.25, 28.25, 24.703206699081115, 24.70301235533609, 28.25, 28.25, 28.25, 28.25, 28.25, 28.25, 28.25, 28.25, 28.25, 17.17248698976034, 17.1724869897632, 17.17248698976563, 17.172486989764817, 17.172486989766966, 17.17248698976181, 17.17248698976097, 17.172486989760838, 17.1724869898084, 17.172486989760174, 17.17248698976016, 17.172486989759637, 17.17248698976089, 17.172486989764426, 17.17248698976544, 17.17248698976063, 17.172486989760966, 17.172486989759637, 17.172486989759584, 17.172486989760664, 17.172486989759886, 17.172486989760078, 17.172486989759722, 17.172486989759783, 17.172486989759626, 17.172486989759904, 17.172486989759765, 17.17248698975972, 17.172486989759626, 17.17248698975958, 17.172486989759637, 17.172486989759562, 17.172486989759555, 17.172486989759825, 17.172486989760227, 17.17248698976148, 17.17248698975969, 17.17248698976085, 17.172486989760205, 17.172486989760444, 17.172486989760383, 17.172486989759907, 17.172486989759612, 17.17248698975984, 17.172486989759676, 17.17248698975989, 17.17248698975999, 17.17248698975971, 17.172486989759555, 17.17248698975959, 17.17248698975965, 17.172486989759996, 17.1724869897597, 17.172486989759832, 17.172486989759655, 17.172486989759744, 17.172486989759584, 17.172486989759587, 17.17248698975957, 17.172486989759566, 17.172486989759733, 17.17248698975955, 17.172486989759577, 17.17248698976005, 17.172486989760262, 17.172486989760365, 17.1724869897605, 17.172486989759836, 17.172486989760316, 17.172486989759648, 17.17248698976125, 17.172486989759786, 17.17248698976012, 17.172486989759722, 17.172486989760344, 17.172486989760024, 17.172486989763165, 17.17248698976063, 17.17248698975999, 17.172486989759854, 17.172486989759857, 17.172486989759587, 17.172486989759566, 17.17248698975988, 17.172486989759758, 17.17248698976028, 17.17248698976052, 17.17248698976021, 17.172486989759726, 17.172486989759665, 17.17248698976043, 17.172486989760593, 17.1724869897604, 17.172486989760273, 17.17248698976015, 17.172486989761037, 17.172486989761456, 17.17248698976206, 17.172486989759967, 17.17248698976048, 17.172486989760117, 17.172486989760415, 17.172486989762152, 17.17248698976345, 17.172486989761914, 17.172486989763666, 17.172486989765044, 17.17248698976681, 17.17248698977656, 17.172486989790286, 17.17248698976062, 17.172486989760586, 17.172486989763428, 17.172486989771894, 17.172486989765833, 17.172486989767396, 17.172486989773844, 17.172486989786925, 17.1724869897603, 17.17248698976004, 17.172486989761534, 17.17248698975969, 17.172486989760497, 17.172486989761484, 17.17248698980489, 17.17248698980944, 17.172486989823852, 17.172486989982893, 17.172486989866112, 17.172486989900303, 17.17248698977095, 17.172486989806476, 17.17248698976693, 17.17248698976169, 17.172486989760486, 17.17248698976744, 17.172486989796315, 17.548216450574383, 17.8098216029985, 18.057375705208386, 17.226499841914723, 17.363267869335125, 17.41339870499964, 17.26250176255356, 17.277328110602145, 17.172486989759737, 17.172486989759634, 17.17248698975967, 17.17248698975967, 17.172486989759637]",
    "crumbs": [
      "3-gaussian-mixturemodels-regression(boston housing)"
    ]
  },
  {
    "objectID": "learn/Regression/3-gaussian-mixturemodels-regression.html#measure-model",
    "href": "learn/Regression/3-gaussian-mixturemodels-regression.html#measure-model",
    "title": "3 Gaussian Mixture Model Regression in Boston Housing",
    "section": "5. measure model",
    "text": "5. measure model\n\nrmse(y_res,y)\n\n7.9566567641159605",
    "crumbs": [
      "3-gaussian-mixturemodels-regression(boston housing)"
    ]
  },
  {
    "objectID": "learn/Regression/5-ecommerce-dataset-linear-regression.html",
    "href": "learn/Regression/5-ecommerce-dataset-linear-regression.html",
    "title": "5 Ecommerce customers dataset Linear Regression",
    "section": "",
    "text": "info\n\n\n\ndataset: Linear Regression E-commerce Dataset\nmodel : E-commerce Dataset - Linear Regression Model",
    "crumbs": [
      "5-ecommerce-customer-dataset-linear-regression"
    ]
  },
  {
    "objectID": "learn/Regression/5-ecommerce-dataset-linear-regression.html#load-package",
    "href": "learn/Regression/5-ecommerce-dataset-linear-regression.html#load-package",
    "title": "5 Ecommerce customers dataset Linear Regression",
    "section": "1. load package",
    "text": "1. load package\n\n import MLJ:fit!,match,predict,table,fitted_params\n using CSV,DataFrames,Tidier,Pipe\n using CairoMakie,AlgebraOfGraphics,MakieThemes\n using MLJ,StatsBase",
    "crumbs": [
      "5-ecommerce-customer-dataset-linear-regression"
    ]
  },
  {
    "objectID": "learn/Regression/5-ecommerce-dataset-linear-regression.html#load-data",
    "href": "learn/Regression/5-ecommerce-dataset-linear-regression.html#load-data",
    "title": "5 Ecommerce customers dataset Linear Regression",
    "section": "2. load data",
    "text": "2. load data\n\ndf=CSV.File(\"../../data/Ecommerce-Customers.csv\")|&gt;DataFrame\ndf=@chain df begin\n  @select(4:8)\n  @clean_names\nend;",
    "crumbs": [
      "5-ecommerce-customer-dataset-linear-regression"
    ]
  },
  {
    "objectID": "learn/Regression/5-ecommerce-dataset-linear-regression.html#eda",
    "href": "learn/Regression/5-ecommerce-dataset-linear-regression.html#eda",
    "title": "5 Ecommerce customers dataset Linear Regression",
    "section": "3. EDA",
    "text": "3. EDA\necommerce-customer-dataset EDA",
    "crumbs": [
      "5-ecommerce-customer-dataset-linear-regression"
    ]
  },
  {
    "objectID": "learn/digits-kmeans.html",
    "href": "learn/digits-kmeans.html",
    "title": "mlj digits kmeans",
    "section": "",
    "text": "import MLJ:transform,inverse_transform,fit!,predict,fitted_params\nusing CSV, DataFrames,JLSO,LinearAlgebra,MLJ,GLMakie,Pipe\n\nWARNING: using DataFrames.transform in module Main conflicts with an existing identifier.",
    "crumbs": [
      "Learn",
      "mlj digits kmeans"
    ]
  },
  {
    "objectID": "learn/digits-kmeans.html#load-package",
    "href": "learn/digits-kmeans.html#load-package",
    "title": "mlj digits kmeans",
    "section": "",
    "text": "import MLJ:transform,inverse_transform,fit!,predict,fitted_params\nusing CSV, DataFrames,JLSO,LinearAlgebra,MLJ,GLMakie,Pipe\n\nWARNING: using DataFrames.transform in module Main conflicts with an existing identifier.",
    "crumbs": [
      "Learn",
      "mlj digits kmeans"
    ]
  },
  {
    "objectID": "learn/digits-kmeans.html#load-data",
    "href": "learn/digits-kmeans.html#load-data",
    "title": "mlj digits kmeans",
    "section": "2. load data",
    "text": "2. load data\n\ndata=@pipe CSV.File(\"../data/load_digits.csv\")|&gt;DataFrame\nXtr = Matrix(data[!,1:64])\nytr = categorical(data[!,65])\ncat=levels(ytr)\n\n10-element Vector{Int64}:\n 0\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9",
    "crumbs": [
      "Learn",
      "mlj digits kmeans"
    ]
  },
  {
    "objectID": "learn/digits-kmeans.html#pca-workflow",
    "href": "learn/digits-kmeans.html#pca-workflow",
    "title": "mlj digits kmeans",
    "section": "3. pca workflow",
    "text": "3. pca workflow\n从 64维数字降维到 2 维\n\nPCA = @load PCA pkg=MultivariateStats\n\npcamodel=PCA(maxoutdim=2)\n\nkpcamachine = machine(pcamodel,Xtr ) |&gt; fit!\npcaXtr=transform(kpcamachine, Xtr)\n\n\nnums=200\nfunction boundary_data(data;n=nums)\n    xs=extrema(data[:x1]).|&gt;d-&gt;round(d,digits=3)\n    ys=extrema(data[:x2]).|&gt;d-&gt;round(d,digits=3)\n    tx = LinRange(xs...,n)\n    ty = LinRange(ys...,n)\n    xtest = mapreduce(collect, hcat, Iterators.product(tx, ty));\n    xtest=MLJ.table(xtest')\n    return tx,ty, xtest\nend\n\ntx,ty, Xtest=boundary_data(pcaXtr)\n\n[ Info: For silent loading, specify `verbosity=0`. \n┌ Warning: The number and/or types of data arguments do not match what the specified model\n│ supports. Suppress this type check by specifying `scitype_check_level=0`.\n│ \n│ Run `@doc MultivariateStats.PCA` to learn more about your model's requirements.\n│ \n│ Commonly, but non exclusively, supervised models are constructed using the syntax\n│ `machine(model, X, y)` or `machine(model, X, y, w)` while most other models are\n│ constructed with `machine(model, X)`.  Here `X` are features, `y` a target, and `w`\n│ sample or class weights.\n│ \n│ In general, data in `machine(model, data...)` is expected to satisfy\n│ \n│     scitype(data) &lt;: MLJ.fit_data_scitype(model)\n│ \n│ In the present case:\n│ \n│ scitype(data) = Tuple{AbstractMatrix{Continuous}}\n│ \n│ fit_data_scitype(model) = Tuple{Table{&lt;:AbstractVector{&lt;:Continuous}}}\n└ @ MLJBase ~/.julia/packages/MLJBase/fEiP2/src/machines.jl:230\n[ Info: Training machine(PCA(maxoutdim = 2, …), …).\n\n\nimport MLJMultivariateStatsInterface ✔\n\n\n(LinRange{Float64}(-31.17, 31.7, 200), LinRange{Float64}(-30.092, 27.494, 200), Tables.MatrixTable{Adjoint{Float64, Matrix{Float64}}} with 40000 rows, 2 columns, and schema:\n :x1  Float64\n :x2  Float64)",
    "crumbs": [
      "Learn",
      "mlj digits kmeans"
    ]
  },
  {
    "objectID": "learn/digits-kmeans.html#kmeans-workflow",
    "href": "learn/digits-kmeans.html#kmeans-workflow",
    "title": "mlj digits kmeans",
    "section": "4. kmeans workflow",
    "text": "4. kmeans workflow\n\nKMeans = @load KMeans pkg=Clustering\nmodel = KMeans(k=10)\nmach = machine(model, pcaXtr) |&gt; fit!\ncenters=fitted_params(mach)[:centers]\nyhat = predict(mach, Xtest)\nytest=yhat|&gt;Array|&gt;d-&gt;reshape(d,nums,nums)\n\nimport MLJClusteringInterface ✔\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n[ Info: Training machine(KMeans(k = 10, …), …).\n\n\n200×200 Matrix{Int64}:\n 4  4  4  4  4  4  4  4  4  4  4  4  4  …   1   1   1   1   1   1   1   1   1\n 4  4  4  4  4  4  4  4  4  4  4  4  4      1   1   1   1   1   1   1   1   1\n 4  4  4  4  4  4  4  4  4  4  4  4  4      1   1   1   1   1   1   1   1   1\n 4  4  4  4  4  4  4  4  4  4  4  4  4      1   1   1   1   1   1   1   1   1\n 4  4  4  4  4  4  4  4  4  4  4  4  4      1   1   1   1   1   1   1   1   1\n 4  4  4  4  4  4  4  4  4  4  4  4  4  …   1   1   1   1   1   1   1   1   1\n 4  4  4  4  4  4  4  4  4  4  4  4  4      1   1   1   1   1   1   1   1   1\n 4  4  4  4  4  4  4  4  4  4  4  4  4      1   1   1   1   1   1   1   1   1\n 4  4  4  4  4  4  4  4  4  4  4  4  4      1   1   1   1   1   1   1   1   1\n 4  4  4  4  4  4  4  4  4  4  4  4  4      1   1   1   1   1   1   1   1   1\n 4  4  4  4  4  4  4  4  4  4  4  4  4  …   1   1   1   1   1   1   1   1   1\n 4  4  4  4  4  4  4  4  4  4  4  4  4      1   1   1   1   1   1   1   1   1\n 4  4  4  4  4  4  4  4  4  4  4  4  4      1   1   1   1   1   1   1   1   1\n ⋮              ⋮              ⋮        ⋱                   ⋮              \n 6  6  6  6  6  6  6  6  6  6  6  6  6     10   8   8   8   8   8   8   8   8\n 6  6  6  6  6  6  6  6  6  6  6  6  6     10  10   8   8   8   8   8   8   8\n 6  6  6  6  6  6  6  6  6  6  6  6  6  …  10  10  10   8   8   8   8   8   8\n 6  6  6  6  6  6  6  6  6  6  6  6  6     10  10  10  10   8   8   8   8   8\n 6  6  6  6  6  6  6  6  6  6  6  6  6     10  10  10  10   8   8   8   8   8\n 6  6  6  6  6  6  6  6  6  6  6  6  6     10  10  10  10  10   8   8   8   8\n 6  6  6  6  6  6  6  6  6  6  6  6  6     10  10  10  10  10  10   8   8   8\n 6  6  6  6  6  6  6  6  6  6  6  6  6  …  10  10  10  10  10  10  10   8   8\n 6  6  6  6  6  6  6  6  6  6  6  6  6     10  10  10  10  10  10  10  10   8\n 6  6  6  6  6  6  6  6  6  6  6  6  6     10  10  10  10  10  10  10  10  10\n 6  6  6  6  6  6  6  6  6  6  6  6  6     10  10  10  10  10  10  10  10  10\n 6  6  6  6  6  6  6  6  6  6  6  6  6     10  10  10  10  10  10  10  10  10",
    "crumbs": [
      "Learn",
      "mlj digits kmeans"
    ]
  },
  {
    "objectID": "learn/digits-kmeans.html#plot-results",
    "href": "learn/digits-kmeans.html#plot-results",
    "title": "mlj digits kmeans",
    "section": "5. plot results",
    "text": "5. plot results\n\nfunction plot_img()\n    fig=Figure(resolution=(900,900))\n    ax= Axis(fig[1,1])\n    colors=[:red, :yellow,:purple,:lightblue,:black,:orange,:pink,:blue,:tomato,:lightgreen,]\n    markers=['0','1','2','3','4','5','6','7','8','9']\n    contourf!(ax, tx,ty,ytest,levels=length(cat),colormap=:redsblues)\n    scatter!(ax, centers[1,:],centers[2,:],marker=:cross,markersize=26,color=:white)\n    for (i,c) in enumerate(cat)\n        dx=pcaXtr[:x1][ytr.==c]\n        dy=pcaXtr[:x2][ytr.==c]\n        scatter!(ax, dx,dy,color=(colors[i],0.8),label=\"$c\",marker=markers[i],markersize=18)\n    end\n    axislegend(ax)\n\nfig\n\nend\n\nplot_img()",
    "crumbs": [
      "Learn",
      "mlj digits kmeans"
    ]
  },
  {
    "objectID": "learn/ensemblemodel-with-bostonhouse.html",
    "href": "learn/ensemblemodel-with-bostonhouse.html",
    "title": "ensemblemodel-with-bostonhouse",
    "section": "",
    "text": "info\n\n\n\nRandom Forest :ref What is random forest?",
    "crumbs": [
      "Learn",
      "ensemble model with boston housing dataset"
    ]
  },
  {
    "objectID": "learn/ensemblemodel-with-bostonhouse.html#loading-package",
    "href": "learn/ensemblemodel-with-bostonhouse.html#loading-package",
    "title": "ensemblemodel-with-bostonhouse",
    "section": "1. loading package",
    "text": "1. loading package\n\nusing MLJ\nimport DataFrames: DataFrame\nusing PrettyPrinting\nusing StableRNGs\nusing CairoMakie,AlgebraOfGraphics,MakieThemes\nrng = StableRNG(512)\n\nStableRNGs.LehmerRNG(state=0x00000000000000000000000000000401)",
    "crumbs": [
      "Learn",
      "ensemble model with boston housing dataset"
    ]
  },
  {
    "objectID": "learn/ensemblemodel-with-bostonhouse.html#load-data",
    "href": "learn/ensemblemodel-with-bostonhouse.html#load-data",
    "title": "ensemblemodel-with-bostonhouse",
    "section": "2. load data",
    "text": "2. load data\n\nX, y = @load_boston\n\n((Crim = [0.00632, 0.02731, 0.02729, 0.03237, 0.06905, 0.02985, 0.08829, 0.14455, 0.21124, 0.17004  …  0.2896, 0.26838, 0.23912, 0.17783, 0.22438, 0.06263, 0.04527, 0.06076, 0.10959, 0.04741], Zn = [18.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.5, 12.5, 12.5, 12.5  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Indus = [2.31, 7.07, 7.07, 2.18, 2.18, 2.18, 7.87, 7.87, 7.87, 7.87  …  9.69, 9.69, 9.69, 9.69, 9.69, 11.93, 11.93, 11.93, 11.93, 11.93], NOx = [0.538, 0.469, 0.469, 0.458, 0.458, 0.458, 0.524, 0.524, 0.524, 0.524  …  0.585, 0.585, 0.585, 0.585, 0.585, 0.573, 0.573, 0.573, 0.573, 0.573], Rm = [6.575, 6.421, 7.185, 6.998, 7.147, 6.43, 6.012, 6.172, 5.631, 6.004  …  5.39, 5.794, 6.019, 5.569, 6.027, 6.593, 6.12, 6.976, 6.794, 6.03], Age = [65.2, 78.9, 61.1, 45.8, 54.2, 58.7, 66.6, 96.1, 100.0, 85.9  …  72.9, 70.6, 65.3, 73.5, 79.7, 69.1, 76.7, 91.0, 89.3, 80.8], Dis = [4.09, 4.9671, 4.9671, 6.0622, 6.0622, 6.0622, 5.5605, 5.9505, 6.0821, 6.5921  …  2.7986, 2.8927, 2.4091, 2.3999, 2.4982, 2.4786, 2.2875, 2.1675, 2.3889, 2.505], Rad = [1.0, 2.0, 2.0, 3.0, 3.0, 3.0, 5.0, 5.0, 5.0, 5.0  …  6.0, 6.0, 6.0, 6.0, 6.0, 1.0, 1.0, 1.0, 1.0, 1.0], Tax = [296.0, 242.0, 242.0, 222.0, 222.0, 222.0, 311.0, 311.0, 311.0, 311.0  …  391.0, 391.0, 391.0, 391.0, 391.0, 273.0, 273.0, 273.0, 273.0, 273.0], PTRatio = [15.3, 17.8, 17.8, 18.7, 18.7, 18.7, 15.2, 15.2, 15.2, 15.2  …  19.2, 19.2, 19.2, 19.2, 19.2, 21.0, 21.0, 21.0, 21.0, 21.0], Black = [396.9, 396.9, 392.83, 394.63, 396.9, 394.12, 395.6, 396.9, 386.63, 386.71  …  396.9, 396.9, 396.9, 395.77, 396.9, 391.99, 396.9, 396.9, 393.45, 396.9], LStat = [4.98, 9.14, 4.03, 2.94, 5.33, 5.21, 12.43, 19.15, 29.93, 17.1  …  21.14, 14.1, 12.92, 15.1, 14.33, 9.67, 9.08, 5.64, 6.48, 7.88]), [24.0, 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9  …  19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22.0, 11.9])",
    "crumbs": [
      "Learn",
      "ensemble model with boston housing dataset"
    ]
  },
  {
    "objectID": "learn/ensemblemodel-with-bostonhouse.html#single-model",
    "href": "learn/ensemblemodel-with-bostonhouse.html#single-model",
    "title": "ensemblemodel-with-bostonhouse",
    "section": "3. single model",
    "text": "3. single model\n\n3.1 load model\n\nDecisionTreeRegressor = @load DecisionTreeRegressor pkg=DecisionTree\n\nimport MLJDecisionTreeInterface ✔\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n\n\nMLJDecisionTreeInterface.DecisionTreeRegressor\n\n\n\n\n3.2 instantiate and evaulate model\n\ntree = machine(DecisionTreeRegressor(), X, y)\ne = evaluate!(tree, resampling=Holdout(fraction_train=0.8),\n              measure=[rms, rmslp1])\n\n\nPerformanceEvaluation object with these fields:\n  model, measure, operation, measurement, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_rows, resampling, repeats\nExtract:\n┌──────────────────────────────────────┬───────────┬─────────────┬──────────┐\n│ measure                              │ operation │ measurement │ per_fold │\n├──────────────────────────────────────┼───────────┼─────────────┼──────────┤\n│ RootMeanSquaredError()               │ predict   │ 7.06        │ [7.06]   │\n│ RootMeanSquaredLogProportionalError( │ predict   │ 0.328       │ [0.328]  │\n│   offset = 1)                        │           │             │          │\n└──────────────────────────────────────┴───────────┴─────────────┴──────────┘",
    "crumbs": [
      "Learn",
      "ensemble model with boston housing dataset"
    ]
  },
  {
    "objectID": "learn/ensemblemodel-with-bostonhouse.html#ensemble-mode-random-forest",
    "href": "learn/ensemblemodel-with-bostonhouse.html#ensemble-mode-random-forest",
    "title": "ensemblemodel-with-bostonhouse",
    "section": "4. Ensemble mode : Random Forest",
    "text": "4. Ensemble mode : Random Forest\n\n4.1 instantiate random forest\n\nforest = EnsembleModel(model=DecisionTreeRegressor())\nforest.model.n_subfeatures = 3\n\n3\n\n\n\n\n4.2 by learning curve to get best trees\n\nrng = StableRNG(5123) # for reproducibility\nm = machine(forest, X, y)\nr = range(forest, :n, lower=10, upper=1000)\ncurves = MLJ.learning_curve(m, resampling=Holdout(fraction_train=0.8, rng=rng),\n                         range=r, measure=rms);\n\n[ Info: Training machine(DeterministicTunedModel(model = DeterministicEnsembleModel(model = DecisionTreeRegressor(max_depth = -1, …), …), …), …).\n[ Info: Attempting to evaluate 30 models.\nEvaluating over 30 metamodels:   0%[&gt;                        ]  ETA: N/AEvaluating over 30 metamodels:   3%[&gt;                        ]  ETA: 0:00:00Evaluating over 30 metamodels:   7%[=&gt;                       ]  ETA: 0:00:00Evaluating over 30 metamodels:  10%[==&gt;                      ]  ETA: 0:00:00Evaluating over 30 metamodels:  13%[===&gt;                     ]  ETA: 0:00:00Evaluating over 30 metamodels:  17%[====&gt;                    ]  ETA: 0:00:00Evaluating over 30 metamodels:  20%[=====&gt;                   ]  ETA: 0:00:00Evaluating over 30 metamodels:  23%[=====&gt;                   ]  ETA: 0:00:00Evaluating over 30 metamodels:  27%[======&gt;                  ]  ETA: 0:00:00Evaluating over 30 metamodels:  30%[=======&gt;                 ]  ETA: 0:00:00Evaluating over 30 metamodels:  33%[========&gt;                ]  ETA: 0:00:00Evaluating over 30 metamodels:  37%[=========&gt;               ]  ETA: 0:00:00Evaluating over 30 metamodels:  40%[==========&gt;              ]  ETA: 0:00:00Evaluating over 30 metamodels:  43%[==========&gt;              ]  ETA: 0:00:00Evaluating over 30 metamodels:  47%[===========&gt;             ]  ETA: 0:00:00Evaluating over 30 metamodels:  50%[============&gt;            ]  ETA: 0:00:00Evaluating over 30 metamodels:  53%[=============&gt;           ]  ETA: 0:00:00Evaluating over 30 metamodels:  57%[==============&gt;          ]  ETA: 0:00:00Evaluating over 30 metamodels:  60%[===============&gt;         ]  ETA: 0:00:00Evaluating over 30 metamodels:  63%[===============&gt;         ]  ETA: 0:00:00Evaluating over 30 metamodels:  67%[================&gt;        ]  ETA: 0:00:00Evaluating over 30 metamodels:  70%[=================&gt;       ]  ETA: 0:00:00Evaluating over 30 metamodels:  73%[==================&gt;      ]  ETA: 0:00:00Evaluating over 30 metamodels:  77%[===================&gt;     ]  ETA: 0:00:00Evaluating over 30 metamodels:  80%[====================&gt;    ]  ETA: 0:00:00Evaluating over 30 metamodels:  83%[====================&gt;    ]  ETA: 0:00:00Evaluating over 30 metamodels:  87%[=====================&gt;   ]  ETA: 0:00:00Evaluating over 30 metamodels:  90%[======================&gt;  ]  ETA: 0:00:00Evaluating over 30 metamodels:  93%[=======================&gt; ]  ETA: 0:00:00Evaluating over 30 metamodels:  97%[========================&gt;]  ETA: 0:00:00Evaluating over 30 metamodels: 100%[=========================] Time: 0:00:00\n\n\n\nlet\n  ax=(width = 400, height = 400,xlabel=\"rmsr\",ylabel=\"numbers of trees\")\n  datalayer=data((parameter_values=curves.parameter_values,measurements=curves.measurements))\n  mappinglayer=mapping(:parameter_values,:measurements)\n  vislayer=visual(Lines,color=:blue)\n  plt=datalayer*mappinglayer*vislayer\n  draw(plt, axis=ax)\nend\n\n\n\n\n\n\n\n\n\n\n4.3 setting 150 trees\n\nforest.n = 150;\n\n\n\n4.4 setting tunning parameters range\n\nr_sf = range(forest, :(model.n_subfeatures), lower=1, upper=12)\nr_bf = range(forest, :bagging_fraction, lower=0.4, upper=1.0);\n\n\n\n4.4 tuning random forest\n\ntuned_forest = TunedModel(model=forest,\n                          tuning=Grid(resolution=3),\n                          resampling=CV(nfolds=6, rng=StableRNG(32)),\n                          ranges=[r_sf, r_bf],\n                          measure=rms)\nm = machine(tuned_forest, X, y)\ne = evaluate!(m, resampling=Holdout(fraction_train=0.8),\n              measure=[rms, rmslp1])\ne\n\n\nPerformanceEvaluation object with these fields:\n  model, measure, operation, measurement, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_rows, resampling, repeats\nExtract:\n┌──────────────────────────────────────┬───────────┬─────────────┬──────────┐\n│ measure                              │ operation │ measurement │ per_fold │\n├──────────────────────────────────────┼───────────┼─────────────┼──────────┤\n│ RootMeanSquaredError()               │ predict   │ 4.04        │ [4.04]   │\n│ RootMeanSquaredLogProportionalError( │ predict   │ 0.255       │ [0.255]  │\n│   offset = 1)                        │           │             │          │\n└──────────────────────────────────────┴───────────┴─────────────┴──────────┘\n\n\n\n\n\nr = report(m)\n\n(best_model = DeterministicEnsembleModel(model = DecisionTreeRegressor(max_depth = -1, …), …),\n best_history_entry = (model = DeterministicEnsembleModel(model = DecisionTreeRegressor(max_depth = -1, …), …),\n                       measure = StatisticalMeasuresBase.RobustMeasure{StatisticalMeasuresBase.FussyMeasure{StatisticalMeasuresBase.RobustMeasure{StatisticalMeasuresBase.Multimeasure{StatisticalMeasuresBase.SupportsMissingsMeasure{StatisticalMeasures.RootMeanSquaredErrorOnScalars}, Nothing, StatisticalMeasuresBase.RootMean{Int64}, typeof(identity)}}, Nothing}}[RootMeanSquaredError()],\n                       measurement = [3.5722631166673007],\n                       per_fold = [[2.4889985261233605, 2.385715818338134, 4.825513471374633, 3.9974877555114485, 3.4217104683246666, 3.708432091912111]],),\n history = NamedTuple{(:model, :measure, :measurement, :per_fold), Tuple{MLJEnsembles.DeterministicEnsembleModel{MLJDecisionTreeInterface.DecisionTreeRegressor}, Vector{StatisticalMeasuresBase.RobustMeasure{StatisticalMeasuresBase.FussyMeasure{StatisticalMeasuresBase.RobustMeasure{StatisticalMeasuresBase.Multimeasure{StatisticalMeasuresBase.SupportsMissingsMeasure{StatisticalMeasures.RootMeanSquaredErrorOnScalars}, Nothing, StatisticalMeasuresBase.RootMean{Int64}, typeof(identity)}}, Nothing}}}, Vector{Float64}, Vector{Vector{Float64}}}}[(model = DeterministicEnsembleModel(model = DecisionTreeRegressor(max_depth = -1, …), …), measure = [RootMeanSquaredError()], measurement = [3.8094502225320523], per_fold = [[2.6442935772490963, 2.6347662294070195, 5.180223943255861, 4.121633628755032, 3.402975674534002, 4.216118068170253]]), (model = DeterministicEnsembleModel(model = DecisionTreeRegressor(max_depth = -1, …), …), measure = [RootMeanSquaredError()], measurement = [5.416797814622986], per_fold = [[4.480610589273179, 4.094948763264285, 7.132537822015005, 5.384819020672911, 4.707260488286648, 6.097756283082266]]), (model = DeterministicEnsembleModel(model = DecisionTreeRegressor(max_depth = -1, …), …), measure = [RootMeanSquaredError()], measurement = [4.91695833115296], per_fold = [[3.6596309799981337, 3.741953403720741, 6.717443336628948, 4.937702185406267, 4.2470371282610575, 5.488318721228702]]), (model = DeterministicEnsembleModel(model = DecisionTreeRegressor(max_depth = -1, …), …), measure = [RootMeanSquaredError()], measurement = [3.9437875486722973], per_fold = [[3.0747877290149153, 3.073547224160072, 5.1088743894788875, 4.49232090548857, 3.717456825964539, 3.787278328193902]]), (model = DeterministicEnsembleModel(model = DecisionTreeRegressor(max_depth = -1, …), …), measure = [RootMeanSquaredError()], measurement = [4.5872235020609455], per_fold = [[3.512487539994043, 3.3544633706394067, 6.06724474892402, 4.767415489319376, 4.030311481897942, 5.188766348831169]]), (model = DeterministicEnsembleModel(model = DecisionTreeRegressor(max_depth = -1, …), …), measure = [RootMeanSquaredError()], measurement = [3.5722631166673007], per_fold = [[2.4889985261233605, 2.385715818338134, 4.825513471374633, 3.9974877555114485, 3.4217104683246666, 3.708432091912111]]), (model = DeterministicEnsembleModel(model = DecisionTreeRegressor(max_depth = -1, …), …), measure = [RootMeanSquaredError()], measurement = [4.088355178253196], per_fold = [[2.9594662972525345, 2.7408403025426593, 5.532571280122637, 4.37336784261903, 3.5178510319195486, 4.685615288115846]]), (model = DeterministicEnsembleModel(model = DecisionTreeRegressor(max_depth = -1, …), …), measure = [RootMeanSquaredError()], measurement = [4.237002137785962], per_fold = [[4.332862308738402, 3.5041798259437305, 5.466874767426336, 4.7402739799589515, 3.6186264610860777, 3.332125102319342]]), (model = DeterministicEnsembleModel(model = DecisionTreeRegressor(max_depth = -1, …), …), measure = [RootMeanSquaredError()], measurement = [4.102928466791321], per_fold = [[2.9936565278974525, 2.8277922166553706, 5.367651887312894, 4.8295750685245284, 3.545286939742617, 4.4057722267933634]])],\n best_report = (measures = Any[],\n                oob_measurements = missing,),\n plotting = (parameter_names = [\"model.n_subfeatures\", \"bagging_fraction\"],\n             parameter_scales = [:linear, :linear],\n             parameter_values = Any[6 0.7; 1 0.4; … ; 12 1.0; 12 0.4],\n             measurements = [3.8094502225320523, 5.416797814622986, 4.91695833115296, 3.9437875486722973, 4.5872235020609455, 3.5722631166673007, 4.088355178253196, 4.237002137785962, 4.102928466791321],),)\n\n\n\n\n4.5 plot tunning results\n\nres = r.plotting\ntable=(vals_b = res.parameter_values[:, 1],\n            vals_k = res.parameter_values[:, 2],\n            measurement=res.measurements\n)\n\ndatalayer=data(table)\nmappinglayer=mapping(:vals_b,:vals_k,:measurement,color=:measurement)\nvislayler=visual(Tricontourf,colormap = :batlow)\nax=(width = 400, height = 400)\nplt=datalayer*mappinglayer*vislayler\ndraw(plt,axis=ax)\n\n\n\n\n\n\n\nFigure 1: fig-ensemble-params-tunning\n\n\n\n\n\n\n\n4.6 predict with ensemble model\n\nŷ = predict(m,X)\n@show rms(ŷ, y)\n\nrms(ŷ, y) = 2.4308074793520404\n\n\n2.4308074793520404",
    "crumbs": [
      "Learn",
      "ensemble model with boston housing dataset"
    ]
  },
  {
    "objectID": "learn/menu-test.html",
    "href": "learn/menu-test.html",
    "title": "Menu Test",
    "section": "",
    "text": "using WGLMakie\nfig = Figure()\n\nmenu = Menu(fig, options = [\"viridis\", \"heat\", \"blues\"], default = \"blues\")\n\nfuncs = [sqrt, x-&gt;x^2, sin, cos]\n\nmenu2 = Menu(fig,\n    options = zip([\"Square Root\", \"Square\", \"Sine\", \"Cosine\"], funcs),\n    default = \"Square\")\n\nfig[1, 1] = vgrid!(\n    Label(fig, \"Colormap\", width = nothing),\n    menu,\n    Label(fig, \"Function\", width = nothing),\n    menu2;\n    tellheight = false, width = 200)\n\nax = Axis(fig[1, 2])\n\nfunc = Observable{Any}(funcs[1])\n\nys = lift(func) do f\n    f.(0:0.3:10)\nend\nscat = scatter!(ax, ys, markersize = 10px, color = ys)\n\ncb = Colorbar(fig[1, 3], scat)\n\non(menu.selection) do s\n    scat.colormap = s\nend\nnotify(menu.selection)\n\non(menu2.selection) do s\n    func[] = s\n    autolimits!(ax)\nend\nnotify(menu2.selection)\n\nmenu2.is_open = true\n\nfig",
    "crumbs": [
      "Learn",
      "Menu Test"
    ]
  },
  {
    "objectID": "learn/tailwind-test.html",
    "href": "learn/tailwind-test.html",
    "title": "tailwind css test",
    "section": "",
    "text": "Leslie Alexander\n\n\nleslie.alexander@example.com\n\n\n\n\n\nCo-Founder / CEO\n\n\nLast seen 3h ago\n\n\n\n\n\nHello world!\n\n\n\n\n\n\n\nChitChat\n\n\nYou have a new message!\n\n\n\n\n\n\n\n\nErin Lindford\n\n\nProduct Engineer\n\n\n\nMessage\n\n\n\n\n\n\n\n\n\nLeslie Alexander\n\n\nleslie.alexander@example.com\n\n\n\n\n\nCo-Founder / CEO\n\n\nLast seen 3h ago\n\n\n\n\n\n\n\n\nMichael Foster\n\n\nmichael.foster@example.com\n\n\n\n\n\nCo-Founder / CTO\n\n\nLast seen 3h ago\n\n\n\n\n\n\n\n\nDries Vincent\n\n\ndries.vincent@example.com\n\n\n\n\n\nBusiness Relations\n\n\n\n\n\n\n\n\nOnline\n\n\n\n\n\n\n\n\n\nLindsay Walton\n\n\nlindsay.walton@example.com\n\n\n\n\n\nFront-end Developer\n\n\nLast seen 3h ago\n\n\n\n\n\n\nJumbotron\n\n\n\n\nJumbotron",
    "crumbs": [
      "Learn",
      "tailwind css test"
    ]
  },
  {
    "objectID": "packages/index.html",
    "href": "packages/index.html",
    "title": "Packages",
    "section": "",
    "text": "List\n\nGLM.jl",
    "crumbs": [
      "Packages"
    ]
  },
  {
    "objectID": "start/index.html",
    "href": "start/index.html",
    "title": "Start",
    "section": "",
    "text": "import MLJ:evaluate,MLJInterface\nusing MLJ,DataFrames\niris=load_iris()|&gt;DataFrame\n\ny, X = unpack(iris, ==(:target); rng=123);\n\nfunction  build_model(X,y)\n    Tree = @load DecisionTreeClassifier pkg=DecisionTree\n    tree = Tree()\n    evaluate(tree, X, y, resampling=CV(shuffle=true),\n        measures=[log_loss, accuracy],\n        verbosity=0)\nend\n\nbuild_model(X,y)",
    "crumbs": [
      "Start",
      "GET STARTED"
    ]
  },
  {
    "objectID": "start/index.html#first-code-snippets",
    "href": "start/index.html#first-code-snippets",
    "title": "Start",
    "section": "",
    "text": "import MLJ:evaluate,MLJInterface\nusing MLJ,DataFrames\niris=load_iris()|&gt;DataFrame\n\ny, X = unpack(iris, ==(:target); rng=123);\n\nfunction  build_model(X,y)\n    Tree = @load DecisionTreeClassifier pkg=DecisionTree\n    tree = Tree()\n    evaluate(tree, X, y, resampling=CV(shuffle=true),\n        measures=[log_loss, accuracy],\n        verbosity=0)\nend\n\nbuild_model(X,y)",
    "crumbs": [
      "Start",
      "GET STARTED"
    ]
  },
  {
    "objectID": "workflow/index.html",
    "href": "workflow/index.html",
    "title": "MLJ workflow",
    "section": "",
    "text": "graph TD\n    A[(Data)] --&gt;|Tidy Data| B(Ingestion)\n    B --&gt; C{Model Search}\n    C --&gt;|One| D[Regression]\n    C --&gt;|Two| E[Classification]\n    C --&gt;|Three| F[Dimension Reduction]\n    C --&gt;|Four| G[Ensemble Models]\n    C --&gt;|Five| H[Other Models]\n    D--&gt;J(Instantiating Model)\n    E--&gt;J\n    F--&gt;J\n    G--&gt;J\n    H--&gt;J\n    J--&gt;K(Evaluating Model)\n    K--&gt;L(Inspecting training results)\n    L--&gt;M(hyperparameter tuning)\n    M--&gt;N(Constructing linear pipelines)\n    N--&gt;O(ensemble of models) \n    O--&gt;P(Performance curves)\n\n\n\n\n\ngraph TD\n    A[(Data)] --&gt;|Tidy Data| B(Ingestion)\n    B --&gt; C{Model Search}\n    C --&gt;|One| D[Regression]\n    C --&gt;|Two| E[Classification]\n    C --&gt;|Three| F[Dimension Reduction]\n    C --&gt;|Four| G[Ensemble Models]\n    C --&gt;|Five| H[Other Models]\n    D--&gt;J(Instantiating Model)\n    E--&gt;J\n    F--&gt;J\n    G--&gt;J\n    H--&gt;J\n    J--&gt;K(Evaluating Model)\n    K--&gt;L(Inspecting training results)\n    L--&gt;M(hyperparameter tuning)\n    M--&gt;N(Constructing linear pipelines)\n    N--&gt;O(ensemble of models) \n    O--&gt;P(Performance curves)\n\n\n\n\nFigure 1: MLJ main work flow",
    "crumbs": [
      "Workflow",
      "flowchart"
    ]
  },
  {
    "objectID": "workflow/index.html#common-workflow",
    "href": "workflow/index.html#common-workflow",
    "title": "MLJ workflow",
    "section": "",
    "text": "graph TD\n    A[(Data)] --&gt;|Tidy Data| B(Ingestion)\n    B --&gt; C{Model Search}\n    C --&gt;|One| D[Regression]\n    C --&gt;|Two| E[Classification]\n    C --&gt;|Three| F[Dimension Reduction]\n    C --&gt;|Four| G[Ensemble Models]\n    C --&gt;|Five| H[Other Models]\n    D--&gt;J(Instantiating Model)\n    E--&gt;J\n    F--&gt;J\n    G--&gt;J\n    H--&gt;J\n    J--&gt;K(Evaluating Model)\n    K--&gt;L(Inspecting training results)\n    L--&gt;M(hyperparameter tuning)\n    M--&gt;N(Constructing linear pipelines)\n    N--&gt;O(ensemble of models) \n    O--&gt;P(Performance curves)\n\n\n\n\n\ngraph TD\n    A[(Data)] --&gt;|Tidy Data| B(Ingestion)\n    B --&gt; C{Model Search}\n    C --&gt;|One| D[Regression]\n    C --&gt;|Two| E[Classification]\n    C --&gt;|Three| F[Dimension Reduction]\n    C --&gt;|Four| G[Ensemble Models]\n    C --&gt;|Five| H[Other Models]\n    D--&gt;J(Instantiating Model)\n    E--&gt;J\n    F--&gt;J\n    G--&gt;J\n    H--&gt;J\n    J--&gt;K(Evaluating Model)\n    K--&gt;L(Inspecting training results)\n    L--&gt;M(hyperparameter tuning)\n    M--&gt;N(Constructing linear pipelines)\n    N--&gt;O(ensemble of models) \n    O--&gt;P(Performance curves)\n\n\n\n\nFigure 1: MLJ main work flow",
    "crumbs": [
      "Workflow",
      "flowchart"
    ]
  },
  {
    "objectID": "workflow/mlj-pca-workflow.html",
    "href": "workflow/mlj-pca-workflow.html",
    "title": "MLJ PCA Workflow",
    "section": "",
    "text": ":::{.row} :::{.col-sm-6 col-md-6 col-lg-6}\n\n\n\n\nload package\n\n\n\n using CSV,DataFrames,Tidier,Pipe\n using CairoMakie,AlgebraOfGraphics,MakieThemes\n using MLJ,MLJModelInterface,Random\n import MLJ:transform,predict\n #Makie.set_theme!(ggthemr(:flat))\n\n\n\n\n\nload csv\n\n\n\n\n\ndf=CSV.File(\"../../data/iris.csv\")|&gt;DataFrame\ndf=@chain  df  begin\n    @clean_names\nend\nfirst(df,5)\n\n\n\n\n\n\n\ndescribe df\n\n\n\n\n\ndescribe(df)\n\n\n\n\n\n\n\ndataprocessing\n\n\n\n\n\n   coerce!(df,:labels=&gt;Multiclass)\n   ytrain, Xtrain=  unpack(df, ==(:species), rng=123)\n   cat=levels(ytrain)\n   rows,cols=size(Xtrain)\n\n\n\n\n\n\n\nPCA workflow\n\n\n\n\n\nPCA = @load PCA pkg=MultivariateStats\nmaxdim=6\nmodel=PCA(maxoutdim=maxdim)\nmach = machine(model, Xtrain) |&gt; fit!\nYtr =transform(mach, Xtrain)\nfirst(Ytr,10)\n\n\n\n\n6. report mach\n\nreport(mach)\n\n7. plot results\n\n\n7.1 2 pcs\n#| label: fig-iris-pca-2pcs\n#| fig-cap: \"iris-pca-2pcs\"\n#| fig-cap-location : margin\n#| fig-align: \"center\"\n  ax=(width=400, height=300)\n  table=(pc1=Ytr.x1,pc2=Ytr.x2,cat=ytrain)\n  datalayer=data(table)\n  maplayer=mapping(:pc1,:pc2,color=:cat)\n  vislayer=visual(Scatter,markersize=10,strokewidth=1,strokecolor=:black)\n  plt=datalayer*maplayer*vislayer*visual(alpha = 0.5)\n  draw(plt,axis=ax)\n\n\n7.2 3 pcs\n#| label: fig-iris-pca-3pcs\n#| fig-cap: \"iris-pca-3pcs\"\n#| fig-cap-location : margin\n#| fig-align: \"center\"\nlet\nax = (type = Axis3, width = 400, height = 300,azimuth =-0.1pi,elevation=0.1pi)\ntable=(pc1=Ytr.x1,pc2=Ytr.x2,pc3=Ytr.x3,cat=ytrain)\ndatalayer=data(table)\nmaplayer=mapping(:pc1,:pc2,:pc3,color=:cat)\nvislayer=visual(Scatter,markersize=10,strokewidth=1,strokecolor=:black)\nplt=datalayer*maplayer*vislayer*visual(alpha = 0.5)\ndraw(plt,axis=ax)\nend\n::: :::",
    "crumbs": [
      "Workflow",
      "MLJ PCA Workflow"
    ]
  }
]